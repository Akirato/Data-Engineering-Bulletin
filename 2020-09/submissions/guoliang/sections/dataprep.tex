%!TEX root = ../main.tex
\section{Human-in-the-loop Machine Learning Pipeline}
\label{sec:pipeline}

As shown in Figure~\ref{fig:framwork}, humans play significant roles in machine learning pipeline. First, given some unstructured data, we have to transform it structured data, in order to construct features for ML. Then for structured data from multiple sources, we should integrate them for enriching data and features to achieve  well-performed ML model. What's more, data is always dirty in the real world. To further improve the performance, we should clean the data, such as repairing records that violate integrity constraint  and  removing outliers and duplicates. Finally, we should annotate the data for building the model. For all above steps in the pipeline, humans can contribute their intelligence to provide high quality training data and improve the ML model. Next, we will introduce what humans can contribute in these steps.



\subsection{Data Extraction}

Extracting structured data from unstructured data is an important problem both in industry and academia, which has been studied broadly from rule-based~\cite{DBLP:conf/acl/LiRC11} systems to ML-based  approaches~\cite{DBLP:conf/wsdm/NakasholeTW11, DBLP:conf/aaai/MitchellCHTBCMG15}. However, these methods either need domain experts to design rules or humans to provide large quantities of labels. Recently, DeepDive~\cite{DBLP:conf/sigmod/Zhang0RCN16} is a representative system in this area, which provides  declarative language  for  non-expert users to extract data.  The execution of DeepDive can be divided into three parts: candidate generation, supervision, statistical inference and learning. Humans mainly contribute in the first part, i.e., candidate generation. In this part, humans write some extraction rules described by declarative languages to retrieve data with attributes or relations, such as  entity B is the wife of A if there exists mention ``and his wife" between A and B in a corpus. The goal of this part is to generate candidates with high recall and low precision. Secondly, the supervision part applies distant supervision rules from knowledge bases or incomplete databases  to provide labels for some of the candidates. The rules do not need to label all candidates from the first part, which are intended to be a low recall and high precision. For the last part, DeepDive constructs a graphical model that represents all of the labeled candidate extractions, trains the model, and then infers a correct probability for each candidate. At the end of this stage, DeepDive applies a threshold to each inferred probability and then derives the extractions to  the output database. In conclusion, Deepdive leverages humans to provide extraction candidates with high recall, uses weak supervision(distance supervision) to label them and finally trains a statistical ML model to fine-tune the labels.
 
 

\subsection{Data Integration}
Given relational tables from multiple sources, in many cases we want to integrate them for extending existing datasets, including features and records. To this end, schema matching~\cite{DBLP:journals/pvldb/ZhangCJC13,DBLP:conf/icde/FanLOTZ14} and entity resolution~\cite{DBLP:crowder, DBLP:transitivity} have to be applied, where the first part is going to align the columns and the second will match records from different tables. Recently, many existing works focused on leveraging human intelligence to achieve these. 

For schema matching, existing works~\cite{DBLP:journals/pvldb/ZhangCJC13} utilize human-machine hybrid approaches to improve the performance. They utilize machine-based  schema matching tools to generate a set of possible matchings, each of which has a probability to be matched. 
They define a correspondence correctness question (CCQ) for humans to answer, which denotes a pair of attributes from two columns, so each matching consists of a set of correspondences. Then the problem is to wisely choose  the correspondences to ask the human to obtain the highest certainty of correct schema matching at the lowest cost. The uncertainty is measured by entropy on top of the probabilities that the tools generate. 
In the correspondence selection, they consider the column correlations, selection efficiency and human quality to match schemes effectively and efficiently. 
Fan et.al~\cite{DBLP:conf/icde/FanLOTZ14} introduce knowledge base together with humans to do schema matching. First, they propose a concept-based approach that maps each column of a  table to the best concept in knowledge bases. This approach overcomes the problem that sometimes values of two columns may be disjoint, even though the columns are related, due to incompleteness in the column values. Second, they develop a hybrid machine-crowdsourcing framework that leverages human intelligence to discern the concepts for ``difficult'' columns. The overall framework assigns the most ``beneficial'' column-to-concept matching tasks to the human under a given budget and utilizes the answers to  infer the best matching. 

After the schemes are aligned, we can integrate different relational tables by the join operation. Traditionally, join is always executed by exact matching between values of attributes from two tables. However, in the real world,  data is  always dirty. For example, ``Apple iPhone 8" and "iPhone 8th" refer to the same entities and should be joined, which cannot be done by a traditional database. Therefore, the human-based join is proposed to address this problem. Wang et.al.~\cite{DBLP:crowder} propose  crowd-based join framework, which generates many candidate pairs, uses similarity based pruning techniques to eliminate dissimilar pairs and ask the crowd to answer the rest  pairs. To further reduce the cost, Wang et.al.~\cite{DBLP:transitivity} leverage the transitivity technique to deduce unknown answers based on current answers from humans. Chai et.al.~\cite{DBLP:journals/vldb/ChaiLLDF18, DBLP:conf/sigmod/ChaiLLDF16} build a partial-order graph based on value similarities of different attributes and utilize the graph to prune pairs that are not necessary to ask. To improve the quality,  Wang et.al.~\cite{DBLP:conf/sigmod/WangXL15} first cluster the entities to be joined  and then leverage humans to refine the clusters. Yalavarthi et.al.~\cite{DBLP:conf/cikm/YalavarthiKK17} select questions judiciously considering the crowd errors.





\subsection{Data Cleaning}

Data is dirty in the real world, which is likely to hurt the ML performance. For example, some values may be out of range (e.g., age is beyond 120 or below 0) or utilize wrong units (e.g., some distances are in meters while other are in kilometers); Some records  refer to the same entity; Integrity constraints (e.d. functional dependencies) are violated among records. Recently, many researchers focused on leveraging human to clean the data. For instance, crowd-based entity resolution~\cite{DBLP:crowder, DBLP:journals/vldb/ChaiLLDF18, DBLP:conf/sigmod/ChaiLLDF16} is applied  to remove duplicates. Chai et.al. ~\cite{DBLP:conf/sigmod/ChaiC00LM20} use human expertises to identify outliers among the data. Specifically, they first utilize machine-based outlier detection algorithms to detect some outlier candidates as well as inlier candidates, and then human is asked to verify these candidates by comparing outlier candidates with inliers.  Chu et.al.~\cite{DBLP:journals/pvldb/ChuOMIP0Y15} clean the  data that violates integrity constraints with the help of knowledge base and humans. They first identify the relationships between columns using knowledge base and then use humans to verify them. Then the discovered relationships can be utilized to detect errors among data, and then these error can be repaired by the knowledge base and humans. 


Recently, a line of interesting data cleaning works focus on cleaning with the  explicit goal of improving the ML results. Wang et al.~\cite{DBLP:conf/sigmod/KrishnanFGWW16} propose a cleaning framework ActiveClean for machine learning tasks. Given a dataset and machine learning model with a convex loss, it selects records that can most improve the performance of the model to clean iteratively. ActiveClean consists of 4 modules, sampler, cleaner, updater and estimator. Sampler is used to select a batch of records to be cleaned. The selection criterion is measured by how much improvement can be made after cleaning a record, i.e., the variation of the gradient, which is estimated by the Estimator. Then the selected records will be checked and repaired by the Cleaner, which can be humans. Next, the Updater updates the gradient based on these verified dirty data. The above four steps are repeated until the budget is used up.
BoostClean~\cite{DBLP:journals/corr/abs-1711-01299} cleans the data where an attribute value is out of range. It takes as input a dataset and a set of functions for detecting errors and repair functions. These functions can be provided by humans. Each pair of detection and repair functions can produce a new model. BoostClean uses statistical boosting to find the best ensemble of pairs that maximize the final performance.
Recently, TARS~\cite{DBLP:journals/pvldb/DolatshahTWP18} was proposed to clean human labels using oracles, which provides two pieces of advice. First, given test data with noisy labels, TARS estimates the performance of the model on  true labels, which is shown to be unbiased and confidence intervals are computed to bound the error. Second, given
training data with noisy labels, TARS determines which examples to be sent to an oracle so as to maximize the expected model improvement of cleaning each noisy label.

\subsection{Iterative Labeling}

After the above steps of data preprocessing,  we can label the data in relational tables for ML tasks. The most  straightforward method is to directly leverage humans to annotate a bunch of data for training. Thus we can adopt the cost control and quality control approaches proposed in Section~\ref{sec:overview} to derive high quality labels with low cost (see ~\cite{DBLP:conf/icde/LiWZF17} for a survey). However, in many cases,  a user does not have enough budget to obtain so many annotations. Therefore, many researchers focused on how to label data iteratively and make the model performance better and better using techniques like active learning or weak supervision.

Mozafari et.al.~\cite{DBLP:journals/pvldb/MozafariSFJM14} use active learning to scale up the human labeling, which can be utilized in two scenarios, the upfront and iterative scenario. In the upfront scenario, the user cares more about the latency than the cost. Therefore, given a budget and an initial model, the algorithm uses a ranker to rank and selects some of the most informative examples to label while the rest are predicted by the model. In the iterative scenario, since the user cares more about the cost, the ranker selects a batch of examples to label, retrains the model and selects again until the budget is used up. There are two strategies (Uncertainty and MinExpError) that the user can choose for ranking.  Leveraging the traditional active learning technique, Uncertainty selects examples that the current model is the most uncertain about.  MinExpError uses a more sophisticated algorithm that considers both  the uncertainty and expected model change. Besides, the work also utilizes the bootstrap theory, which makes the algorithms available to any classifier and also enables parallel processing. Also, active learning techniques in section~\ref{subsec:active_learning} can also be integrated in the framework.

DDLite~\cite{DBLP:conf/sigmod/Ehrenberg0RFR16} leverage human to conduct data programming rather than hand-labeling data, in order to generate large quantities of labels. Given a set of input documents, DDLite aims to produce a set of extracted entities or relation mentions, which consists of four steps. First,  given input documents,  preprocessing like domain-specific tokenizers or parsers  of the raw text has to be performed. Second, DDLite provides a library of general candidate extraction operators, which can be designed by humans. Third, humans develop a set of labeling functions through iterating between labeling some small subsets and analyzing the  performance of  labeling functions.
Lastly,  features are automatically generated for the candidates, and then the model is trained using the labeling functions. The humans then analyze the performance on a test set.

\subsection{Model Training and Inference}

 For different machine learning tasks, there are different techniques that leverage humans' knowledge to train and infer the results,  considering humans' diverse qualities. In this part, we mainly discuss two common ML tasks, classification and clustering, and  show how to leverage human intelligence as well as ML techniques to achieve high quality results.


For classification,  it is expensive to obtain reliable labels to train a model, so multiple humans are required to collect subjective labels.  Raykar et.al.~\cite{raykar2010learning} first proposed a straightforward method that simply utilizes majority voting(MV) to infer labels. However, MV does not consider  features of examples. Therefore, given the human labels and features of examples, they improve the model by considering the true labels as latent variables and utilize the Expectation-Maximization (EM) algorithm to train the model. The parameters include the worker qualities and feature weights. Rodrigues et.al.~\cite{DBLP:conf/aaai/RodriguesP18} also use EM algorithm to jointly learn the parameters of humans and examples. The difference is that they use deep learning to train the model, where a crowd layer is proposed to allow the neural network to learn directly from noisy humans labels in an end-to-end manner. In some cases, acquiring large quantities of labels is expensive, so Atarashi et.al.~\cite{DBLP:conf/aaai/AtarashiOK18} proposed to  learn from a small number of  human labels and unlabeled data   using deep generative model in a semi-supervised way. More specifically,  they leverage the unlabeled data effectively by introducing latent features and a data distribution. Because the data distribution can be complex, they use a deep neural network for the data distribution. 
Classification based on taxonomy is a particular but important task that the labels can consist of a taxonomy. For example, BMW X3 and   BMW X5 belong to  BMW, which belongs to Car. For this scenario, Parameswaran et.al.~\cite{DBLP:journals/pvldb/ParameswaranSGPW11} utilize a human-machine hybrid method to classify the examples on the taxonomy. For example, given a picture of an Audi car, we can ask the humans to label whether it is a BMW car. If not, the children of BMW (BMW X3 and BMW X5) can be pruned. They study how to use the minimum number of questions to get all  the labels.




For clustering, we can also leverage human intelligence to cluster examples that are hard to identify by computers. Following the k-means algorithm,  Heikinheimo et.al.~\cite{DBLP:conf/hcomp/HeikinheimoU13} propose a  human-in-the-loop framework that asks the humans to answer a simple task each time and aggregate all the answers to deduce the final clustering result. Specifically, the simple task is, given a triple with three objects, asking the human to select the one different from the other two objects. First, the algorithm picks a large enough number of triplets from the entire dataset and asks humans to label them. Second, for each example, they compute a penalty score defined as  the number of times the example was chosen to be different. Third, the example having the lowest penalty score is returned. Thus, the centroid example of each cluster is computed and we can obtain the clustering results iteratively. However, this method is expensive because of the large number of triple tasks and cannot generalize when there are new examples. To address this,  Gomes et.al.~\cite{DBLP:conf/nips/GomesWKP11} propose to use a generative  model to infer the clusters. Moreover, it can capture multiple  clustering criteria from diverse viewpoints of humans. For example, given a set of pictures of products, one may want to cluster by brands while another human is likely to cluster by types. Specifically, they divide the entire set into small groups and ask humans to cluster examples in each group. Then considering the humans' quality and labels, \cite{DBLP:conf/nips/GomesWKP11} uses a Bayesian generative model to infer the clustering results.





