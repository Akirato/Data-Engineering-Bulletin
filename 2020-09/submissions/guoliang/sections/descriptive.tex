%!TEX root = ../main.tex
\section{Open Challenges and Opportunities}
\label{sec:future}


~~~~~~\textbf{Data discovery for ML.} Suppose an AI developer aims to build an ML model. Given a dataset corpus, the user requires to find relevant datasets to build the model. Data discovery aims to automatically find relevant datasets from data warehouse considering the applications and user needs. Many companies propose data discovery systems, like Infogather~\cite{DBLP:conf/sigmod/YakoutGCC12} in Microsoft and Goods~\cite{DBLP:conf/sigmod/HalevyKNOPRW16} in Google. However,  such systems focus on keyword-based dataset search or just linking datasets. Therefore, it may be  worth studying to discover datasets that  can directly maximize the performance of the downstream ML model. The key challenges lie in how to find valuable features and data among the corpus.


\textbf{Modules selection in ML pipeline.}
Figure~\ref{fig:framwork} shows the standard ML pipeline from data preparation to the model training and testing, which consists of several modules like schema matching, data cleaning and integration, etc, and data cleaning can also be extended to many scenarios, like missing values, outliers and so on. Given an ML task, asking the humans to process all  modules is expensive, and it may not be necessary. Thus, we can study which modules are significant to the ML model and drop the other ones. For example, given a classification task, some data cleaning tasks like removing duplicates are not necessary. Therefore, how to select modules to optimize the ML pipeline is worth to study.


\textbf{Trade-off between human quality and model performance.}
Some existing works~\cite{DBLP:journals/vldb/RatnerBEFWR20, DBLP:conf/aaai/MitchellCHTBCMG15, DBLP:conf/acl/MintzBSJ09} focus on  acquiring weak labels to derive a model with good performance. One idea is to study the trade-off between human quality and model performance. That is, given a performance requirement, such as 80\% F-measure, we can decide how to  select humans to label the training data with the goal of optimizing the cost. Also, given human qualities, we can study how to produce results with the highest quality. 




%\textbf{Crowdsourcing.}  Recently, famous crowdsourcing platforms like AMT~\cite{amt} provide hundreds of thousands of humans who can process big data tasks. Even though many works~\cite{li2017cdb,DBLP:conf/icde/ChaiLFL20,9039632,8989823, chai2018incentive,DBLP:conf/icde/ChaiF0WZ19,DBLP:conf/sigmod/QinCL0020,DBLP:conf/icde/HaoC00WY20,DBLP:conf/icde/LuoCQ0020} have been proposed to study how to improve the humans' quality and  save the cost,  there still exist some challenges to be solved. For example, existing crowdsourcing works mainly focus on micro-tasks like classification because the tasks are easy to decompose and the quality is easy to control. However, macro-tasks, like designing a data repairing rule, are also important in ML task. Therefore, how to design macro tasks and derive high quality results are challenging problems.


\textbf{Benchmark.} A large variety of TPC benchmarks (e.g., TPC-H for analytic workloads, TPC-DI for data integration) standardize performance comparisons for database systems and promote the development of the database community. Even though there are some open datasets for crowdsourcing  or machine learning tasks, there is still lack of standardized benchmarks that covered the entire human involved machine learning pipeline. To better explore the research topic, it is significant to study how to develop evaluation methodologies and benchmarks for the human-in-the-loop machine learning system.








%\stitle{Linked Visualizations.}

