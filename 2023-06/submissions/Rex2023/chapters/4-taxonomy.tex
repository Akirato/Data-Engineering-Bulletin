\section{Method Taxonomy}\label{sec:taxonomy}
\begin{table*}[htb]
\caption{A comprehensive summary of existing generative explanation methods for Graph Neural Networks. RL-MDP denotes the reinforcement learning approach based on Markov Decision Process and RL-DAG denotes the reinforcement learning approach based on Direct Acyclic Graph.} 
\vspace{-5pt}
\centering
\resizebox{0.95\linewidth}{!}{
\begin{tabular}{lccccc}
\toprule
\textbf{Method} & \textbf{Generator} & \textbf{Information Constraint} & \textbf{Level} & \textbf{Scenario} & \textbf{Output}\\
\midrule
PGExplainer~\cite{PGExplainer}&Mask Generation&size&instance&factual&E\\
GIB~\cite{yugraph}&Mask Generation & mutual information & instance&factual & N\\
GSAT~\cite{GSAT}&Mask Generation&variational&instance&factual&E\\
GNNInterpreter~\cite{gnninterpreter}&Mask Generation&size&model&factual&N / E / NF\\
GEM~\cite{GEM}&VGAE&size&instance&factual&E\\
CLEAR~\cite{CLEAR} & VGAE & size & instance & counterfactual & E / NF\\
OrphicX~\cite{OrphicX} & VGAE & variational \& size  & instance & factual & E\\
D4Explainer&Diffusion&size&instance \& model &counterfactual & E\\
GANExplainer~\cite{Gan-Explainer} & GAN &- & instance&factual&E\\
RCExplainer~\cite{RCExplainer}&RL-MDP&size&instance&factual&\textsc{subgraph}\\
XGNN~\cite{XGNN}&RL-MDP&size&model&factual&\textsc{subgraph}\\
GFlowExplainer~\cite{GFlowExplainer}&RL-DAG&size&instance&factual&\textsc{subgraph}\\
\bottomrule
\end{tabular}}
\label{table:comparison}
\vspace{-0.2cm}
\end{table*}
The information constraints $\mathcal{L}_{\textsc{INFO}}$ in Sec.~\ref{sec:info_constraint} and the attribution constraints $\mathcal{L}_{\textsc{ATTR}}$in Sec.~\ref{sec:generative_model} can be combined to construct an overall optimization objective for GNN explainability. 
We provide a comprehensive comparison and summary of existing generative explanation methods and their corresponding generators and information constraints in Table~\ref{table:comparison}. Most existing approaches focus on instance-level factual explanations, while CLEAR~\cite{CLEAR} focuses on counterfactual explanation and D4Explainer is applicable for both counterfactual and model-level explanations. GIB~\cite{yugraph} proposes to deploy mutual information between the generated explanation graph and the original graph as the information constraint, while GSAT~\cite{GSAT} utilizes the variational constraint. We further compare the outputs of these approaches (the last column in Table~\ref{table:comparison}), where $\textsc{E}$ denotes outputting edge importance with continuous values, $\textsc{N}$ denotes node importance with continuous values, $\textsc{NF}$ denotes the importance of node features and $\textsc{subgraph}$ denotes hard masks for discrete explanatory subgraphs.