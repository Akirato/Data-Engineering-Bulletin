\section{Generative Framework for Graph Explanations}
\subsection{Unified Optimization Objective}\label{sec:generative_formulation}
%The instance-dependent explainers~\cite{GNNExplainer,PGM-Explainer,SubgraphX, Grad-CAM-Graph} focus on a single input graph and optimize a set of instance-dependent model parameters, thus suffering from some key disadvantages. Firstly, they often ignore valuable high-level insights that can only be gained from analyzing the dataset as a whole. Secondly, these methods may not be efficient for large datasets and may not generalize well to new instances or datasets. \rex{I think we could emphasize our contribution here. Sth like Here we illustrate a general framework that encompasses more recent approaches that attempt to overcome the above limitations.}
% \rex{better words? by learning a strategy to search for the most explanatory subgraphs?} 

To overcome the aforementioned limitations, recent research has develop approaches that leverage deep generative methods to explain GNNs. Instead of optimizing an explanation for individual instances,  the generative methods aim to generate explanations for new graphs by learning a strategy to search for the most explanatory subgraphs across the whole dataset. Formally, given a set of input graphs $\mathcal{G}$, the generative explainer learns the distribution of the underlying explanation graphs $p(G_{e}|G)$ using a parameterized subgraph generator $g_{\theta}:\mathcal{G}\rightarrow \mathcal{G}_{e}$. After training, the subgraph generator is capable of identifying the explanation subgraphs that are most important to the desired graph labels:
\begin{equation}\label{eq:objective1}
     \theta^\ast = \argmax_{\theta}\log P_{Y^\ast}(G_e|\theta, G),
\end{equation} 
where $P_{Y^\ast}(G_e|\theta, G)$ is the probability that the generated $G_e=g_\theta(G)$ is a valid explanation for the desired label $Y^\ast$.
In addition to the validity requirement, an ideal explanation graph should be sparse and compact compared with the given graph. Directly optimizing Eq.~\ref{eq:objective1} leads to a trivial solution where $G_{e}=G$, as the input graph is most informative for the graph label.
To obtain a compact explanation, we impose an information constraint $\mathcal{L}_{\textsc{INFO}}(G_e, G)$ that restricts the amount of information contained in the generated explanation subgraph, thereby ensuring the conciseness and brevity of the explanations. The overall objective of generative explanation is
\begin{equation}\label{eq:objective2}
    \min_{\theta} -\log P_{Y^\ast}(G_e|\theta, G)+ \mathcal{L}_{\textsc{INFO}}(G_e, G):=\mathcal{L}_{\textsc{ATTR}}(G_e,Y^\ast) + \mathcal{L}_{\textsc{INFO}}(G_e, G).
\end{equation}
We name the first term in Eq.~\ref{eq:objective2} the attribution loss $\mathcal{L}_{\textsc{ATTR}}(G_e, Y^\ast)$, which measures whether $G_e$ captures the most important substructures for the desired label $Y^\ast$. $\mathcal{L}_{\textsc{ATTR}}$ is typically the cross-entropy loss for categorical $Y^\ast$ and mean squared loss for continuous $Y^\ast$.

%between he output probability $P_f(G_e)$ and $Y^\ast$ as well as model-specific constraints to $G_e$.
 % \rex{in the first sentence, we can give a summary. What's similar and what's different from VAE objectives?}
\xhdr{Connection With Variational Auto-encoder} $\mathcal{L}_{\textsc{INFO}}$ in Eq.~\ref{eq:objective2} can be set as the variational constraint, \ie $\mathcal{L}_{\textsc{INFO}}(G, G_e):=\mathrm{D_{KL}}\left(q_\theta(G_e |  G) \| Q(G_e)\right)$, where $\mathrm{D_{KL}}$ denotes Kullback–Leibler divergence, $Q(G_e)$ is the prior distribution of the generated explanation graph $G_e$, and $q_\theta(G_e|G)$ is the variational approximation to $g_\theta(G_e| G)$, variational constraint drives the posterior distribution of $G_e$ generated by $g_\theta(\cdot|G)$ to its prior distribution, thus restricting the information contained in $G_e$ in the process. The overall objective is 
\begin{equation}\label{eq:objective_vc}
    \mathcal{L}=\mathbb{E}_G-\log P_{Y^\ast}(G_e | \theta, G) + \mathrm{D_{KL}}\left(q_\theta(G_e |  G) \| Q(G_e)\right),
\end{equation}
In this case, the objective of generative explanation shares similar spirits with the Variational Auto-encoder (VAE)~\cite{GVAE}.
Recall the optimization objective of VAE is
\begin{equation}\label{eq:vae}
    \mathcal{L}_{\textsc{VAE}}=\mathbb{E}_{z \sim q_\phi(z | G)} -\log \left(p_\varphi(G | z)\right)+\mathrm{D_{KL}}\left(q_\phi(z| G) \| q (z)\right),
\end{equation}
where $q_\phi$ is the encoder that maps graph $G$ into a latent space, then the decoder $p_\varphi$ recovers the original graph $G$ based on the latent representation $z$. $q (z)$ is the prior distribution of the latent representation, which is usually a Gaussian distribution. Notably, the generative explanation approach with the variational constraint as $\mathcal{L}_{\textsc{INFO}}$ (Eq.~\ref{eq:objective_vc}) is a variant of Variational Auto-encoder (VAE) (Eq.~\ref{eq:vae}), albeit with two fundamental distinctions. Firstly, VAE aims to learn the distribution of the original graph, whereas generative explanation focuses on learning the underlying distribution of explanatory structures. Secondly, VAE constrains the distribution of latent representation $z$, while the generative explanation constrains the posterior distribution of $G_e$. These distinctions highlight the methodologies for generalizing generative models to the task of GNN explainability.


% \rex{i don't understand the last sentence. what do the similarities and differences imply? is it trying to justify why our objective is the right approach, because it has the variational interpretation? I think we can be more specific here.}
% \rex{how would be mix and match constraints with models?}

% In this work, we present an encoder-decoder framework for generative explanations. The encoder $q_\phi$ with parameters $\phi$ maps original graph $G$ into a latent space by $z=q_\phi(G)$, then the decoder $p_\theta$ with parameters $\theta$ generates an explanation graph $G_e$ based on the latent representation $z$ by $G_e = p_{\theta}(z)$. The objective of generative explanations is as follows,
% \begin{equation}\label{eq:objective}
%     \phi^\ast, \theta^\ast = \argmax_{\phi, \theta}\log P_{Y^\ast}(G_e\mid\phi, \theta, G),
% \end{equation} 
% where $P_{Y^\ast}(G_e|\theta, \phi, G)$ is the probability that the generated $G_e$ is a valid explanation for the original graph $G$ and the desired label $Y^\ast$.  However, it is hard to directly optimize the intractable objective in Eq.~\ref{eq:objective}. We introduce the latent representation $z$ to jointly model the distribution of $z$ and $G_e$, hence deriving a tractable variational lower bound for Eq.~\ref{eq:objective}. Therefore, the generative explanation is to maximize the following evidence lower bound (ELBO):
% \begin{equation}\label{eq:elbo}
%     L_{\theta, \phi}(G)=\mathbb{E}_{z \sim q_\phi(\cdot \mid G)}\left[\log P_{Y^\ast}(G_e \mid \phi, \theta, G, z)\right]-\mathrm{D_{KL}}\left(q_\phi(z|G)\| q(z)\right), 
% \end{equation}
% where $\mathrm{D_{KL}}$ denotes Kullback–Leibler divergence, $q(z)$ is the prior distribution of the latent representation $z$. $q_\phi(z|G)$ is the approximate posterior distribution over $z$ given the input graph $G$. Notably, the latent representation $z$ with the decoder $p_\theta$ essentially controls the distribution of the generated graph $G_e$. Besides, $G_e$ is more intuitive and meaningful than the latent representation $z$. Hence, the second term in Eq.~\ref{eq:elbo} is equivalent to driving the posterior distribution of generated graph $G_e$ to its prior distribution $Q(G_e)$. Therefore, the optimization objective of generative explanation is
% \begin{equation}
%     \min_{\phi, \theta} -\mathbb{E}_{G}\mathbb{E}_{z \sim q_\phi(\cdot \mid G)}\left[\log P_{Y^\ast}(G_e \mid \phi, \theta, G, z)\right]+ \mathbb{E}_{G}\left[\mathrm{D_{KL}}\left(p_\theta(G_e \mid \phi, z, G) \| Q(G_e)\right)\right]
% \end{equation}
% Taking the input graph $G$ and the corresponding generated graph $G_e$ as two random variables, we obtain the following inequality regarding the second term in Eq.~\ref{eq:objective2}:
% \begin{equation}
%     \mathbb{E}_{G}\left[\mathrm{D_{KL}}\left(p_\theta(G_e \mid \phi, z, G) \| Q(G_e)\right)\right]\geq MI(G_e;G),
% \end{equation}
% where $MI(G_e; G)$ denotes the mutual information between $G_e$ and $G$. To optimize Eq.~\ref{eq:objective2}, it is necessary to minimize the following objective:
% \begin{equation}\label{eq:objective3}
%     \min_{\phi, \theta} \underbrace{-\mathbb{E}_{G}\mathbb{E}_{z \sim q_\phi(\cdot \mid G)}\left[\log P_{Y^\ast}(G_e \mid \phi, \theta, G, z)\right]}_{\hbox{\small validity loss}}+ \underbrace{MI(G_e, G)}_{\hbox{\small compression loss}}.
% \end{equation}
% The first term $P_{Y^\ast}(G_e\mid\theta, \phi, G)$ in Eq.~\ref{eq:objective3} measures how much $G_e$ is valid for the desired label $Y^\ast$, hence it typically contains the cross entropy between the output probability $P_f(G_e)$ and $Y^\ast$ and model-specific constraints. $MI(G_e, G)$ is a compression term that measures the amount of information contained in the generated graph $G_e$ given the original graph $G$. It drives $G_e$ to be compact and compressed compared with $G$. In the counterfactual explanation scenario, we aim for a graph $G_{ce}$ with minimal changes to the input graph $G$, therefore, we replace $G_e$ with $G-G_{ce}$ in the compression loss and with $G_{ce}$ in the validity loss. 

% Due to the intractability of mutual information, Eq.~\ref{eq:objective2} is essentially a practical and efficient upper-bound estimation of Eq.~\ref{eq:objective3}. In the following sections, we take Eq.~\ref{eq:objective3} as the general expression of the optimization objective for the existing explanation methods.




% \xhdr{Objective of Generative Framework}
% \begin{equation}
%     L_{\theta, \phi}(x)=\mathbb{E}_{z \sim q_\theta(\cdot \mid x)}\left[\log p_\phi(x \mid z)\right]-D_{K L}\left(q_\theta(\cdot \mid x) \| p_\phi(\cdot)\right)
% \end{equation}
\subsection{Taxonomies of Generative Models}\label{sec:generative_model}
In this section, we will discuss several taxonomies of generative models that have been employed in the field of GNN explainability. These models aim to learn the probability distribution of the underlying explanatory substructures by training across the entire graph dataset.
\paragraph{Mask Generation (MG)~\cite{PGExplainer, yugraph,GSAT,gnninterpreter}}
The mask generation method is to optimize a mask generator $g_{\theta}$ to generate the edge mask $M$ for the input graph $G$. The elements of the mask represent the importance score of the corresponding edges, which is further employed to select the important substructures of the input graph as explanations. The mask generator is usually a graph encoder followed by a multi-layer perception (MLP), which first embeds edge representations $h_{e_{i}}$ for each edge and then generates the sampling probability $p_{i}$ for edge $e_{i}$. The mask $m_{i}\in \{0,1\}$ of $e_{i}$ is sampled from the Bernoulli distribution $\mathrm{Bern}(p_{i})$. Since the sampling process is non-differentiable, the Gumbel-Softmax trick is usually employed for continuous relaxation as follows:

\begin{equation}\label{eq:gumbel-softmax}
    m_{i}=\sigma ((\log{\epsilon}-\log{(1-\epsilon)}+\log(p_{i})-\log(1-p_i))/\tau), \epsilon \sim \mathrm{Uniform}(0,1)
\end{equation}
where $\tau$ is the temperature, $\sigma$ is the sigmoid function. When $\tau$ goes to zero, $m_i$ is close to the discrete Bernoulli distribution. The explanation $G_{e}$ is obtained by applying the edge mask $M$ to the input graph $G$, \ie $G_{e}=M\odot G=g_\theta(G)\odot G$, where $\odot$ is element-wise multiplication.
Given an input graph $G$ and the desired label $Y^\ast$, the parameter $\theta$ of the mask generator $g_{\theta}$ is optimized by minimizing the following attribution loss:

\begin{equation}\label{eq:MO}
\mathcal{L}_{\textsc{ATTR}} =-\mathbb{E}_{G}\log P_{Y^\ast}(G_e|G,\theta) \hbox{~~with~~} G_e=g_\theta(G)\odot G,
 \end{equation}
which is equivalent to the cross entropy between the output probability $P_f(G_e)$ made by the base GNN $f$ and the desired label $Y^\ast$.


\paragraph{Variational Graph Autoencoder (VGAE)~\cite{GEM,CLEAR,OrphicX}}
Variational Graph Autoencoder (VGAE)~\cite{GVAE} is a variational autoencoder for graph-structured data, where the encoder $q_\phi(\cdot)$ and the decoder $p_\theta(\cdot)$ are typically parameterized by graph neural networks. VGAE can be used to learn the distribution of the underlying explanations and thus generate explanation graphs for unseen instances. The encoder maps an input graph $G$ to a probability distribution over a latent space. The decoder then samples from the latent space and recovers an explanation graph by $G_e=p_{\theta}(z)$. The attribution loss of VGAE for generating explanation graphs is
\begin{equation}\label{eq:loss_GVAE}
    \mathcal{L}_{\textsc{ATTR}}=\mathbb{E}_{z\sim q_\phi(z| G)}-\log P_{Y^\ast}(G_e|\theta, z, G)+\mathrm{D_{KL}}(q_\phi(z | G) \| q(z)).
\end{equation}
The standard VGAE shown in Eq.~\ref{eq:vae} aims to generate realistic graphs. On the contrary, the VGAE-based explainer maximizes the likelihood of the valid explanation graph $G_e$ for the desired label $Y^\ast$. The former term in Eq.~\ref{eq:loss_GVAE} evaluates whether the explanation graph $G_e$ captures the most important structures for $Y^\ast$. It can be replaced by the cross entropy between the output probabilities $P_f(G_e)$ and $Y^\ast$ as CLEAR~\cite{CLEAR}, or the cross entropy between the generated graph $G_e$ and ground-truth explanations as GEM~\cite{GEM}. The second term of KL divergence is a model-specific constraint that drives the posterior distribution $q_\phi(z  | G)$ to the prior distribution $q(z )$, which is usually a Gaussian distribution. 
\paragraph{Generative Adversarial Networks (GAN)~\cite{Gan-Explainer}}
Generative Adversarial Network (GAN) is a type of generative model that does not include an explicit encoder
component. Instead, GANs consist of a generator $g_\theta$ that creates an explanation graph $G_e=g_\theta(z)$ for $z$ sampled from a prior distribution $q(z)$, and a discriminator $d_\phi$ that distinguishes between the input graph $G$ and the generated explanation graph $G_e$. The objective function of a GAN is a min-max game in which the generator $g_\theta$ tries to minimize the function while the discriminator $d_\phi$ tries to maximize it.
\begin{equation}\label{eq:loss_GAN}
    \mathcal{L}_{\textsc{ATTR}} = -\mathbb{E}_{z\sim q(z)}\log P_{Y^\ast}(G_e|\theta, z, G) + \log d_\phi(G) + \mathbb{E}_{z\sim q(z)}\log(1-d_\phi(g_\theta(z))),
\end{equation}
where the first term can be the cross entropy between $P_f(g_\theta(z))$ and the desired label $Y^\ast$. $d_\theta(\cdot)$ denotes the probability that the discriminator predicts that the input is an explanation.  GAN-based Explainer~\cite{Gan-Explainer} was recently proposed with the first term replaced with the square of the difference between the output logit of $f$ for $G$ and $G_e$, \ie $\mathbb{E}_{z\sim q(z)} \left(f(G)-f(g_\theta(z))\right)^2$. Once the GAN is well-trained, the generator $g_\theta$ can be employed to generate valid explanation graphs for any unseen instances, given a point in the prior distribution $q(z)$.

\paragraph{Diffusion~\cite{jeanneret2022diffusion, diffusion_visual_explanation, vignac2022digress}}The Diffusion model is a class of generative models that have been used in graph generation tasks to generate realistic graphs, which contains two key components: the forward diffusion process, and the reverse denoising network. Given an original graph $G_0$, the forward diffusion process progressively generates a sequence of noisy graphs $\{G_0, G_1,\cdots, G_T\}$ with increasing levels of noise, and $G_T$ becomes pure noise. Let $\boldsymbol{A}_t=[\boldsymbol{a}_t^{ij}]_{ij}$ denote the one-hot version of the adjacency matrix of $G_t$ at timestep $t$, where $\boldsymbol{a}_t^{ij}\in\{0,1\}^2$ is a 2-dimensional one-hot encoding of the $ij$-element. The discrete forward diffusion process is defined as $q(\boldsymbol{a}_t^{ij}|\boldsymbol{a}_{t-1}^{ij})=\hbox{Cat}(\boldsymbol{a}_t^{ij}; \boldsymbol{P}=\boldsymbol{a}_{t-1}^{ij}\boldsymbol{Q}_t)$,
where $\hbox{Cat}(\boldsymbol{x}, \boldsymbol{P})$ is a categorical distribution over the one-hot vector $\boldsymbol{x}$ with probability vector $\boldsymbol{P}$ and $\boldsymbol{Q}_t\in[0,1]^{2\times 2}$ is a symmetric transition matrix at timestamp $t$. The forward diffusion is a Markov process that independently performs over all edges in the full adjacency matrix. Therefore, the graph-level diffusion process is $q(G_t|G_{t-1})=\prod_{ij}q(\boldsymbol{a}_t^{ij}|\boldsymbol{a}_{t-1}^{ij})$ and $ q(G_T|G_0)=\prod_{t=1}^{T} q(G_t|G_{t-1})$. The reverse denoising network $g_\theta$ learns to remove the noise and recover the target explanation graph by $G_e=g_\theta(G_t)$ for $t=1,2,\cdots, T$. Since an ideal $G_e$ is a compact subgraph of the original graph $G$, it is equivalent to ensuring that the complementary subgraph $G-G_e$ is close to $G$. Therefore, instead of using the generated graph to reconstruct the original graph in the standard diffusion model, we make $G-G_e$ approximate $G$ and take $G_e$ as the output explanation graph for $G$. The loss function is as follows,
\begin{equation}\label{eq:loss_diffusion}
\mathcal{L} = -\mathbb{E}_{t\in[0,T]}\mathbb{E}_{G_t\sim q(G_t|G_0)}\log P_{Y^\ast}(G_e|\theta, G_t, G_0) + \mathbb{E}_{t\in[0,T]}\mathbb{E}_{G_t\sim q(G_t|G_0)}\mathcal{L}_{\textsc{CE}}(G_0-g_\theta(G_t), G_0).
\end{equation}
The second term denotes the binary cross entropy loss across all elements in the adjacency matrices of $(G_0-g_\theta(G_t)$ and $G_0$. Notably, the diffusion-based explainer naturally involves the \textit{Information} constraint into the optimization objective, as $\mathcal{L}_{\textsc{CE}}$ plays a role of $\mathcal{L}_{\textsc{INFO}}$ 
 that restricts the size of generated explanation graph $G_e$.

\paragraph{Reinforcement Learning Approaches}
Reinforcement Learning (RL) can be used to learn the distribution of underlying explanation graphs by framing the process of generating an explanation graph as a trajectory of step-wise states. Let $\tau=(s_0, \cdots_, s_K)\in\mathcal{T}$ denote a trajectory $\tau$ that consists of states $s_0, \cdots, s_K$ and $\mathcal{T}$ is a set of all possible trajectories. At the $k$-th step, the state $s_k$ refers to a subgraph of the given graph, denoted as $G_k$. $G_0$ is a starting node from the given graph and $G_K$ is the terminal explanation graph. Let $a_k$ denote the action from $s_{k-1}$ to $s_k$, which is usually adding a neighboring edge to the current subgraph $G_{k-1}$. Instead of learning the distribution of the holistic explanation graphs $G_e$, reinforcement learning approaches learn the distribution of the state transition, \ie the distribution of the selected edge to be added given the current state. The objective of these  approaches is to learn a generative agent (policy network) $g_\theta(G_{k-1})$ with parameters $\theta$ that determines the next action by $a_k\sim g_\theta(G_{k-1})$. The reward function is a crucial component of reinforcement learning to address the non-differentiability issue of the sampling process within the generative agent. In the explanation task, the reward function measures the quality of the subgraph $G_k$ for the desired label $Y^\ast$ given the current subgraph $G_{k-1}$. RCExplainer~\cite{RCExplainer} proposes to take the individual causal effect (ICE)~\cite{glymour2016causal} of the action $a_k$ as the reward. GFlowExplainer~\cite{GFlowExplainer} involves the output probability $P_f(G_k)$ over the desired label $Y^\ast$ into the reward design. Reinforcement learning is used in conjunction with another probabilistic model to represent the distribution over the states, \eg Markov Decision Process (MDP), Direct Acyclic Graph (DAG), etc.
\begin{itemize}[leftmargin=*]
\item \xhdr{Markov Decision Process (MDP)~\cite{XGNN, RCExplainer}} The trajectories of states can be framed as a Markov Decision Process. The generative agent $g_\theta(G_{k-1})$ captures the sequential effect of each edge in the generating process toward a target explanation graph. The attribution loss function is as follows,
    \begin{equation}\label{eq:MDP_loss}
        \mathcal{L}_{\textsc{ATTR}}=-\mathbb{E}_{k}[R(G_{k-1}, a_k)]\log P(a_k| \theta, G_{k-1}),
    \end{equation} 
    where $R(G_{k-1},a_k)$ is the reward for the action $a_k$ at state $G_{k-1}$ and $P(a_k| \theta, G_{k-1})$ is the probability of yielding $a_k$ from the distribution $g_{\theta}(G_{k-1})$. This loss function encourages the generative agent to attach higher probabilities to the edges that bring larger rewards, thus leading to an ideal explanation.
    \item \xhdr{Direct Acyclic Graph (DAG)~\cite{GFlowExplainer}} GflowNet~\cite{GFlowNet} frames the trajectories of the states as a direct acyclic graph and aims to train a generative policy network where the distribution over the states is proportional to a pre-defined reward function. A concept of \textit{flow} is introduced to measure the probability flow along the trajectories. Let $F(\tau)$ denote the flow of the trajectory $\tau$ and $F(s)$ denote the flow of the state $s$, which is the sum of all trajectory flows passing through that state. It satisfies that the inflows of a state $s_k$ equals the outflows of $s_k$. The attribution loss is as follows,
    \begin{equation}\label{eq:DAG_loss}
        \mathcal{L}_{\textsc{ATTR}}(\tau)=\sum_{s_{k+1}\in\tau}\left(\sum_{(s_k,a_k)\rightarrow s_{k+1}}F(s_k, a_k)-\mathbbm{1}_{s_{k+1}=s_K}R(s_K) - \mathbbm{1}_{s_{k+1}\neq s_K}\sum_{a_{k+1}}F(s_{k+1},a_{k+1})\right)^2,
    \end{equation}
    where $\sum_{(s_k,a_k)\rightarrow s_{k+1}}F(s_k, a_k)$ and $\sum_{a_{k+1}}F(s_{k+1},a_{k+1})$ denote the inflows and outflows of a state $s_{k+1}$, respectively. $\mathbbm{1}$ is used to check whether $s_{k+1}$ is the terminal state $s_K$.  $R(s_K)$ is the reward of the graph $G_K$ corresponding to the terminal state $s_K$. It is provable that the distribution of the terminal states generated by the agent $P(s_K|\theta)$ trained with Eq.~\ref{eq:DAG_loss} is proportional to their rewards.
\end{itemize}
Typically, reinforcement learning approaches do not rely on an explicit $\mathcal{L}_{\textsc{INFO}}$ to constrain the sparsity of the generated explanation graph. One common strategy is that we create trajectories $\tau=(s_0, \cdots, s_K)$ by iteratively sampling $a_k\sim g_{\theta}(G_{k-1})$ and stop this process once the stopping criteria are attained, \eg $K$ achieves the pre-defined size of explanation graphs. 

%\subsection{Taxonomies of Mutual Information Estimations}
% We discuss three categories of subgraph information constraint.
\subsection{Taxonomies of Information Constraint}\label{sec:info_constraint}

Only maximizing the likelihood of the explanatory subgraph with Eq.~\ref{eq:objective1} typically leads to a trivial solution of the whole input graph, which is unsatisfactory. An ideal explanatory subgraph is supposed to have a small portion of the original graph information as well as be faithful for the prediction.
Hence, existing methods~\cite{GSAT,yugraph} introduce an additional information constraint $\mathcal{L}_{\textsc{INFO}}$ as a regularization term to restrict the information of the generated explanation apart from the attribution loss $\mathcal{L}_{\textsc{ATTR}}$. 
The information constraint $\mathcal{L}_{\textsc{INFO}}$ can be categorized as Size Constraint, Mutual Information Constraint, and Variational Constraint.


\paragraph{Size Constraints \cite{GNNExplainer}}
The size constraint is a straightforward approach to restricting subgraph information. Given an input graph $G$ and the size tolerance $K\in (0,|G|)$, the size constraint is 
 $|G_{e}|\leq K$. Here, $|\cdot|$ denotes the volume of a graph, and $K$ is an integer hyperparameter to constrain the volume of explanatory subgraphs. This constraint is first introduced in GNNExplainer \cite{GNNExplainer}. Since applying the same size constraint to different graph sizes is problematic, 
some work employs the sparsity constraint $k\in (0,1)$ and constrains the volume of explanatory subgraph with $|G_{e}|\leq k\cdot |G|$. Recent works~\cite{CLEAR, OrphicX} further utilize a soft version of size constraint, \ie $\mathcal{L}_{\textsc{INFO}}=d(G, G_e)$, where $d(G, G_e)$ is the element-wise distance between the adjacency matrices of $G$ and $G_e$.
 
Although the size constraint is intuitive, it has several limitations. Firstly, the topological size is insufficient in measuring the subgraph information as they ignore the information within node and edge features. Secondly, one has to choose different sparsity tolerance $k$ to achieve the best explanation performance, which is difficult due to the trade-off between the sparsity and validity of the explanations. 
% \jialin{discuss $d(G, G-G_e)$ and $d(G,G_{ce})$, which should be a soft size constraints?}


\paragraph{Mutual Information Constraint \cite{yugraph}}
The mutual information constraint restricts the subgraph information by reducing the relevance between the original graphs and the explanatory subgraphs.
Given the graph $G$ and the explanatory subgraph $G_{e}$, the mutual information constraint is formulated as: \begin{equation}\label{eq:MI-constrain}
\mathcal{L}_{\textsc{INFO}}=MI(G,G_{e})=\mathbb{E}_{p(G)}\mathbb{E}_{p(G_{e}|G)}\log{\frac{p(G_{e}|G)}{p(G_{e})}}.
\end{equation}
Here $MI(x,y)$ is the mutual information of two random variables. Minimizing Eq.~\ref{eq:MI-constrain} reduces the relevance between $G$ and $G_{e}$. Thus, the explanation generator tends to leverage limited input graph information to generate the explanation subgraph. 
Compared with the size constraints, the mutual information constraint is more fundamental in information measurement and flexible to different graph sizes.
However, mutual information is intractable to compute, making the constraint impractical to use. One solution is resorting to computationally expensive estimation techniques, such as the Donsker-Varadhan representation of mutual information~\cite{Mine, yugraph}.


\paragraph{Variational Constraint~\cite{GSAT}} Since the mutual information constraint is intractable, the variational constraint is proposed by deriving a tractable variational upper bound of the mutual information constraint. One can plug a prior distribution $q(G_{e})$ into $MI(G,G_{e})$ as the variational approximation to $p(G_e|G)$:
\begin{equation}\label{eq:variational-constraint}
\begin{aligned}
MI(G,G_{e})&=\mathbb{E}_{p(G)}\mathbb{E}_{p(G_{e}|G)}\log{\frac{p(G_{e}|G)}{p(G_{e})}}=\mathbb{E}_{p(G)}\mathbb{E}_{p(G_{e}|G)}\log{\frac{p(G_{e}|G)}{q(G_{e})}} -\mathrm{D_{KL}}(q(G_{e})\|p(G_{e})) \\
&\leq \mathbb{E}_{p(G)}\mathbb{E}_{p(G_{e}|G)}\log{\frac{p(G_{e}|G)}{q(G_{e})}}
:=\mathcal{L}_{\textsc{VC}}.
\end{aligned}
\end{equation}
Here, the inequality is due to the non-negative nature of the Kullback–Leibler (KL) divergence. The posterior distribution $p(G_{e}|G)=\prod_{i=1}^{N}p(e_{i}|\theta)$ is factorized into the multiplication of the marginal distributions of edge sampling $p(e_{i}|\theta)$, which is parameterized with the generative explanation network $\theta$. The prior distribution $q(G_{e})=\prod_{i=1}^{N}q(e_{i})$ is factorized into the prior distributions of edge sampling $q(e_{i})$. $N$ is the total edge number. In practice, $q(e_{i})$ is usually chosen as the Bernoulli distribution or Gaussian distribution. Thus, the variational constraint is an upper bound of the mutual information $MI(G, G_e)$ that can be simplified as
\begin{equation}\label{eq:simplified-variational-constraint}
\begin{aligned}
\mathcal{L}_{\textsc{INFO}}=\mathcal{L}_{\textsc{VC}}= \sum_{i=1}^{N}\mathrm{D_{KL}}(p(e_{i}|\theta)\|q(e_{i})).
\end{aligned}
\end{equation}




\subsection{Extension of Explanation Scenarios}\label{sec:extention_case}

\paragraph{Counterfactual Explanation} Most explanation methods focus on discovering the prediction-relevant subgraph to explain GNNs based on Eq.~\ref{eq:objective1}. 
Although these methods can highlight the important substructures for the predictions, they cannot answer the \textit{counterfactual} problem such as: "Will the removal of certain substructure lead to prediction change of GNNs?"
Counterfactual explanations provide insightful information on how the model prediction would change if some event had occurred differently, which is crucial in some real-world scenarios, \eg drug design and molecular modification~\cite{drug_cf, drug_XAI, molecule_cf}. 
Given an input graph $G$, the goal of the generative counterfactual explanation is to train a subgraph generator $g_{\theta}$ to generate a minimal substructure of the input.
If the substructure is removed, the prediction of GNNs will change the most. 
Formally, the objective for counterfactual explanations is:
\begin{equation}\label{eq:counterfactual}
    \mathcal{L}_{\textsc{CF}}=-\log{P_{Y^\ast}(G_{ce}|\theta,G)} + \mathcal{L}_{\textsc{INFO}}(G, \overline{G_{ce}})
\end{equation}
Here, $\overline{G_{ce}}$ denotes the generated substructure, which is also the modification applied to $G$ to obtain a counterfactual explanation $\overline{G_{ce}}=G-G_{ce}$. $\mathcal{L}_{\textsc{INFO}}$ is a regularization term that constrains the information amount contained in $\overline{G_{ce}}$ to be minimal compared with the input graph $G$.


\xhdr{Connection to Graph Adversarial Attack} The counterfactual explanation methods can capture the vulnerability of GNN's prediction since the counterfactual explanation subgraph leads to the prediction change. This problem setting is similar to graph adversarial attacks as they both aim to alter the prediction behavior of the pre-trained GNN by modifying testing graphs. Recall that graph adversarial attacks modify the node features or graph structures to decrease the average performance of a pre-trained GNN. However, the explanation methods change the prediction of each testing sample by instance-level subgraph deletion instead of decreasing the overall testing performance after a one-time graph modification~\cite{model_attack_cf}.

\xhdr{Connection to Graph Out-of-distribution Generalization}
Deep learning models have been found to rely on spurious patterns in the input to make predictions. These patterns are often unstable under distribution shifts, leading to a drop in performance when testing on out-of-distribution (OOD) data. To address this issue, counterfactual augmentation has been proposed as an effective method for improving the OOD generalization ability of deep learning models. This technique involves minimally modifying the training data to change their labels and training the model with both the original and counterfactually augmented data. For graphs, counterfactually augmented subgraphs can be generated by removing subgraphs to create the complementary subgraph, which is a natural form of counterfactual augmentation. However, this approach has received less attention in the context of graph neural networks, presenting an avenue for future research.

\paragraph{Model-level Explanation}
The goal of model-level explanation in the context of GNNs is to identify important graph patterns that contribute to the decision boundaries of the model. Unlike instance-level explanation, model-level explanation provides insights into the general behavior of the model across a range of input graphs with the same predicted label. A brute-force approach to finding these patterns is to mine the subgraphs that commonly appear in graphs with the same predicted label. However, this is computationally expensive due to the exponentially large search space. Recently, generative methods have been proposed to generate model-level explanations, such as reinforcement learning~\cite{XGNN} and probabilistic generative models~\cite{gnninterpreter}. 

In these approaches, a generator function $g_\theta(\cdot)$ is used to generate the model-level explanation $G_m$ for a given predicted label $Y^\ast$ based on a set of graphs $\mathcal{G}$ via $G_m\sim g_\theta(\mathcal{G}, Y^\ast)$. The optimization objective for the generator function is to minimize the negative log-likelihood of the explanation given the graphs, while also ensuring that the generated explanation is a compact and recurrent substructure in the set of input graphs:
\begin{equation}\label{eq:model-level_loss}
    \mathcal{L}=-\log P_{Y^\ast}(G_m|\theta, \mathcal{G}) + \mathcal{L}_{\textsc{INFO}}(G_m, \mathcal{G}).
\end{equation}
The first term in Eq.~\ref{eq:model-level_loss} measures whether $G_m$ captures the most determinant graph patterns for the prediction of $Y^\ast$. The generator $g_\theta(\cdot)$ can be modeled by other applicable generative models discussed in Sec.~\ref{sec:generative_model}. $\mathcal{L}_{\textsc{INFO}}(G_m,\mathcal{G})$ ensures that $G_m$ is a compact substructure that commonly appears in $\mathcal{G}$.

% \rex{how about extensions to link prediction? extensions to heterogeneous graphs?}

% \rex{we also need relation to graph generative models. It can be here or section 2}