\section{Introduction}
Graph Neural Networks (GNNs) have emerged as a powerful tool for studying graph-structured data in various applications, such as social networks, drug discovery, and recommendation systems~\cite{GNN_application, GNN_anomaly, GNN_financial, GNN_financial_time_series,GNN_molecule, GNN_molecule_review, GNN_socialnetwork}. The explainability and trustworthiness of GNNs are crucial for their successful deployment in real-world scenarios, especially in high-stake applications, such as anti-money laundering, fraud detection, and healthcare forecasting~\cite{Graph-SST2,explainability_GCN, Evaluate_explainability}. Explanations for GNNs aim to discover the reasoning logic behind their predictions, making them more understandable and transparent to users. Explanations also help identify potential biases and build trust in the decision-making process of the model. Furthermore, they aid users in understanding complex graph-structured data, leading to improved outcomes in various applications through better feature extraction~\cite{survey_zhang,Dai:survey,Survey_trustworthy}.

Numerous explanation methods have been extensively studied for GNNs, including gradient-based attribution methods~\cite{Grad-CAM-Graph, SA-Graph, IG}, perturbation-based methods~\cite{GNNExplainer, ReFine, SubgraphX,GraphMask, GraphLIME}, \textit{etc.} However, most of these methods optimize individual explanations for a specific instance, lacking global attention to the overall dataset and the ability to generalize to unseen instances. To tackle this challenge, generative explainability methods have emerged recently, which instead formulate the explanation task as a distribution learning problem. Generative explainability methods aim to learn the underlying distributions of the explanatory graphs across the entire graph dataset~\cite{RCExplainer, GFlowExplainer, CLEAR, XGNN}, providing a more holistic approach to GNN explanations.

Current surveys in the field of Graph Neural Networks (GNNs) explainability primarily focus on the taxonomy and evaluation of explanation methods~\cite{Graph-SST2, Evaluate_explainability, Survey_counterfactual}, as well as broader trustworthy aspects such as robustness, privacy, and fairness~\cite{survey_cyber, survey_zhang, Survey_li, Survey_trustworthy}. 
%However, these surveys lack a comprehensive and systematic overview of the strengths and limitations associated with these explainability methods. In particular, 
The emerging generative explainability methods prompt us to consider the potential advantages of incorporating distribution learning into the optimization objective, such as better explanation efficiency and generalizability.


% \rex{the however statement is a bit too broad. we can mention more concrete advantages of a generative approach that these surveys didn't investigate, such as generalization}

Our work stands apart from previous works by thoroughly investigating the different mechanisms for generating explanations. We explore a comprehensive range of graph generation techniques that have been employed in GNN explanation tasks, including cutting-edge techniques such as the Variational Graph Autoencoder (VGAE) and denoising diffusion models. Our study begins by elucidating the core design considerations of different generative models and employs a novel generative perspective to unify a group of effective GNN explanation approaches. The key insight lies in a unified optimization objective, which includes an \textit{Attribution} constraint and an \textit{Information} constraint to ensure that the generated explanations are sufficiently succinct and relevant to the predictions. We subsequently delve into the details of the practical designs of the \textit{Attribution} and \textit{Information} constraints to facilitate the analysis of the connections and potential extensions of current generative explainability methods. The proposed unified optimization objective also empowers GNN users to efficiently design effective generative explainability methods.

Comprehensive experiments on synthetic and real-world datasets demonstrate the advantages and drawbacks of these existing methods. Specifically, our results show that generative approaches are empirically more efficient during the inference stage. Meanwhile, generative approaches achieve the best generalization capacity compared to other non-generative approaches.

This paper is structured as follows. First, we introduce the notations and problem setting in Sec.~\ref{sec:pre}. Then, we propose a standard optimization objective with \textit{Attribution} constraint and \textit{Information} constraint to unify generative explanation methods in Sec.~\ref{sec:generative_formulation}. Detailed expressions of these two constraints are elaborated in Sec.~\ref{sec:generative_model} and Sec.~\ref{sec:info_constraint}, respectively. In Sec.~\ref{sec:extention_case}, we discuss how to generalize the proposed framework to extensive explanation scenarios, \eg counterfactual and model-level explanations. Additionally, we present a taxonomy of representative works with different generative backbones in Sec.~\ref{sec:taxonomy}. Finally, we conduct comprehensive evaluations and demonstrate the potential of deep generative methods for GNN explanation in Section~\ref{sec:experiment}.

% \rex{A paragraph summarizing the results}






% GNNs are widely used. Explanations for GNNs are essential in applications (cite). Common usage of GNN explainability focuses on instance-level, model-level, and counterfactual explanations.

% Many explainability methods have been developed (cite). However, they utilize a variety of approaches, such as ... (also cite some papers that are not generative models), and thelacksack a unified framework to describe the most effective approaches for GNN explanations under the instance-level, model-level and counterfactual settings. (use a figure to illustrate the challenge)

% There have been extensive survey studies regarding GNN explainability methods. CITE categorizes the methods into XXX. ... (how we are different) They do not investigate the desired properties of high quality GNN explanations such as XXX

% Here we unify a group of effective GNN explainability approaches through a novel generative perspective. The key insight is (sth about Eq 1 / 2)...
% The objective contains XX components: ...

% Advantages of our framework (how we addressed the challenges of many explainability methods). Key insights we can extract from comparison of various generative models for generating explanations.
% \rex{can we claim that we are the focusing on deep explanation models? Are there explainability methods using deep learning but does not use a generative approach?}

% Evaluation.


