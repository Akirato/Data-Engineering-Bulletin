\documentclass[11pt]{article}

\usepackage{deauthor,times,graphicx}
%\usepackage[square,comma,numbers,sort]{natbib}
\usepackage{url}
\urlstyle{same}
\usepackage{hyperref}
\usepackage{multirow,multicol,xspace}
\usepackage{amsmath,amssymb}
\usepackage{float}
\usepackage{booktabs}
\usepackage{graphics}
\usepackage{pifont}% http://ctan.org/pkg/pifont
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

% \definecolor{mydarkblue}{rgb}{0,0.08,0.45}
% \definecolor{myblue}{HTML}{3b75c3}
% \definecolor{myred}{HTML}{E33222}
% \definecolor{mygreen}{HTML}{438773}
% \definecolor{mymaroon}{RGB}{142,27,19}
% \definecolor{maroon}{HTML}{800000}
% \definecolor{mycite}{cmyk}{0.55,1,0,0.15}
\definecolor{codeblue}{rgb}{0.25,0.5,0.5}
% \definecolor{codekw}{rgb}{0.85, 0.18, 0.50}
% \definecolor{codegreen}{rgb}{0,0.6,0}
% \definecolor{codegray}{rgb}{0.5,0.5,0.5}
% \definecolor{codepurple}{rgb}{0.58,0,0.82}
% \definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\hypersetup{
%     colorlinks=true,
    % citecolor=green,
%     urlcolor=blue,
%     linkcolor=red
    colorlinks=true,
    citecolor=codeblue,
    % linkcolor=mymaroon,
    % urlcolor=maroon,
    bookmarksnumbered=true,
    pdfpagemode={UseOutlines},
    plainpages=false,
    pdftitle={Graph Data Augmentation for Graph Machine Learning: A Survey},
    pdfauthor={T. Zhao, et al.}
}

\setlength{\bibsep}{2.5pt}

\renewcommand\paragraph[1]{\vspace{0.05in} \noindent \textbf{#1.}}
\newcommand*{\Scale}[2][4]{\scalebox{#1}{$#2$}}


\bulletinyear{2023}

\begin{document}
\title{Graph Data Augmentation for Graph Machine Learning:\\ A Survey}
\author
{Tong Zhao$^{1,4}$, Wei Jin$^2$, Yozen Liu$^1$, Yingheng Wang$^3$, Gang Liu$^4$, \\ 
Stephan GÃ¼nnemann$^5$, Neil Shah$^1$, Meng Jiang$^4$ \\
\small{$^1$Snap Inc., $^2$Michigan State University, $^3$Cornell University,} \\
\small{$^4$University of Notre Dame, $^5$Technical University of Munich} \\
\small\texttt{$^1$\{tzhao,yliu2,nshah\}@snap.com,$^2$jinwei2@msu.edu,$^3$yw2349@cornell.edu,}\\
\small\texttt{$^4$\{gliu7,mjiang2\}@nd.edu,$^5$guennemann@in.tum.de}
}

\maketitle
\renewcommand\thesection{\arabic{section}}
\setcounter{section}{0}
\setcounter{figure}{0}
\setcounter{table}{0}


\begin{abstract}
Data augmentation has recently seen increased interest in graph machine learning given its demonstrated ability to improve model performance and generalization by added training data. Despite this recent surge, the area is still relatively under-explored, due to the challenges brought by complex, non-Euclidean structure of graph data, which limits the direct analogizing of traditional augmentation operations on other types of image, video, or text data.  Our work aims to give a necessary and timely overview of existing graph data augmentation methods; notably, we present a comprehensive and systematic survey of graph data augmentation approaches, summarizing the literature in a structured manner. We first introduce three different taxonomies for categorizing graph data augmentation methods from the data, task, and learning perspectives, respectively. Next, we introduce recent advances in graph data augmentation, differentiated by their methodologies and applications. We conclude by outlining currently unsolved challenges and directions for future research. Overall, our work aims to clarify the landscape of existing literature in graph data augmentation and motivates additional work in this area, providing a helpful resource for researchers and practitioners in the broader graph machine learning domain. Additionally, we provide a continuously updated reading list at \url{https://github.com/zhao-tong/graph-data-augmentation-papers}. 
\end{abstract}

\section{Introduction}
\label{sec:tong_intro}
\noindent Data driven inference has received a significant boost in generalization capability and performance improvement in recent years from data augmentation (DA) techniques. DA techniques increase the amount of training data by creating plausible variations of existing data without additional ground-truth labeling efforts, and have seen widespread adoption in fields such as computer vision (CV)~\cite{cubuk2019autoaugment} and natural language processing (NLP)~\cite{feng2021survey}. These techniques allow machine learning models to learn to generalize across those variations and attend to signal over noise.
In recent years, with the rapid development of graph machine learning (GML) methods such as graph neural networks (GNNs)~\cite{kipf2016semi,hamilton2017inductive}, studies have shown that the effectiveness of GML approaches also largely depends on the data quality. Given the dependent nature of graph data and the message-passing design of most GNNs, GML faces unique challenges such as: structural data sparsity brought by power-law degree distributions in most graphs, noisy and even erroneous topology brought by imperfect construction of the graph structure from raw data under other formats, low quality and incomplete node attributes, adversarial attacks on structure and attributes, lack of labelled data due to costly human annotations, and over-smoothing caused by the message passing design in GNNs.
As DA allows researchers to alleviate such challenges from a data perspective, there has been increased interest and demand for such techniques on graph data~\cite{zhao2021data}, and there has been a growing number of works on graph data augmentation (GDA).

With the irregular and non-Euclidean structure of graph data, it is non-trivial to directly analogize DA techniques from CV and NLP to the graph domain, except for the most basic operations such as random masking/dropping/cropping. To better promote the effectiveness of GML approaches and alleviate the unique challenges in GML, recent literature designed graph-specific augmentation techniques following methodologies such as graph structure learning, graph adversarial training, graph rationalization, etc.
Creating a unified taxonomy for all GDA techniques is not intuitive as they can be categorized under different facets. 
For example, taking the data modelity that the augmentation methods work on, they can be separated into structure augmentations, feature augmentations, and label augmentations. On the other hand, the focusing downstream tasks (i.e., node-level, edge-level, and graph-level tasks) can also categorize the GDA techniques. Moreover, the GDA methods can also be separated by whether the methods involves learning during the augmentation process. That is, whether they are rule-based approaches or learned approaches.

This paper aims to sensitize the GML community towards this growing area of work, as DA has already drawn much attention in CV and NLP~\cite{cubuk2019autoaugment,feng2021survey}.
As interest and work on this topic continue to increase, this is an opportune time for a comprehensive work to (i) introduce background and motivation of GDA, (ii) give a bird's-eye view of existing GDA techniques under different taxonomies, (iii) introduce representative GDA techniques with their usage and applications, and (iv) identify key challenges to effectively motivate and orient interest in this area.
We hope this survey can serve as a guide for researchers and practitioners who are new to or interested in studying this topic, and also inspire future research in this area.

The text is structured as follows: Section~\ref{sec:tong_background} gives background and motivation on GNNs and GDA. It defines GDA and motivates its use in GML tasks.
Section~\ref{sec:tong_taxonomy} categorizes GDA techniques based on three different taxonomies: the operated graph data, the downstream tasks, and whether the method involves learning. Section~\ref{sec:tong_simpleaug} describes rule-based GDA techniques for GML -- which we partition into Data Removal (Section~\ref{sec:tong_dataremove}), Data Addition (Section~\ref{sec:tong_dataadd}), and Data Manipulation (Section~\ref{sec:tong_datamani}) focuses. Similarly, Section~\ref{sec:tong_learnedaug} introduces learned GDA techniques, which are further categorized by their methodologies: Graph Structure Learning (Section~\ref{sec:tong_gsl}), Graph Adversarial Training (Section~\ref{sec:tong_adv}), Graph Rationalization (Section~\ref{sec:tong_rat}), and Automated Augmentation (Section~\ref{sec:tong_autoaug}).
Section~\ref{sec:tong_application} introduces GDA techniques that are used under three different self-supervised learning objectives: Contrastive Learning (Section~\ref{sec:tong_contrastive}), Non-contrastive Learning (Section~\ref{sec:tong_noncontrastive}), and Consistency Training (Section~\ref{sec:tong_consistent}). Finally, Section~\ref{sec:tong_future} discusses challenges and future directions for GDA.

\begin{figure}[!th]
\centering
\resizebox{0.65\linewidth}{!}{
\begin{forest}
	for tree={
		draw,
		shape=rectangle,
		rounded corners,
		top color=white,
		grow'=0,
		l sep'=1.2em,
		reversed=true,
		anchor=west,
		child anchor=west,
	},
	forked edges,
	root/.style={
		rotate=90, shading angle=90, bottom color=red!40,
		anchor=north, font=\normalsize, inner sep=0.5em},
	level1/.style={
		bottom color=blue!30, font=\normalsize, inner sep=0.3em,
		s sep=0.2em},
	level2/.style={
		bottom color=orange!40, font=\small, inner sep=0.25em,
		s sep=0.1em},
	level3/.style={
		bottom color=teal!40, font=\small, inner sep=0.2em,
		l sep'=0.5em},
	where n=0{root}{},
	where level=1{level1}{},
	where level=2{level2}{},
	where level=3{level3}{},
	[Graph Data Augmentation
            [Backgrounds (\S \ref{sec:tong_background})]
		[Taxonomies (\S \ref{sec:tong_taxonomy})
                [Data Perspective
                    [Structure Augmentations]
                    [Feature Augmentations]
                    [Label Augmentations]
                ]
                [Task Perspective
                    [Node-level Tasks]
                    [Graph-level Tasks]
                    [Edge-level Tasks]
                ]
                [Learning Perspective
                    [Rule-based GDA]
                    [Learned GDA]
                ]
		]
		[Rule-based GDA (\S \ref{sec:tong_simpleaug})
			[Data Removal]
			[Data Addition]
			[Data Manipulation]
		]
		[Learned GDA (\S \ref{sec:tong_learnedaug})
			[Graph Structure Learning]
			[Graph Adversarial Training]
                [Graph Rationalization]
                [Automated Augmentation]
		]
		[GDA in Self-supervsed Learning (\S \ref{sec:tong_application})
			[Contrastive Learning]
			[Non-contrastive Learning]
			[Consistency Training]
		]
            [Challenges and Directions (\S \ref{sec:tong_future})
                [Domain Adaptation and Regularization]
                [Scalability for Large-scale graphs]
                [Comprehensive Evaluation Criteria and Standards]
                [Theoretical Foundation]
            ]
	]
\end{forest}
}
\caption{Structure of this survey.}
\label{fig:overview}
\end{figure}

\section{Preliminaries}
\label{sec:tong_background}
\subsection{Notations}
Let $G = (\mathcal{V}, \mathcal{E})$ be a graph of $N$ nodes, where $\mathcal{V} = \{v_1, v_2, \dots, v_N\}$ is the set of $N$ nodes and $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ is the set of links. We denote the adjacency matrix as $\mathbf{A} \in \{0, 1\}^{N \times N}$, where $A_{i,j} = 1$ indicates nodes $v_i$ and $v_j$ are connected and vice versa. We denote the node feature matrix as $\mathbf{X} \in \mathbb{R}^{N \times F}$, where $F$ is the number of raw node features and $\boldsymbol{x}_{i}$ indicates the feature vector of node $v_i$ (the $i$-th row of $\mathbf{X}$). We use $\boldsymbol{y}$ to denote the label each sample, which can be node, edge, or graph depending on the task. We use symbol with tilde to denote the data generated by GDA methods. For example, $\tilde{\mathbf{A}}$ for the augmented adjacency matrix, $\tilde{\boldsymbol{x}}_i$ for the augmented feature vector of node $v_i$,  etc.


\subsection{Graph Neural Networks}

Graph neural networks (GNNs) enjoy widespread use in modern graph-based machine learning due to their flexibility to incorporate node features, custom aggregations, and inductive operation, unlike earlier works which were based on embedding lookups~\cite{perozzi2014deepwalk,grover2016node2vec}. Following the initial idea of convolution based on spectral graph theory~\cite{bruna2013spectral}, many spectral GNNs have since been developed and improved by~\cite{defferrard2016convolutional,kipf2016semi,levie2018cayleynets,klicpera2018predict,ma2021unified}. As spectral GNNs generally operate (expensively) on the full adjacency, spatial-based methods which perform graph convolution with neighborhood aggregation became prominent~\cite{hamilton2017inductive,velivckovic2017graph}, owing to their scalability and flexibility~\cite{ying2018graph,wu2020comprehensive}. 

Generally, the generic formulation of message passing-based GNNs can be defined by an aggregation function (\textsc{Aggregate}) and an update function (\textsc{Update}). In each layer, \textsc{Aggregate} aggregates the embeddings from previous layer for each node from all its neighbors, and \textsc{Update} updates each node's embedding by combining its own previous embedding and the aggregated neighbor embeddings \cite{hamilton2017inductive}. Specifically,
\begin{equation}
\label{eq:gnn}
\begin{split}
    \boldsymbol{h}_{\mathcal{N}(v)}^l &= \textsc{Aggregate}(\{\boldsymbol{h}_u^{l-1} | u \in \mathcal{N}(v) \}), \\
    \boldsymbol{h}_v^l &= \textsc{Update} (\boldsymbol{h}_v^{l-1}, \boldsymbol{h}_{\mathcal{N}(v)}^l),
\end{split}
\end{equation}
where $\boldsymbol{h}_v^l$ denotes the representation of node $v$ at the $l$-th layer, and $\mathcal{N}(v)$ denotes the set of node $v$'s neighbors.

In implementation, GNNs can usually be implemented with (sparse) matrix multiplications. Without the loss of generality, here we take the most commonly used Graph Convolutional Network (GCN)~\cite{kipf2016semi} as an example. One layer of GCN is defined as
\begin{equation}
\label{eq:gcn}
    \mathbf{H}^l = \sigma(\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}\mathbf{W}^l\mathbf{H}^{l-1}),
\end{equation}
where $\mathbf{D}$ is the diagonal degree matrix s.t. $\mathbf{D}_{i,i} = \sum_j \mathbf{A}_{i,j}$ (assuming $\mathbf{A}$ contains self-loops), $\sigma(\cdot)$ is the nonlinear activation function such as ReLU, and $\mathbf{W}^l$ denotes the learnable weight matrix at the $l$-th GNN layer. Furthermore, we use $g_\Theta(\cdot)$ to denote the mapping function of the whole GNN model parameterized by $\Theta$.

\subsection{Graph Data Augmentation}

The DA area encompasses techniques of increasing/generating training data without directly collecting or labeling more data. Most DA techniques either add slightly modified copies of existing data, or generate synthetic data based on existing data. The augmented data act as a regularizer and reduce overfitting when training data-driven models~\cite{shorten2019survey}. DA techniques has been commonly used in CV~\cite{cubuk2019autoaugment} and NLP~\cite{feng2021survey}, where augmentation operations such as cropping, flipping, and back-translation are usually used in machine learning model training. 

In GML, in contrast to regular and Euclidean data such as grids (e.g., images) and sequences (e.g., sentences), the graph structure is encoded by node connectivity, which is non-Euclidean and irregular. Most structured augmentation operations used frequently in CV and NLP cannot be easily analogized to graph data. Therefore, how to design effective augmentations of graph data is less obvious. For example, the data objects for node-level and edge-level tasks are inter-connected and non-i.i.d, meaning that GDA techniques typically modify the entire dataset (graph) instead of a specific data object (nodes or edge) in isolation. Generally, a GDA method can be defined as a transformation function $f: G \rightarrow \tilde{G}$, where the the transformation function $f$ can be either rule-based or learnable, and the augmented graph $\tilde{G}$ contains the augmented adjacency matrix $\tilde{\mathbf{A}}$ and node feature matrix $\tilde{\mathbf{X}}$ (and optionally augmented edge features, node or graph labels). Moreover, the augmentation function $f$ is not necessarily deterministic. That is, the same $f$ may generate multiple different versions of the augmented graph $\tilde{G}$, and the model may use one or multiple of these augmentations as required for training.



\subsection{Motivation: Why Augment Graphs?}
Graphs are often utilized to model or represent an underlying process of relationships or affinities; for example, ``which individuals are friends with one another?'' or ``which movies do individuals like?''  In some cases, these relationships are strictly defined and known, e.g. researchers jointly co-authoring articles, or atoms interacting in a chemical compound.  However, in many other cases, an ``observed'' graph may be misaligned with the true process it intends to model for a variety of reasons \cite{brugere2018network}.  In some cases, like in social interaction graphs, noise may be inadvertently or adversarially introduced by spammers who pollute underlying data about authentic interactions with inauthentic ones for nefarious purposes \cite{shah2014spotting, kumar2018false}.  In other cases, noise may be inherently created by limited or partial observation (e.g. a movie recommendation system never recommending a certain genre of movies to a group of users) caused by privacy reasons~\cite{chierichetti2015efficient,duong2011modeling}, 
biased recommendation policies \cite{joachims2016counterfactual, zhao2021counterfactual}, or other reasons.  Noise can also occur by measurement or thresholding errors (e.g. discretizing continuous signals between brain voxels into discrete ones) \cite{garrison2015stability}, or human errors (e.g. a person forgetting to add a known contact to their phone's contact-book).  All of these scenarios can introduce gaps between an intended and observed graph.  Moreover, even if all relationships a graph intends to capture are observed properly, there is no guarantee that the graph is a particularly useful \cite{brugere2018network} for a particular downstream learning task, especially when utilized in a GML context, e.g. a graph connecting individuals by similar heights may be unhelpful in regressing income.

GDA methods offer an attractive solution in denoising, imputing, and generally enhancing graph structure to align better with an intended modeling processes, or objectives of a target learning task \cite{zhao2021data}.  Adding or removing nodes and edges can help connect or disconnect a graph to facilitate its use towards targeted objectives.  Moreover, utilizing heuristic graph modification strategies to increase model exposure in training may lead to better generalizing, more robust and higher performance models \cite{wang2020nodeaug, kong2020flag, zhou2020data}.  Both learned and rule-based DA techniques have shown immense potential in other domains like tabular ML (e.g. oversampling \cite{barandela2004imbalanced} and SMOTE \cite{chawla2002smote}),  CV (e.g. rotations, flips and translations of images \cite{shorten2019survey} and random erasure \cite{zhong2020random}) and NLP (e.g. synonym replacement and random token additions/deletions \cite{wei2019eda} and back-translation \cite{sennrich2015improving}); however, as aforementioned, these techniques usually lack clear analogs in the graph domain due to unclear correspondence of their label-preserving transforms.  This lack of clarity motivates work into understanding the limitations of graphs, suitable designs for augmentation techniques, and their breadth of impact. 

\section{Taxonomies}
\label{sec:tong_taxonomy}

In this section, we introduce three different taxonomies that can be used to categorize GDA techniques. They come from different perspectives of data, task, and learnability, respectively. As these taxonomies are orthogonal to each other, and each of them can in some way categorize all GDA methods, we will only focus on one taxonomy (rule-based vs. learned augmentation) for the later sections.

\subsection{Operated Data Modality}
As GDA methods all operate on graph data, they can naturally be categorized by the data modality that they aim to manipulate. Therefore, one intuitive taxonomy for GDA methods would be classifying them into one or more of three categories: structure, feature, and label augmentations.

% \noindent 
\textbf{Structure Augmentations} 
are the GDA operations that modify the graph connectivity via adding/removing edges or adding/removing nodes from the graph. The modifications can be either deterministic (e.g., GDC~\cite{klicpera2019diffusion} and GAug-M~\cite{zhao2021data} both modify the graph structure and used the modifed graph for training/inferencing) or stochastic (e.g., DropEdge\cite{rong2019dropedge} and DropNode~\cite{feng2020graph} randomly drop edges/nodes from the observed training graph). 
% \noindent 
\textbf{Feature Augmentations} 
are the GDA operations that modify or create raw node features. For example, \citet{you2020graph} used Attribute Masking that randomly masked off node features; FLAG~\cite{kong2020flag} augments node features with gradient-based adversarial perturbations. It's worth noting that stucture augmentations and feature augmentations are also sometimes combined in some GDA methods. For example, MoCL~\cite{sun2021mocl} substitutes subgraphs in molecular graphs with subgraphs of different functional groups.
% \noindent 
\textbf{Label Augmentations} 
are the GDA operations that involves modifying the labels. For example, Mixup-based methods~\cite{han2022G,guo2021intrusion} interpolate existing training examples and assign new label for the generated example. Counterfactual data augmentation methods (e.g., CFLP~\cite{zhao2021counterfactual}) generate counterfactual examples with corresponding new labels.

\subsection{Downstream Tasks}
Another straightforward taxonomy of categorizing GDA methods is by the downstream tasks that they tackle. Generally, most GML methods can be categorized into three high-level task types: \textbf{node-level}, \textbf{edge-level}, and \textbf{graph-level} tasks. 
Similarly, many GDA methods are designed toward one of these tasks, and cannot be easily generalized to other tasks. For example, CFLP~\cite{zhao2021counterfactual} generates counterfactual links as augmented data specifically for training a neural link predictor, and these counterfactual links are useless to other tasks like node classification as they are counterfactual labels on node pairs under specific treatments. Moreover, certain GDA methods that are designed for molecular graphs (e.g., MoCL~\cite{sun2021mocl}) are not opeartable on the large graph datasets used in other tasks as they rely on the domain specific substructures of molecular graphs, e.g., functional groups. Nonetheless, the downside of categorizing by downstream tasks is that a fair number of GDA methods were designed more generically for various tasks; for example, DropEdge~\cite{rong2019dropedge} simply conducts random edge dropping during training, and the method can naturally be applied on most GML methods.

\subsection{Rule-based vs. Learned Augmentations}

GDA methods can also be categorized by whether the augmentation process involved learning, namely rule-based GDA approaches and learned GDA approaches. More specifically, \textbf{rule-based GDA approaches} refer to the non-learnable methods that modify or manipulate the graph data following pre-defined rules, which can be stochastic, deterministic, or mixture of both. A rule-based GDA method can be as simple as randomly removing a given fraction of edges~\cite{rong2019dropedge} or randomly cropping out part of the graph~\cite{you2020graph}; it can also be more complicated such as counterfactual augmentation~\cite{zhao2021counterfactual} based on similarity matching rules and graph diffusion methods~\cite{klicpera2019diffusion} that follows specific diffusion kernels. We also categorize Mixup-based augmentations~\cite{han2022G} as rule-based approaches since they usually only contain one non-learnable parameter (sampled from pre-defined distributions) when generating new data objects by interpolating two existing data objects. 

On the other hand, \textbf{learned GDA approaches} refer to the augmentation methods that contains learnable parameters in the process of generating augmented examples. The augmentation module can either be trained independently or in an end-to-end style with the downstream classifier or regressor~\cite{zhao2021data}. For example, graph structure learning methods~\cite{zhu2021survey,jin2020graph,zhao2021data} often assume the observed graph data is noisy, incomplete, or entirely missing, so they first try to learn the ``clean'' graph structure before using it in the training and inference of GNNs. Graph rationalization methods~\cite{wu2021discovering,liu2022graph} learn subgraphs that are likely to be causally related with the graph labels and use them for augmentation. Automated augmentation methods~\cite{zhao2022autogda,luo2022automated} utilize reinforcement learning agents to learn the optimal augmentation strategy for the given data automatically. 

In Sections~\ref{sec:tong_simpleaug} and~\ref{sec:tong_learnedaug}, we will introduce GDA approaches in more detail based on this separation as it provides better differentiation of the methodologies and improved readability. Table~\ref{tab:methods} shows a summary of GDA techniques, categorized following this taxonomy and the methods' methodologies.


\section{Rule-based Approaches for GDA}
\label{sec:tong_simpleaug}


\begin{table}[!th]
\scriptsize
\caption{A summary of GDA techniques, categorized by whether they are learned augmentations and their methodologies.}
% \vspace{-0.1in}
\label{tab:methods}
\centering
\begin{tabular}{l|l|l|ccc|ccc}
\toprule
\multicolumn{1}{c}{} & \multirow{2}{*}{Methodology} & \multirow{2}{*}{Representative Works} & \multicolumn{3}{|c}{Task Level} & \multicolumn{3}{|c}{Augmented Data}  \\
\multicolumn{2}{c|}{} & & Node & Graph & Edge & Structure & Feature & Label \\
\midrule
\multirow{27}{*}{Rule-based GDA}
& \multirow{7}{*}{Stochastic Dropping/Masking}
& DropEdge~\cite{rong2019dropedge} & \cmark &  &  & \cmark &  &  \\
& & DropNode~\cite{feng2020graph} &  & \cmark &  &  & \cmark &  \\
& & NodeDropping~\cite{you2020graph} &  & \cmark &  & \cmark &  &  \\
& & Feature Masking~\cite{thakoor2022largescale} & \cmark &  &  &  & \cmark &  \\
& & Feature Shuffling~\cite{velickovic2019deep} & \cmark &  &  &  & \cmark &  \\
& & DropMessage~\cite{fang2022dropmessage} & \cmark &  & \cmark &  & \cmark &  \\
& & Subgraph Masking~\cite{you2020graph} &  & \cmark &  & \cmark & \cmark &  \\
\cmidrule(lr){2-9}
& \multirow{3}{*}{Subgraph Cropping/Substituting}
& GraphCrop~\cite{wang2020graphcrop} &  & \cmark &  & \cmark &  &  \\
& & M-Evolve~\cite{zhou2020data} &  & \cmark &  & \cmark &  &  \\
& & MoCL~\cite{sun2021mocl} &  & \cmark &  & \cmark & \cmark &  \\
\cmidrule(lr){2-9}
& \multirow{2}{*}{Virtual Node}
& Graphormer~\cite{ying2021transformers} &  & \cmark &  & \cmark &  &  \\
& & GNN-CM$^+$/CM~\cite{hwang2021revisiting} &  &  & \cmark & \cmark &  &  \\
\cmidrule(lr){2-9}
& \multirow{4}{*}{Mixup}
& Graph Mixup~\cite{wang2021mixup} & \cmark & \cmark &  &  &  & \cmark \\
& & ifMixup~\cite{guo2021intrusion} &  & \cmark &  & \cmark & \cmark & \cmark \\
& & Graph Transparent~\cite{park2021graph} &  & \cmark &  & \cmark & \cmark & \cmark \\
& & G-Mixup~\cite{han2022G} &  & \cmark &  & \cmark & \cmark & \cmark \\
\cmidrule(lr){2-9}
& \multirow{3}{*}{SMOTE}
& GraphSMOTE~\cite{zhao2021graphsmote} & \cmark &  &  &  & \cmark &  \\
& & GATSMOTE~\cite{liu2022gatsmote} & \cmark &  &  & \cmark &  &  \\
& & GNN-CL~\cite{li2022graph} & \cmark &  &  & \cmark & \cmark &  \\
\cmidrule(lr){2-9}
& \multirow{1}{*}{Diffusion}
& GDA~\cite{klicpera2019diffusion} & \cmark &  &  & \cmark &  & \\
\cmidrule(lr){2-9}
& \multirow{1}{*}{Counterfactual Augmentation}
& CFLP~\cite{zhao2021counterfactual} &  &  & \cmark & \cmark &  & \cmark \\
\cmidrule(lr){2-9}
& \multirow{2}{*}{Attribute Augmentation}
& LA-GNN~\cite{liu2021local} & \cmark &  &  &  & \cmark &  \\
& & SR+DR~\cite{song2021topological} & \cmark &  &  &  & \cmark &  \\
\cmidrule(lr){2-9}
& \multirow{2}{*}{Pseudo-labeling}
& Label Propagation~\cite{zhu2005semi} & \cmark &  &  &  &  & \cmark \\
& & PTA~\cite{dong2021equivalence} & \cmark &  &  &  &  & \cmark \\
\midrule
\multirow{16}{*}{Learned GDA}
& \multirow{5}{*}{Graph Structure Learning}
& GAug~\cite{zhao2021data} & \cmark &  &  & \cmark &  &  \\
& & GLCN~\cite{jiang2019semi} & \cmark  &  &  & \cmark &  &  \\
& & LDS~\cite{franceschi2019learning} & \cmark  &  &  & \cmark &  &  \\
& & ProGNN~\cite{jin2020graph} & \cmark &  &  & \cmark &  &  \\
& & Eland~\cite{zhao2021action} & \cmark &  &  &  \cmark &  &  \\
\cmidrule(lr){2-9}
& \multirow{4}{*}{Graph Adversarial Training}
& RobustTraining~\cite{xu2019topology} & \cmark &  &  & \cmark &  &  \\
& & AdvT~\cite{dai2019adversarial} &\cmark  &  &\cmark  &\cmark  &  &  \\
& & FLAG~\cite{kong2022robust} & \cmark & \cmark &\cmark  &  & \cmark  &  \\
& & GraphVAT~\cite{feng2019graph} &\cmark  &  &  & & \cmark  &  \\
\cmidrule(lr){2-9}
& \multirow{2}{*}{Graph Rationalization}
% & DIR~\cite{wu2021discovering} &  & \cmark &  & \cmark & \cmark &  \\
& GREA~\cite{liu2022graph} &  & \cmark &  & \cmark & \cmark &  \\
&& AdvCA~\cite{sui2022adversarial} &  & \cmark &  & \cmark & \cmark &  \\
\cmidrule(lr){2-9}
& \multirow{4}{*}{Automated Augmentation}
& AutoGDA~\cite{zhao2022autogda} & \cmark &  &  & \cmark & \cmark &  \\
& & GraphAug~\cite{luo2022automated} &  & \cmark &  & \cmark & \cmark &  \\
& & JOAO~\cite{you2021graph} &  & \cmark &  & \cmark & \cmark &  \\
& & MolCLE~\cite{wang2021molecular} &  & \cmark &  & \cmark & \cmark &  \\
\bottomrule
\end{tabular}
\end{table}

Owing to their simplicity and efficiency, rule-based graph data augmentation methods are the most commonly used augmentation techniques in graph machine learning. The rule-based GDA approaches can generally categorized into three categories, where the first category of methods would remove part of the data (e.g., Stochastic Masking) to create new graph data, the second category of methods augments the graph data by generating new graphs or adding components (e.g., Counterfactual Augmentation, Pseudo-labeling),
and the third category includes methods that manipulate the data following rules can involve both removing and adding operations (e.g., Diffusion, etc.) In the following subsections, we summarize the representative approaches in each category and also discuss their applications on different tasks and domains.

\subsection{Data Removal}
\label{sec:tong_dataremove}

\paragraph{Edge Dropping} Edge dropping methods stochastically remove a certain fraction of edges from the graph data. Aiming to alleviate the known over-smoothing problem of GNNs, \citet{rong2019dropedge} first proposed DropEdge which randomly dropped a fixed fraction of edges in each training epoch, resembling Dropout \cite{srivastava2014dropout}. More specifically, at the beginning of each training epoch, the modified adjacency matrix $\tilde{\mathbf{A}}$ is defined by
\begin{equation}
    \tilde{\mathbf{A}} = \mathbf{M} \odot \mathbf{A}, %\text{ where } M_{i,j} = Bernoulli(\varepsilon).
\end{equation}
where $\mathbf{M} \in \{0, 1\}^{N \times N}$ is a binary mask on the adjacency matrix s.t. $M_{i,j} = Bernoulli(\varepsilon)$, $\varepsilon \in (0,1)$ is the drop rate hyper-parameter, and $\odot$ denotes the Hadamard product.

During GNN training, DropEdge adopts a newly sampled $\tilde{\mathbf{A}}$ instead of the original graph structure $\mathbf{A}$ for message passing (e.g., Equation~\eqref{eq:gcn}) in each training epoch. By showing the GNN models different perturbations of the graph in each training epoch, DropEdge improves the model's generalization and shows significant performance improvements on deeper GNNs, indicating that the strategy mitigates over-smoothing. Several other methods~\cite{you2020graph,thakoor2022largescale,zhao2022autogda} also adopt random edge masking in other learning schemes such as self-supervised learning, which conducts the same operation as DropEdge.

\paragraph{Node Dropping} Similar to edge dropping, node dropping methods stochastically remove nodes from the graph. Node dropping is typically implemented in two ways: removing all features of the target nodes from the feature matrix, or removing the target nodes along with all the edges connected with them from the graph structure. 
\citet{feng2020graph} proposed DropNode, which follows the first schema. Concurrently, \citet{you2020graph} proposed NodeDropping following the latter.

Both DropNode~\cite{feng2020graph} and NodeDropping~\cite{you2020graph} aim to randomly remove a fraction of the nodes from the given graph, assuming that the missing nodes should not affect the semantic meanings of the remaining nodes, or the whole graph $G$. 
\citet{feng2020graph} focused on semi-supervised node classification, where a consistency loss is used on the predicted logits of different augmented versions of the graphs. On the other hand, \citet{you2020graph} focused on self-supervised graph representation learning with contrastive targets.

\paragraph{Feature Masking} Other than the graph structure, i.e., nodes and edges, multiple works also adopted masking augmentations on the node features. For example, graph contrastive learning methods~\cite{thakoor2022largescale,you2020graph,you2021graph,zhu2020deep} commonly utilize stochastic feature masking as an efficient way of augmenting or corrupting the graph. On top of randomly masking feature values (i.e., random entries in $\mathbf{X}$) or feature signals (i.e., random columns in $\mathbf{X}$), \citet{velickovic2019deep} utilized row swapping as an effective way of corrupting the graph. Specifically, \citet{velickovic2019deep} randomly re-assigned the each node's feature vector to another node in the graph, which can be obtained by row-wise shuffling of $\mathbf{X}$.

More recently, \citet{fang2022dropmessage} proposed DropMessage, which masks the features aggregated by message passing in GNNs. More specifically, denoting the aggregated neighbor feature of node $v$ by the $l$-th layer as $\boldsymbol{h}_{\mathcal{N}(v)}^l$ (Equation~\eqref{eq:gnn}), DropMessage randomly applies a binary mask on $\boldsymbol{h}_{\mathcal{N}(v)}^l$ for each node $v \in \mathcal{V}$ in every GNN layer. Similar to other dropping methods, the masks are sampled according to a Bernoulli distribution.

\paragraph{Subgraph Cropping} Another common data removal augmentation approach is cropping out part of the graph data. Such subgraph cropping can usually be achieved by either sampling the remaining subgraph or the subgraph that will be cropped out. For example, \citet{you2020graph} first proposed the Subgraph augmentation, which samples the remaining subgraph via random walk. The method later learns the graph representations by contrasting the sampled subgraphs, with the assumption that the semantics of the whole graph can be preserved in part or its local structure. On the other hand, GraphCrop~\cite{wang2020graphcrop} crops a contiguous subgraph from each of the given graph object. GraphCrop adopts a graph diffusion-based node-centric strategy, performing graph diffusion on the randomly selected seed nodes, to maintain the topology characteristics of original graphs after the cropping.


\subsection{Data Addition}
\label{sec:tong_dataadd}
Opposite to data removal methods, data addition methods augments the graph data by adding components to the existing/observed graph data or directly generating additional graphs. Note that although edge dropping is one of the most common techniques in data removal, rule-based edge addition is rather uncommon due to the huge search space for potential edge addition candidates (with a complexity of $O(N^2)$). While graph diffusion methods include adding edges, we discuss them later in Section~\ref{sec:tong_datamani} as they also include sparsification operations after edge addition.

\paragraph{Virtual Node}
For graph classification, creating a virtual node that connect to all nodes in the graph is a commonly used GDA approach~\cite{gilmer2017neural,li2017learning,ishiguro2019graph,hu2021ogb,ying2021transformers}. The idea of virtual node is to compute a graph representation in parallel with the node representations during the aggregation process. Therefore, instead of using an additional pooling layer, the virtual node's representation can directly be used as the graph representation, in a way similar to the \texttt{[CLS]} token in language modeling. Moreover, as the virtual node connects to all the nodes, it allows feature aggregation between previously unreachable nodes without adding additional GNN layers. \citet{ying2021transformers} further show that it acts similar as self-attention in Transformers. Other than graph-level tasks, \citet{hwang2021revisiting} also studied virtual nodes for link prediction. As the graph data for link prediction is usually much larger for link prediction when compared with those in graph-level tasks, \citet{hwang2021revisiting} proposed to augment the graph data multiple virtual nodes, each connecting with a subset of all nodes in the graph with assignment decided by clustering methods.

\paragraph{Data Interpolation}
With it's simplicity and effectiveness, Mixup~\cite{zhang2017mixup} has been commonly used in image and language domains for augmenting new data samples. Specifically, Mixup constructs virtual training examples by interpolating two labeled training samples:
\begin{equation}
    \begin{array}{cc}
         & \tilde{\boldsymbol{x}} = \lambda\boldsymbol{x}_i + (1-\lambda)\boldsymbol{x}_j, \\
         & \tilde{\boldsymbol{y}} = \lambda\boldsymbol{y}_i + (1-\lambda)\boldsymbol{y}_j,
    \end{array}
\end{equation}
where $(\boldsymbol{x}_i, \boldsymbol{y}_i)$ and $(\boldsymbol{x}_j, \boldsymbol{y}_j)$ are two randomly selected labeled training examples, and $\lambda \in [0,1]$. By linearly interpolating the feature vectors and labels, Mixup incorporates the prior knowledge and extends the training distribution. Similarly, Manifold Mixup~\cite{verma2019manifold} performs Mixup on latent intermediate representations instead of raw features of the two training samples.  

The direct analog of Mixup on graphs is not obvious, given the inter-dependent and irregular nature of graph data.
\citet{verma2019graphmix} proposed GraphMix that augmented the training of a GNNs with a Fully-Connected Network, which is trained by interpolating the hidden states and labels. 
As GraphMix is more of a regularization method than the analog of Mixup on graphs, \citet{wang2021mixup} proposed Graph Mixup, which analogized Manifold Mixup with a two-branch graph convolution module. Given a pair of nodes, Graph Mixup mixes their raw features, passes them into the two-branch GNN layer, and mixes the hidden representations of each layer. Notably, mixing up the nodes on features and hidden states avoids re-assembling the local neighborhoods of the two nodes. Graph Mixup also works for the task of graph classification.
To avoid the node matching problem when mixing up two independent graphs, Graph Mixup mixes the latent representations of the pair of graphs. 

On the other hand, ifMixup~\cite{guo2021intrusion} directly applies Mixup on the graph data instead of the latent space for graph-level tasks. As the pair of graphs are irregular and the nodes from two graphs are not generally aligned, ifMixup arbitrarily assigns indices to the nodes in each graph and matches the nodes according to the indices. 
Empirically, ifMixup shows marginal performance improvements over Graph Mixup on the task of graph classification.
Following ifMixup, Graph Transplant~\cite{park2021graph} also mixes graph in data space
% . Unlike ifMixup which randomly matches nodes during mixing, Graph Transplant 
, but uses substructures as mixing units to preserve the local structural information. Graph Transplant employs the node saliency information to select one meaningful substructure from each graph, where the saliency information is defined as the $l_2$ norm of the gradient of the classification loss. 

Different from the above Mixup-based methods which operate on instance level, \citet{han2022G} proposed G-Mixup that performs Mixup on class-level. Instead of directly interpolating the individual graphs, G-Mixup interpolates the graph generators (graphons) for each class. Specifically, G-Mixup first estimates a graphon for each class of the training graphs, then mixes up the graphons of different classes, and finally generate synthetic graphs with the mixed graphons. Denoting the graphons of classes $a$ and $b$ as $W_a$ and $W_b$, respectively, G-Mixup can be formulated as
\begin{equation}
    \begin{array}{cc}
         \tilde{\boldsymbol{x}} \sim W_c, \text{ where}\quad W_c = \lambda W_a + (1-\lambda)W_b, \\
         \tilde{\boldsymbol{y}} = \lambda\boldsymbol{y}_a + (1-\lambda)\boldsymbol{y}_b,
    \end{array}
\end{equation}
where $\boldsymbol{y}_a$ and $\boldsymbol{y}_b$ are corresponding labels for graphs in classes $a$ and $b$, respectively.

Besides Mixup, SMOTE~\cite{chawla2002smote} is also a classical data augmentation method that interpolates data instances. Different from Mixup which interpolates examples from different classes, SMOTE interpolates examples within the minority classes. Hence, SMOTE is especially effective when dealing with imbalanced data. On graph data, GraphSMOTE~\cite{zhao2021graphsmote} augments the minority class by over-sampling synthetic nodes and then generating edges for them. GATSMOTE~\cite{liu2022gatsmote} and GNN-CL~\cite{li2022graph} further utilize attention designs to improve the edge generating process between the synthetic nodes and original nodes in the graph.

\paragraph{Counterfactual Augmentations}
Counterfactual augmentation has been relatively under-explored in the field of graph machine learning.
\citet{zhao2021counterfactual} first proposed a counterfactual data augmentation method CFLP for the task of link prediction. To better understand the relationship between observed graph structure and link formation, CFLP asks the counterfactual question of ``would the link still exist if the graph structure became different from observation?'' To answer the question, \citet{zhao2021counterfactual} proposed counterfactual links that approximates the unobserved outcome in the question. CFLP then trains a link prediction model with both the given training data and the generated counterfactual links (as augmented data). Similarly, CLBR~\citet{zhu2022data} proposed counterfactual data augmentation for bundle recommendation. CLBR generates the counterfactual example by answering the counterfactual question ``what would a user interact with if the bundle-item affiliation relations change?''.

\paragraph{Attribute Augmentation}
Besides updating the graph topology, several works were also proposed to augment the graph data by generating additional node attributes. For example, LA-GNN~\cite{liu2021local} enhances the locality of node representations by generating node features based on the conditional distribution of the local structures and neighbor features. LA-GNN learns the new features of each node by the conditional distribution of its local neighborhood. The generated feature is directly used together with the raw node features as part of the input of GNNs for both training and inference. Similarly, SR+DR~\cite{song2021topological} generates topology features with DeepWalk~\cite{perozzi2014deepwalk}, and uses a dual GNN model with topology regularization to jointly train with both raw and topology features.

\paragraph{Pseudo-labeling}
The training data in graph tasks is often only partially labeled due to the generally high cost of human labeling. With the large amount of unlabeled data, pseudo-labeling for the unlabeled data is often adopted under semi-supervised learning settings. Label Propagation~\cite{zhu2002learning,zhu2005semi,dong2021equivalence} is one of the most classical methods for generating pseudo labels when only part of the nodes in the graph are labeled. Label propagation assumes that the two nodes are more likely to have the same label if they are connected, so it iteratively propagates node labels along the edges. With the propagated labels on the previously unlabeled nodes, the GNN model can then be trained with more labeled data. 

\subsection{Data Manipulation}
\label{sec:tong_datamani}
Other than only adding or removing graph data, several rule-based methods also augment the graph data by combining both kind of operations. In order to separate them from the methods that purely conducts data removal or data addition, we introduce such augmentation methods in this subsection.

\paragraph{Diffusion}
\citet{klicpera2019diffusion} first proposed generalized graph diffusion that modeled a ``future'' state of the graph where the signals were more spread out. Specifically, the generalized graph diffusion is formulated as
\begin{equation}
    \tilde{\mathbf{A}} = \sum_{k=0}^{\infty}\theta_k\mathbf{T}^k,
\end{equation}
where $\theta_k$ denote the global-local coefficient and $\mathbf{T} \in \mathbb{R}^{N\times N}$ represents the transition matrix derived from the adjacency matrix $\mathbf{A}$ (e.g., $\mathbf{A}\mathbf{D}^{-1}$ or $\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}}$). 
$\theta_k$ is usually pre-defined by specific diffusion variants, e.g., heat kernel~\cite{kondor2002diffusion} ($\theta_k=e^{-t}\frac{t^k}{k!}$) or Personalized PageRank (PPR)~\cite{page1999pagerank} ($\theta_k = \alpha(1-\alpha)^k$), where $\alpha$ denotes the teleport probability in a random walk and $t$ is diffusion time. The analytical solution of the heat kernel and PPR diffusions are defined as
\begin{equation}
    \tilde{\mathbf{A}}^{\text{heat}} = e^{-(t\mathbf{T} - t)}; \quad
    \tilde{\mathbf{A}}^{\text{PPR}} = \alpha (\mathbf{I}_N - (1-\alpha)\mathbf{T})^{-1},
\end{equation}
where $\mathbf{I}_N$ is the $N$ by $N$ identity matrix. As the obtained adjacency matrix after diffusion $\tilde{\mathbf{A}}$ is often too dense as input for GNNs, graph sparsification is commonly conducted to filter out some trivial edges, e.g., setting a threshold to cut-off edges with small weights.

For (semi-)supervised learning on graphs, $\tilde{\mathbf{A}}$ can be directly used for both training and inferencing with GNNs \cite{klicpera2019diffusion}. While most message passing-based GNNs are only capable of aggregating one-hop information in each layer, the augmented graph after diffusion allows GNNs to learn from multi-hop (global) information without specifically re-designing the GNN models. In self-supervised graph representation learning, $\tilde{\mathbf{A}}$ is often used as the augmented view for self-supervised learning objectives such as contrastive learning~\cite{hassani2020contrastive,yuan2021semi}.


\paragraph{Subgraph Substituting}
Several methods also make use of special substructures such as motifs and functional groups during subgraph augmentation. For example,
M-Evolve~\cite{zhou2020data} utilizes motifs to augment the graph data. M-Evolve first finds and selects the target motif in the graph, then adds or removes edges within the selected motifs based on a sampling weight calculated with Resource Allocation index.
Similarly, MoCL~\cite{sun2021mocl} utilizes biomedical domain knowledge to augment the molecular graphs on the substructures such as functional groups. 
MoCL selects a substructure from each molecular graph and replaces it with another substructure. 


\subsection{Applications of Rule-based GDA}
The rule-based augmentation techniques are mostly designed for improving general graph learning, and usually does not have constrains on specific tasks or domains. For example, although \citet{rong2019dropedge} only evaluated DropEdge for node classification task, the usage of it on other tasks is straightforward, and similar for most stochastic data removal methods discussed in Section~\ref{sec:tong_dataremove}. Nonetheless, some rule-based GDA methods are more suitable for certain domains. For instance, subgraph substituting methods~\cite{zhou2020data,sun2021mocl} utilizes substructure information or even biomedical domain knowledge to augment the graphs, which makes them naturally more suitable for graph-level tasks on biomedical data. On the other hand, graph diffusion methods~\cite{klicpera2019diffusion} are designed based on the spread of information along the relations in the graph, which makes such methods for suitable for larger graphs such as social networks or citation networks. Similarly, designed for exploring the formation of the links, counterfactual augmentation methods~\cite{zhao2021counterfactual,zhu2022data} are tailored for link prediction or recommendation on larger graphs. We also specify the targeted tasks for each GDA method in Table~\ref{tab:methods}.

Other than supervised graph representation learning schemes, the stochastic data removal methods (Section~\ref{sec:tong_dataremove}) are also commonly used in self-supervised graph representation learning methods as an efficient way of augmenting/corrupting graph data. For example, several methods~\cite{velickovic2019deep,you2020graph,you2021graph,thakoor2022largescale} use one or multiple of the above-mentioned techniques as augmentation methods for generating the augmented views of graph data. We further elaborate on the usage of data removing augmentations for self-supervised learning in Section~\ref{sec:tong_application}.

\section{Learned Approaches for GDA}
\label{sec:tong_learnedaug}

In the previous section, we introduced rule-based GDA approaches where no learnable parameters are involved during data augmentation. However, these approaches could sometimes be suboptimal since the augmentations do not take advantage of the rich information from downstream tasks, especially in (semi-)supervised training. Indeed, some prior works from the vision \cite{cubuk2019autoaugment} and natural language \cite{niu2019automatically} learning domains show the promise of learned augmentation approaches. To address this concern, learned GDA approaches are proposed to learn augmentation strategies in a data-driven manner. The existing methods can be categorized into the following types: (1) structure learning, (2) adversarial training, (3) rationalization, and (4) automated augmentation.


\subsection{Graph Structure Learning}
\label{sec:tong_gsl}
In real-world scenarios, given graph structures are often incomplete~\cite{franceschi2019learning}, noisy~\cite{jin2020graph,luo2021learning} or manipulated by adversarial attacks~\cite{jin2020adversarial,gunnemann2022graph}. Simply applying rule-based GDA approaches for training (semi-)supervised models on such graphs can lead to suboptimal performances, as they may not necessarily generate better graph structures for downstream tasks. To tackle these issues, several works propose graph structure learning approaches which aim to search for a better graph structure that augments the initial graph structure.
Essentially, those methods treat the graph structure as learnable parameters and iteratively refine it while learning the model parameters~\cite{zhao2021data,jin2020graph,franceschi2019learning,chen2020iterative,luo2021learning,zheng2020robust}. Numerous studies have demonstrated the effectiveness of graph structure learning methods in improving model generalization~\cite{zhao2021data,chen2020iterative} and robustness~\cite{jin2020graph,zheng2020robust}. In the following, we introduce several representative works that fall into the category of graph structure learning. 

\paragraph{Improving Generalization} There are numerous methods for graph structure learning that target improving the generalization performance. Overall, they can be divided into two categories based on the adjacency matrix which they learn: learning \emph{continuous structure} and learning \emph{discrete structure}.

Although the original adjacency matrix is usually discrete (or binary),  \emph{continuous structure} methods do not assume the learned adjacency matrix to be discrete, as modeling discrete structure requires additional efforts in optimization. 
Typically, these methods either model the adjacency matrix as free parameters or use a parameterized neural network to model the structure. For instance, GLCN~\cite{jiang2019semi} is an early work which proposes a unified network architecture to learn an optimal graph structure and GNN. It incorporates the similarities of node features to learn a sparse and continuous graph structure. Formally, it defines a graph learning loss $\mathcal{L}_\text{GL}$ as:
\begin{equation}
   \mathcal{L}_\text{GL} = \sum_{i, j=1}^N\left\|{\boldsymbol{x}}_i-{\boldsymbol{x}}_j\right\|_2^2 \tilde{\bf A}_{i j}+\gamma\|\tilde{\bf A}\|_F^2 + \beta \|\tilde{\bf A} - {\bf A}\|_F,
\end{equation}
where the first two terms control the smoothness and sparsity of the augmented graph, respectively; the third term forces the augmented graph to be close to the original graph; $\gamma$ and $\beta$ are the hyper-parameters that balance the three terms. By minimizing $\mathcal{L}_\text{GL}$ together with the classification loss, GLCN is able to learn a graph structure that best serves the downstream task.
TO-GCN~\cite{yang2019topology} also considers the feature similarity, but it further employs label similarity to refine the graph topology.  To handle the inductive learning setting, IDGL~\cite{chen2020iterative} casts the graph structure learning problem as similarity metric learning  which will be jointly trained with the prediction model dedicated to a downstream task. 
To encourage learning graph structure invariant to task-irrelevant information, \citet{sun2022graph} utilized the Information Bottleneck~\cite{tishby2000information} principle  to solve the graph structure learning problem. Moreover, SLAPS~\cite{fatemi2021slaps} identifies a supervision starvation problem in previous structure learning approaches and proposes to incorporate additional self-supervision by designing a feature denoising task. 

Despite the appeal of the first type of methods, continuous structures typically deviate from the original, sparse and \emph{discrete structure} evident in many real-world graphs.   
 To address this concern, some works focus on sampling the graph structures from a targeted distribution. 
For instance, by taking advantage of neural edge predictors like GAE~\cite{kipf2016variational}, \citet{zhao2021data} proposed GAug to generate plausible edge augmentations for an input graph. The output of the edge predictor can be formulated as 
\begin{equation}
\mathbf{M}=\sigma_0\left(\mathbf{Z Z}^T\right), \text { with } \mathbf{Z}= {\bf H}^{l},
\end{equation}
where ${\bf M}$ is the edge probabilities matrix and $\sigma_0$ is an element-wise sigmoid function.
Based on the edge probabilities matrix, two variants GAug-M and GAug-O are proposed to tackle augmentation in settings where edge manipulation is and is not feasible at inference time, respectively. Specifically, GAug-M deterministically adds edges with the highest edge probabilities to the graph at inference time; GAug-O optimizes the graph structure by minimizing the downstream classification loss together with the edge prediction loss and samples the adjacency matrix according to an element-wise Bernoulli distribution.
Another representative work is LDS~\cite{franceschi2019learning}, which aims at learning discrete structure between data points while learning GNN parameters. It models the process as learning the edge probability matrix, which parameterizes the element-wise Bernoulli distribution from which the discrete structure is sampled. Then it formulates the learning process as a bi-level problem and updates the structure and model parameters in a differentiable way. 
\citet{shang2021discrete} improves the efficiency of LDS by converting the bi-level problem to a uni-level problem and extends it to multivariate time series.
In addition to Bernoulli distribution, recent studies have investigated other distributions to sample the discrete structure. For example, to account for the underlying generation of graphs, GEN~\cite{wang2021graph} hypothesizes that the estimated graph is drawn from Stochastic Block Model (SBM)~\cite{holland1983stochastic}.
% and learns it in an end-to-end manner. 
Similarly, BGCN~\cite{zhang2019bayesian} iteratively trains an assortative mixed membership stochastic block model with predictions of GCN to produce multiple denoised graphs, and ensembles results from multiple GCNs. To explicitly guarantee the strength and diversity of graph augmentation, MH-Aug~\cite{park2021metropolis} draws augmented graphs from an explicit target distribution through the Metropolis-Hastings algorithm, which can also be viewed as a  graph structure learning process.

Instead of drawing discrete structures from targeted distributions, another line of works focus on dropping/adding edges from the original graph which can also lead to a discrete adjacency matrix. For instance, to improve the performance of GNNs under random noise, PTDNet~\cite{luo2021learning} proposes to prune task-irrelevant edges by penalizing the number of edges in the sparsified graph and imposing the low-rank constraint with parameterized networks.  Similarly, NeuralSparse~\cite{zheng2020robust} learns to drop task-irrelevant edges; it takes node/edge features as parts of input and jointly optimizes graph sparsification from the supervision of downstream task. Moreover, \citet{gao2021training} proposed TADropEdge which leverages the graph spectrum to generate edge weights that represent the edges' criticality for the graph connectivity and drops edges by treating their weights as probabilities. Besides node classification tasks, \citet{spinelli2021fairdrop} proposed FairDrop for the task of fair graph representation learning, which biasedly dropped edges with a sensitive attribute homophily mask to protect against unfairness. Later, \citet{chen2020measuring} proposed AdaEdge, which iteratively adds/removes edges according to the node classification prediction. In each iteration, after the GNN model is sufficiently trained, AdaEdge adds edges between nodes that are predicted to be in the same class with high confidence, and vice versa. AdaEdge iteratively performs GNN training and graph modification until convergence. Besides, \citet{zhao2021action} proposed Eland for the task of anomaly detection on time-stamped user-item bipartite graphs. Eland first transforms the user-item graph into users' action sequences and adopts seq2seq model for future action prediction. The predicted user actions are added back into the graph to yield the augmented graph data. As the augmented graph contains richer user behavior information, Eland enhances the anomaly detection performance and detects anomalies at an early stage.
It is worth mentioning that the aforementioned techniques are focused on one specific task such as node classification. To make graph structure learning benefit various downstream tasks, \citet{liu2022towards} proposed an unsupervised approach to learn graph structures with the aid of self-supervised contrastive learning~\cite{zhu2021graph}.  

While existing methods majorly focus on training-time augmentation, i.e., modifying the training graph data, a new line of work (e.g., GTrans~\cite{jin2022empowering}) introduces test-time augmentation by transforming the test graph through optimizing a self-supervised loss. It has been demonstrated to significantly improve the generalization performance of GNNs on out-of-distribution data. 

\paragraph{Improving Robustness} Recent studies have demonstrated the vulnerability of GNNs under adversarial attacks, i.e., carefully-crafted small perturbation on the input graph leads GNNs into giving wrong predictions~\cite{zugner2018adversarial,dai2018adversarial,zugner2019adversarial,jin2021adversarial}. A series of works are proposed to focus on enhancing the robustness of graph neural networks under adversarial attacks by learning clean graph structure. \citet{jin2020graph} observed that adversarial attacks violate important graph properties such as sparsity, low-rank, and feature smoothness; it then proposes the ProGNN framework to robustify GNNs by alternatively updating the graph structure by preserving these graph properties by adding penalizing regularization terms and training GNN parameters on the updated graph structure. Specifically, it defines the following graph learning loss:
\begin{equation}
\mathcal{L}_\text{GL}=\alpha\|\tilde{\bf A}\|_1+\beta\|\tilde{\bf A}\|_* +\lambda \operatorname{tr}\left(\mathbf{X}^T \tilde{\mathbf{L}} \mathbf{X}\right) + \|\tilde{\bf A} - {\bf A}\|_F^2,
\label{eq:prognn}
\end{equation}
where $\|\cdot\|_1$ is the $\ell_1$ norm, $\|\cdot\|_*$ is the nuclear norm, and $\tilde{\bf L}$ is the normalized Laplacian matrix of $\tilde{\bf A}$. The first three terms in Equation~\eqref{eq:prognn} force the learned graph to preserve the properties of sparsity, low-rank, and feature smoothness, respectively. Similar to GLCN~\cite{jiang2019semi}, ProGNN also includes the downstream classification loss in the graph learning process. Despite the  robustness of ProGNN, it is computationally expensive with O($N^3$) time complexity and O$(N^2)$ space complexity. To speed up ProGNN, LRGNN~\cite{xu2021speedup} decouples the adjacency matrix into a low-rank component and a sparse one, and learns the graph structure by minimizing the rank of the low-rank component and suppressing the sparse one. Furthermore, as robust GNNs tend to yield unsatisfying performance when trained with limited labeled nodes, \citet{dai2022towards} took advantage of self-supervision and uses node attributes to predict the links so as to boost robust performance, which also saves computational cost from direct structure learning. Also using a link predictor, DefenseVAE~\cite{zhang2020defensevgae} employs variational graph autoencoder~\cite{kipf2016variational} to reconstruct graph structure that can reduce the effects of adversarial perturbations and boost the performance of GNNs under adversarial attacks. In addition, utilizing information theory, CoGSL~\cite{liu2022compact} targets at learning the most compact structure relevant to downstream tasks in order to achieve a better balance between robustness and accuracy. Instead of explicitly learning the graph structure, GNNGuard~\cite{zhang2020gnnguard} mitigates the negative effects of  adversarial attacks by assigning higher weights to edges connecting similar nodes while pruning edges between dissimilar nodes, which can also be considered as implicit graph structure learning. While the aforementioned techniques have shown robustness in some specific settings, one recent work~\cite{mujkanovic2022defenses} revealed that their robustness decreases significantly under proper evaluation (in particular the adaptive attacks). This suggests that a more powerful and adaptive GSL method is needed for effective defense. 

It is worth noting that there are some other graph structure learning 
 works which aim at learning graphs to improve the scalability of graph machine learning models~\cite{jin2022graph, jin2022condensing,liu2022graphc}. They do not target improving model performance or robustness of GNNs, and hence are not in the GDA scope tackled in this work. 

\subsection{Graph Adversarial Training}
\label{sec:tong_adv}
Adversarial training is a widely used countermeasure for adversarial attacks on computer vision~\cite{goodfellow2014explaining}, and has also been extended to graph domain~\cite{dai2018adversarial,feng2019graph,deng2019batch,hu2021robust,dai2019adversarial,chen2020smoothing,kong2022robust}.  Unlike graph structure learning, graph adversarial training does not seek to find an optimal graph structure. Instead, it augments input graphs with adversarial patterns during model training by perturbing node features or graph structure. The adversarially trained models are expected to tolerate adversarial perturbations in graph data and yield better generalization and robustness performance at test time. At the core of adversarial training is the injection of adversarial examples into the training set, with which the trained model can predict the test adversarial examples properly.  Thus, we can adopt this strategy to enhance the robustness of GNNs as follows, 
\begin{equation}
\min _{\Theta} \max _{\Delta_{\bf A} \in \mathcal{P}_{\bf A} \atop \Delta_{\bf X} \in \mathcal{P}_{\bf X}} \mathcal{L}_{\text{train}}\left(g_{\Theta}({\bf A}+\Delta_{\bf A}, {\bf X}+\Delta_{\bf X}) \right),
\label{eq:adv-training}
\end{equation}
where $\mathcal{L}_\text{train}$ denotes the training loss for the downstream task; $\Delta_{\bf A}$ and $\Delta_{\bf X}$ stand for the perturbation on ${\bf A}, {\bf X}$, respectively; $\mathcal{P}_{\bf A}$ and $\mathcal{P}_{\bf X}$ denote the perturbation space. From the bi-level optimization problem in Equation~\eqref{eq:adv-training}, we can observe that adversarial training generates perturbations that maximize the prediction loss and updates model parameters to minimize the prediction loss. The process of generating perturbations (i.e., ${\bf A}+\Delta_{\bf A}, {\bf X}+\Delta_{\bf X}$) can be viewed as adversarial data augmentation and we can leverage such augmentations to improve the model robustness and generalization.  

To augment the adjacency matrix, \citet{dai2018adversarial} proposed to randomly drop edges during adversarial training without any optimization on the graph data. While this strategy does not bring significant improvement, such cheap adversarial training still shows some improvement in robust classification accuracy. This finding is also in line with that from \citet{zugner2020certifiable}. Instead of randomly dropping edges, \citet{xu2019topology} leveraged projected gradient descent (PGD) to optimize the bi-level problem and generate perturbations on the discrete structure, which achieves significant improvement in robust performance. Similarly, \citet{chen2019can} and~\citet{dai2019adversarial} also used existing adversarial attacks to modify the input graph structure during adversarial training, designed for network embedding methods. Furthermore, \citet{suresh2021adversarial} proposed to generate adversarial graph augmentation by learning to drop edges such that the augmentation can capture the minimal information that is sufficient to classify each graph. 

On the other hand, there are some works focusing on perturbing the input features to serve as adversarial examples. For instance, \citet{feng2019graph} proposed an adversarial training strategy with dynamic regularization, which aims to reconstruct graph smoothness and constrains the divergence between the prediction of the target node and its connected nodes. \citet{deng2019batch} proposed batch virtual adversarial training to promote the smoothness of GNNs and thus defend against adversarial perturbations. Moreover, \citet{kong2022robust} proposed FLAG which utilizes adversarial training to iteratively augment the node features with gradient-based adversarial perturbations and improves the performances of GNNs on node classification, link prediction, and graph classification tasks. In addition, \citet{zugner2019certifiable} studied certifiable robustness of GNNs w.r.t. perturbations of node attributes and propose a robust training scheme inspired by the certificates.
Several other variants of adversarial training on perturbing node features are introduced in~\cite{wang2019graphdefense-adv-training,hu2021robust}.  

\subsection{Rationalization}
\label{sec:tong_rat}
A \emph{rationale} is defined as a subset of input features that best represent, guide and support model prediction~\cite{liu2022graph}. In the graph domain, rationales are subgraphs intrinsically learned by graph learning models. Rationales can be viewed as a form of augmented graph data that provide intrinsic explanations to the graph models' predictions, as opposed to the post-hoc explanation methods. 
Rationalization is commonly applied to graph-level property prediction or classification tasks~\cite{wu2020generalization,liu2022graph, you2020graph, chenlearning, miao2022interpretable, li2022learning} for drug and material discovery on molecular and polymer datasets, etc. 

Rationalization emerged in the graph domain as an approach to enhance both the interpretability and overall performance of graph classification and regression.
\citet{yu2020graph} found similarity to the Information Bottleneck (IB) problem, and proposed the Graph Information bottleneck framework (GIB), which learns to generate the maximally informative and compressed subgraph (IB-graph) by leveraging a bi-level optimization scheme and a novel connectivity loss. Also rooted in the IB paradigm, \citet{miao2022interpretable} proposed GSAT to better learn and select task-relevant subgraphs that improve interpretation and prediction by injecting stochasticity into the attention weights in order to constrain information from task-irrelevant components.
GREA, another rationalization work proposed by \citet{liu2022graph}, proposed a novel environment replacement augmentation method, which separates the rationale and the environment subgraphs (the remaining and complementary subgraphs to the rationale ones) and optimized the separation (rationalization identification) with data augmentation by replacing the original environment subgraph with a different one in the latent space.

Rationalization models are also effective in addressing data bias and out-of-distribution (OOD) problems for graph property prediction tasks since rationales are both interpretable and generalizable~\cite{miao2022interpretable}. \citet{wu2021discovering} proposed DIR to generate distribution perturbation on training data with causal intervention. Based on the idea that causal patterns are stable to distribution shift, they created a rationale generator that separates causal and non-causal graphs, applies causal intervention to create perturbed distributions, and then jointly learn both the causal and non-causal representation to minimize invariant risk. Similarly, \citet{chenlearning} also took a causal perspective to solve the OOD problem. They proposed CIGA to model the graph generation process and the interactions between invariant and spurious features with Structural Causal Models (SCM). The resulting subgraphs generated by CIGA maximally preserves the invariant intra-class information. \citet{li2022learning} also proposed to separate invariant and variant graphs. In their framework GIL, they proposed a GNN based subgraph generator to identify potentially invariant subgraphs, then infer latent environment labels for the variant subgraphs, before jointly optimizing all modules. To address the limited environments and unstable causal
features in data augmentation methods for graph rationalization, AdvCA~\cite{sui2022adversarial} was proposed to improve the generalization capacity against covariate shift through adversarial causal augmentation.

\subsection{Automated Augmentation}
\label{sec:tong_autoaug}
GDA techniques mentioned in Section~\ref{sec:tong_simpleaug} take rule-based approaches to augment graph data, applying the same augmentation method to subgraphs and graphs which embody different attributes and characteristics like degree distribution and homophily. To tackle this issue, Automated GDA techniques~\cite{sun2021automated, luo2022automated,zhao2022autogda,you2021graph, kose2022fair, hassani2022learning, zhu2021graph} were recently explored to automatically learn tailored augmentations for different subgraphs or graphs. For example, \citet{sun2021automated} proposed AutoGRL for the task of node classification. Through the training process, AutoGRL learns the best combination of GDA operations, GNN architecture, and hyper-parameters. The searching space of AutoGRL includes four GDA operations implemented by random masking and GAug-M~\cite{zhao2021data}: drop features, drop nodes, add edges, and remove edges.

Since automated GDA objectives are often complex to optimize, some recent works use reinforcement learning approaches as a solution. \citet{zhao2022autogda} framed the AutoGDA as a bi-level optimization problem, aiming to find a different set of augmentation strategies for each community in the graph as they observed various characteristics in each community. They employ an RL-agent to generalize the learning and find localize augmentation strategies for node classification tasks. On graph classification tasks, \citet{luo2022automated} set out to learn an automated augmentation model with GraphAug, to provide label-invariant augmentations for each graph in the dataset. Applying reinforcement learning, they maximize the estimated label-invariance probability to learn the augmentation category and transformation selection.

Another group of works on automated augmentation focus on graph contrastive learning. \citet{you2021graph} proposed to learn augmentations to replace ad hoc and handpicked augmentations for contrastive learning. They design an augmentation-aware projection head to avoid complicated augmentations, and formulate a bi-level optimization problem to learn both the augmentation strategy and graph representation. \citet{hassani2022learning} learned a probabilistic policy that contains a set of distributions over different augmentation operations in their method LG2AR, and samples an augmentation strategy from the policy in each training epoch. 
\citet{zhu2021graph} proposed GCA, which proposes adaptive augmentations based on node centrality measures. Unlike the aforementioned methods which find the best augmentation strategy for the dataset, GCA adaptively augments different nodes according to their importance. \citet{wang2021molecular} proposed to use a generative probabilistic model and a learnable feature selector to automatically parameterize topological and attribute augmentations, which can also provide explanations for underlying patterns in molecular graphs. Lastly, \citet{kose2022fair} proposed FairAug which utilizes adaptive augmentations for fair graph representation learning.



\section{GDA for Self-supervised Learning}
\label{sec:tong_application}

\begin{table}[t]
\scriptsize
\caption{Representative self-supervised graph learning works that utilized graph data augmentation techniques.  \\ $^\dag$Although the methods in this category are semi-supervised methods, they used GDA operations with only self-supervised learning objectives (i.e, consistency loss). Therefore, we categorize their GDA techniques as designed for self-supervised learning objectives.}
\label{tab:sslmethods}
\centering
\Scale[1]{\begin{tabular}{l|l|ccc|ccc}
\toprule
\multicolumn{1}{c|}{} & \multirow{2}{*}{Representative Works} & \multicolumn{3}{c|}{Task Level} & \multicolumn{3}{c}{Augmented Data} \\
\multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & Node & Graph & Edge & Structure & Feature & Label \\
\midrule
\multirow{5}{*}{Contrastive Learning}
& DGI~\cite{velickovic2019deep} & \cmark &  &  &  &  \cmark \\
& GRACE~\cite{zhu2020deep} & \cmark &  &  & \cmark & \cmark &  \\
& MVGRL~\cite{hassani2020contrastive} & \cmark &  &  & \cmark &  \\
& GraphCL~\cite{you2020graph} &  & \cmark &  & \cmark & \cmark \\
& JOAO~\cite{you2021graph} &  & \cmark &  & \cmark & \cmark &  \\
\midrule
\multirow{4}{*}{Non-contrastive Learning}
& CCA-SSG~\cite{zhang2021canonical} & \cmark &  &  & \cmark & \cmark \\
& GBT~\cite{bielak2022graph} & \cmark &  &  & \cmark & \cmark \\
& BGRL~\cite{thakoor2022largescale} & \cmark &  &  & \cmark & \cmark \\
& T-BGRL~\cite{shiao2022link} & \cmark &  &  & \cmark & \cmark \\
\midrule
\multirow{4}{*}{Consistency Training$^\dag$}
& GRAND~\cite{feng2020graph} & \cmark &  &  & \cmark & \cmark \\
& NodeAug~\cite{wang2020nodeaug} & \cmark &  &  & \cmark & \cmark \\
& MV-GCN~\cite{yuan2021semi} & \cmark &  &  & \cmark \\
& NASA~\cite{bo2022regularizing} & \cmark &  &  & \cmark & \\
\bottomrule
\end{tabular}}
\end{table}

Other than directly using the augmented graph data in supervised learning, the most common use case for GDA is under self-supervised learning (SSL) schemes, e.g. contrastive learning. Self-supervised objectives learn representations that are robust to noise and perturbations by maximizing the (dis)agreements of learned representations. Therefore, unlike most of the previously mentioned learned GDA techniques (Section~\ref{sec:tong_learnedaug}) which aim to enhance the task-relevant information in the data, most of the GDA techniques for self-supervised learning are rule-based augmentations (Section~\ref{sec:tong_simpleaug}) which aim to corrupt or perturb the given graph data. Moreover, most self-supervised graph representation learning methods tend to use a combination of several simple GDA operations. In this section, we introduce three commonly used self-supervised graph learning schemes as well as the GDA approaches they utilize.

% \vspace{-0.1in}
\subsection{Contrastive Learning}
\label{sec:tong_contrastive}
In recent years, with the rapid development of contrastive learning in CV~\cite{chen2020simple}, many  contrastive learning methods~\cite{zhu2020deep,you2020graph,trivedi2021augmentations,xie2022self,liu2022graph,ju2022multi} have been proposed for applications on graph data.
Typically, a graph contrastive learning framework includes three main components: a GDA module that generates different views of the given graph data, a GNN-based encoder to compute the representations, and a contrastive learning objective to train the model. For each data example (nodes for node-level tasks and graphs for graph-level tasks), these methods consider augmented views or variants of itself as associated positive samples and other data examples in the same batch as associated negative samples. Contrastive learning objectives then maximize the (dis)agreements of the representations between each data example with their (negative) positive examples. 

To efficiently generate different augmented data for graph contrastive learning, rule-based data removal operations (Section~\ref{sec:tong_dataremove}) are the most commonly used GDA techniques, as they are fast and easy to apply. For example, multiple methods (GRACE~\cite{zhu2020deep}, GraphCL~\cite{you2020graph}, etc.) adopt stochastic edge dropping and/or feature masking due to their simplicity. DGI~\cite{velickovic2019deep} adopts feature corruption by conducting a row-wise shuffling on the raw node feature matrix $\mathbf{X}$. In general, graph contrastive learning methods usually adopt a combination of multiple augmentation techniques to generate different augmented views. 
GraphCL~\cite{you2020graph} and InfoGCL~\cite{xu2021infogcl} adopt four GDA operations: node dropping which randomly removes nodes along with its edges, edge perturbation which randomly adds or drops edges, attribute masking which randomly masks off certain node attributes, and subgraph sampling which samples connected subgraphs. SUBG-CON~\cite{jiao2020sub} utilizes a subgraph sampler to extract the context subgraph as a proxy of data augmentation.
GRACE~\cite{zhu2020deep} uses only the basic random edge dropping and attribute masking for creating different views of the graph. 

Other than data removal augmentations, graph diffusion is also commonly used in contrastive learning as it can naturally create a ``future view'' of the given graph where the information are more spread out. MVGRL~\cite{hassani2020contrastive} adopts the diffusion graph proposed by GDC~\cite{klicpera2019diffusion} as the second view. Interestingly, \citet{hassani2020contrastive} showed that using three views (original graph, PPR diffusion graph and heat kernel diffusion graph) would not result with better performance than using two views (original graph and one diffusion graph), and concluded ``increasing the number of views does not improve the performance.'' However, \citet{yuan2021semi} later proposed MV-CGC which adopted a similar contrastive learning framework with three views: the original graph, diffusion graph, and a proposed feature similarity view. Empirically, the node representations learned by MV-CGC outperformed those learned by MVGRL on node classification, suggesting that additional well-designed GDA methods or views may be helpful to graph contrastive learning approaches.

More recently, several studies \cite{suresh2021adversarial, trivedianalyzing,liu2023data} pointed out that stochastic rule-based GDA operations may suffer from failing to induce useful task-relevant invariance on common benchmark datasets. Specifically, \citet{trivedianalyzing} analyzed that the generalization error of graph contrastive learning can be bounded under the assumptions of invariance to relevant augmentations, recoverability, and separability, which refer to \textit{data-centric properties}, by instantiating rule-based GDA as a composition of graph edit operations. Such bound demonstrates conditions with low separability and recoverability during the usage of rule-based GDA, which motivates the necessity of inducing task-relevant invariance. 
Following the theoretical analysis, \citet{zhang2022costa} proposed a covariance-preserving feature augmentation technique, in which the augmented feature has bounded variance. \citet{wang2021multi} proposed to use different levels in hierarchical graphs as augmented views.

% \vspace{-0.1in}
\subsection{Non-contrastive Learning}
\label{sec:tong_noncontrastive}
While showing promising performance on various tasks, contrastive learning methods rely heavily on disagreement between data examples and their associated negative examples to avoid model collapse~\cite{grill2020bootstrap}. As sampling high quality negative examples is often costly, and random negative sampling usually requires large batch sizes, several works~\cite{grill2020bootstrap,zbontar2021barlow,balestriero2022contrastive} propose non-contrastive self-supervised learning methods to learn representations in a self-supervised manner without needing negative examples. Instead of comparing across different samples, non-contrastive self-supervised methods compare only between different views of the same sample and use designs such as prediction heads and stop gradient to avoid model collapsing~\cite{grill2020bootstrap}, or measure the cross-correlation matrix between the representations learned form different views~\cite{zbontar2021barlow}.

As the non-contrastive methods are designed for more efficient self-supervised learning than the contrastive methods, the GDA techniques they adopt are all the most basic, stochastic ones (Section~\ref{sec:tong_dataremove}). Specifically, all the non-contrastive self-supervised graph representation learning methods (CCA-SSG~\cite{zhang2021canonical}, GBT~\cite{bielak2022graph}, BGRL~\cite{thakoor2022largescale}, and T-BGRL~\cite{shiao2022link}) utilized only random edge dropping and node feature masking as the augmentation strategies. 
While the first three methods generates two augmented views for comparison, to further improve the performance on link prediction under inductive settings, T-BGRL~\cite{shiao2022link} also used the same augmentation strategies but with higher masking probability as an efficient corruption to create an third ``negative'' view to mitigate collapse, which is later used in a triplet loss.

% \vspace{-0.1in}
\subsection{Consistency Training}
\label{sec:tong_consistent}

In real GML applications, semi-supervised learning usually plays an important role as only a small fraction of training data are labeled in most of the cases~\cite{wu2020comprehensive}. Due to such label scarcity, consistency training is commonly used to leverage the unlabeled data to improve the model quality. Similar to contrastive learning, consistency training itself is a self-supervised learning objective that aims to maximizes the agreement of representations learned from different views of the data. However, unlike (non-)contrastive learning that compares between data objects, the consistency loss compares the distributions of a batch of representations via metrics like KL-divergence. Therefore, the consistency loss is rarely used itself, but often used along with supervised losses in the semi-supervised learning settings. The final learning objective is usually a linear combination of the supervised loss (e.g., cross entropy for classification tasks) and the consistency loss.

NodeAug~\cite{wang2020nodeaug} uses three local structure-based augmentation operations: replacing attributes, removing and adding edges. NodeAug minimizes the KL-divergence between the node representations learned from the original graph and augmented graph. GRAND~\cite{feng2020graph} creates multiple different augmented graphs with node dropping and feature masking. The consistency loss then minimizes the distances of the representations learned from the augmented graphs. 
NASA~\cite{bo2022regularizing} proposes Neighbor Replace augmentation to randomly replace the 1-hop neighbors with 2-hop neighbors, and then use a neighbor-constrained consistency regularization during training.
To further utilize the information given by different graph diffusions, MV-GCN~\cite{yuan2021semi} generates two complementary views with PPR and heat kernel and learns from both created views and the original graph. Then, it feeds three views of the graph into three GCNs, and uses a consistency regularization loss to reduce the distribution distance of the representations learned across the views, and derives the final node representations as a  combination of the three.



\section{Challenges and Directions}
\label{sec:tong_future}
Despite substantial progress has been achieved in graph data augmentation research, several open problems remain to solve. In this section, we summarize several promising yet under-explored research directions.

% \vspace{-0.1in}
\subsection{Domain Adaptation and Regularization}
Given the rapid development of GDA techniques in recent years, automated GDA methods have been proposed to automatically tune the augmentation strategy for different datasets and tasks. 
Nonetheless, the existing automated GDA methods for graph data (as introduced in Section~\ref{sec:tong_autoaug}) mainly focus on specific datasets and downstream tasks. 
Ideally, automated augmentation solutions should be transferable. That is, domain adaptation is a desired characteristic for automated GDA techniques. When the automated augmentation method trained on one dataset could only be used on that dataset, the method may be equivalent to automating the hyperparameter tuning process and lose the generalizability across datasets~\cite{zhao2022autogda}. Therefore, for an ideal automated GDA method, it should be able to be trained on one dataset and used for many, ideally cross domain or under OOD settings. While OOD benchmarks are already available in the GML community~\cite{gui2022good}, automated GDA methods that can be transferable across domains are still missing in the literature. 
Moreover, on certain types of graph data such as molecule graphs, most commonly used GDA operations would change the underlying semantics of the graph. For example, dropping a carbon atom from the phenyl ring of aspirin breaks the aromatic system and results in a alkene chain~\cite{lee2021augmentation}, which is an entirely different chemical compound. This motivates a need for domain-based regularization methods for such tasks. So far, only \citet{sun2021mocl} proposed MoCL that considers the semantic information brought by local substructures when augmenting the molecule graphs, leaving domain-based regularization GDA methods rather under-explored.

% \vspace{-0.1in}
\subsection{Scalability for Large-Scale Graphs}
GDA techniques add additional complexity on top of the existing GNNs, and many GDA techniques use global information during the augmentation process, which might not be able to easily scale. For example, GAug-M~\cite{zhao2021data} involves selecting the top $K$ out of $O(N^2)$ logits for node pairs when selecting edges to add. Such high complexity operations can cause scalability issues in actual applications where the graph size can be very large, e.g., at billion scale.
While complex GDA techniques bring significant performance improvements, the scalability of these methods are still worthy of attention. For example, in order to enable end-to-end training, GAug-O~\cite{zhao2021data} requires back-propagating on the entire learned adjacency matrix, creating massive memory overheads. To improve the performance of DropEdge~\cite{rong2019dropedge}, TADropEdge~\cite{gao2021training} required the pre-calculation of a score for each edge in the graph prior to the training of GNNs. Therefore, to be applicable in practical applications, efficiency is also a necessity for GDA techniques. As mentioned in the previous subsections, automated solution which combine the fast and simple augmentation operations may be a promising direction. Nonetheless, how to design a scalable and efficient automated GDA framework is still an open line of research.

% \vspace{-0.1in}
\subsection{Comprehensive Evaluation Criteria and Standards}
Similar to the DA research in other domains, a general concern for GDA research is that the evaluation only focuses on the prediction performance on specific datasets. Although this is likely the most important metric, other metrics such as additional time and resource consumption, transferability, or scalability are also important for researchers to more comprehensively understand the methods. For example, as aforementioned, while graph structure learning methods such as GAug~\cite{zhao2021data} shows promising performances for node classification, the method's design inherently limits its ability to generalize on large-scale graphs. Furthermore, only few works discuss the additional time and resource requirement needed for applying their proposed GDA methods, especially for the learned augmentations which may require training of additional modules. 
Therefore, a set of comprehensive evaluation criteria and standards is desired for better understanding the benefits and costs of the newly proposed GDA methods.
Ideally, such a benchmark could contain multiple datasets in different scales and domains, enabling researchers to better evaluate transferability and scalability tradeoffs.

% \vspace{-0.1in}
\subsection{Theoretical Foundation}
GDA is a powerful technology to improve the performance of data-driven inference on graphs without the need of extra labeling effort or complex models. Empirically, GDA methods are also shown to improve the generalization of GML methods and alleviate the over-smoothing problem encountered by GNNs. Yet, there is little rigorous understanding of how and why GDA achieves those benefits, especially for (semi-)supervised learning. Although several works~\cite{zhao2021data,chen2020measuring} have analyzed the relation between graph homophily and classification performance or the over-smoothing problem, there is limited work showcasing rigorous proofs or theoretical bounds on these relationships.

Recently, several works provided theoretical insights of DA in the CV domain. For example, \citet{wu2020generalization} theoretically analyzed the generalization effect of data augmentation on images. They interpreted the effect of data augmentation from a bias-variance perspective, where data augmentation adds new information to model training while also serving as a regularizer. Due to the irregular characteristics of graph data, these theoretical analysis cannot be directly applied for the GDA context. Besides the generalization perspective, several recent works have studied the certified robustness of GNNs~\cite{zugner2020certifiable}. Improved robustness bounds would be a desired property of GDA techniques. Recent studies~\cite{topping2021understanding} on the topology bottleneck and over-squashing of GNNs provide theoretical guides for edge-based GDA techniques. Counterfactual augmentation methods on graphs such as CFLP~\cite{zhao2021counterfactual} can also bring insights for analyzing GDA from the perspective of causality. 

% \vspace{-0.1in}
\subsection{Data Augmentation for Complex Graph Types}
Existing GDA approaches are mainly designed for homogeneous graphs, while not all of them can be easily generalized to other complex types of graphs such as heterogeneous graphs, dynamic graphs, hypergraphs, etc. These complex graphs have broader applications with their ability of modeling more complex relationship, nonetheless, the complexity of the data requires more sophisticated design of GDA methods. Taking heterogeneous graphs as an example, even the simplest edge dropping would require a drop rate hyper-parameter for each of the edge types in the graph, which could introduce significant computational overhead for hyper-parameter searching. Additionally, beyond direct analogous of GDA methods for homogeneous graph for complex graph types, specially designed GDA methods for different graph types could better utilize the rich information contained in them. Therefore, a comprehensive evaluation of the existing GDA methods on complex graphs is needed by the community to better understand the effectiveness of existing GDA methods and also better design principled augmentation approaches for each graph types.

% \vspace{-0.1in}
\section{Conclusion}
\label{sec:tong_conclusion}
Our work presents a comprehensive and structured survey of data augmentation techniques for graph machine learning (GML). We categorized existing graph data augmentation (GDA) techniques three taxonomies from different perspectives, introduced recent GDA approaches based on their core methodology, and introduced their applications in self-supervised learning. Finally, we outlined current challenges as well as directions for future research explorations in the GDA domain. We hope this survey serves as a guide for GML researchers and practitioners to study and use GDA techniques, and inspires additional interest and work on this topic.

% \small{
% \bibliographystyle{abbrvnat}
% \bibliography{ref}
% }

% \vspace{-0.1in}
\begin{thebibliography}{10}
\itemsep=1.1pt
\begin{small}

\bibitem[Balestriero and LeCun(2022)]{balestriero2022contrastive}
R.~Balestriero and Y.~LeCun.
\newblock Contrastive and non-contrastive self-supervised learning recover
  global and local spectral embedding methods.
\newblock \emph{arXiv:2205.11508}, 2022.

\bibitem[Barandela et~al.(2004)Barandela, Valdovinos, S{\'a}nchez, and
  Ferri]{barandela2004imbalanced}
R.~Barandela, R.~M. Valdovinos, J.~S. S{\'a}nchez, and F.~J. Ferri.
\newblock The imbalanced training sample problem: Under or over sampling?
\newblock In \emph{Joint IAPR international workshops on SPR and SSPR}, 2004.

\bibitem[Bielak et~al.(2022)Bielak, Kajdanowicz, and Chawla]{bielak2022graph}
P.~Bielak, T.~Kajdanowicz, and N.~V. Chawla.
\newblock Graph barlow twins: A self-supervised representation learning
  framework for graphs.
\newblock \emph{Knowledge-Based Systems}, 2022.

\bibitem[Bo et~al.(2022)Bo, Hu, Wang, Zhang, Shi, and Zhou]{bo2022regularizing}
D.~Bo, B.~Hu, X.~Wang, Z.~Zhang, C.~Shi, and J.~Zhou.
\newblock Regularizing graph neural networks via consistency-diversity graph
  augmentations.
\newblock In \emph{AAAI}, 2022.

\bibitem[Brugere et~al.(2018)Brugere, Gallagher, and
  Berger-Wolf]{brugere2018network}
I.~Brugere, B.~Gallagher, and T.~Y. Berger-Wolf.
\newblock Network structure inference, a survey: Motivations, methods, and
  applications.
\newblock \emph{CSUR}, 2018.

\bibitem[Bruna et~al.(2013)Bruna, Zaremba, Szlam, and LeCun]{bruna2013spectral}
J.~Bruna, W.~Zaremba, A.~Szlam, and Y.~LeCun.
\newblock Spectral networks and locally connected networks on graphs.
\newblock \emph{arXiv:1312.6203}, 2013.

\bibitem[Chawla et~al.(2002)Chawla, Bowyer, Hall, and
  Kegelmeyer]{chawla2002smote}
N.~V. Chawla, K.~W. Bowyer, L.~O. Hall, and W.~P. Kegelmeyer.
\newblock Smote: synthetic minority over-sampling technique.
\newblock \emph{JAIR}, 2002.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Lin, Li, Li, Zhou, and
  Sun]{chen2020measuring}
D.~Chen, Y.~Lin, W.~Li, P.~Li, J.~Zhou, and X.~Sun.
\newblock Measuring and relieving the over-smoothing problem for graph neural
  networks from the topological view.
\newblock In \emph{AAAI}, 2020{\natexlab{a}}.

\bibitem[Chen et~al.(2019)Chen, Wu, Lin, and Xuan]{chen2019can}
J.~Chen, Y.~Wu, X.~Lin, and Q.~Xuan.
\newblock Can adversarial network attack be defended?
\newblock \emph{arXiv:1903.05994}, 2019.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Lin, Xiong, Wu, Zheng, and
  Xuan]{chen2020smoothing}
J.~Chen, X.~Lin, H.~Xiong, Y.~Wu, H.~Zheng, and Q.~Xuan.
\newblock Smoothing adversarial training for gnn.
\newblock \emph{IEEE Transactions on Computational Social Systems},
  2020{\natexlab{b}}.

\bibitem[Chen et~al.(2020{\natexlab{c}})Chen, Kornblith, Norouzi, and
  Hinton]{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{ICML}, 2020{\natexlab{c}}.

\bibitem[Chen et~al.(2020{\natexlab{d}})Chen, Wu, and Zaki]{chen2020iterative}
Y.~Chen, L.~Wu, and M.~Zaki.
\newblock Iterative deep graph learning for graph neural networks: Better and
  robust node embeddings.
\newblock In \emph{NeurIPS}, 2020{\natexlab{d}}.

\bibitem[Chen et~al.(2022)Chen, Zhang, Bian, Yang, KAILI, Xie, Liu, Han, and
  Cheng]{chenlearning}
Y.~Chen, Y.~Zhang, Y.~Bian, H.~Yang, M.~KAILI, B.~Xie, T.~Liu, B.~Han, and
  J.~Cheng.
\newblock Learning causally invariant representations for out-of-distribution
  generalization on graphs.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Chierichetti et~al.(2015)Chierichetti, Epasto, Kumar, Lattanzi, and
  Mirrokni]{chierichetti2015efficient}
F.~Chierichetti, A.~Epasto, R.~Kumar, S.~Lattanzi, and V.~Mirrokni.
\newblock Efficient algorithms for public-private social networks.
\newblock In \emph{KDD}, 2015.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2019autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le.
\newblock Autoaugment: Learning augmentation strategies from data.
\newblock In \emph{CVPR}, 2019.

\bibitem[Dai et~al.(2022)Dai, Jin, Liu, and Wang]{dai2022towards}
E.~Dai, W.~Jin, H.~Liu, and S.~Wang.
\newblock Towards robust graph neural networks for noisy graphs with sparse
  labels.
\newblock In \emph{WSDM}, 2022.

\bibitem[Dai et~al.(2018)Dai, Li, Tian, Huang, Wang, Zhu, and
  Song]{dai2018adversarial}
H.~Dai, H.~Li, T.~Tian, X.~Huang, L.~Wang, J.~Zhu, and L.~Song.
\newblock Adversarial attack on graph structured data.
\newblock In \emph{ICML}, 2018.

\bibitem[Dai et~al.(2019)Dai, Shen, Zhang, Li, and Wang]{dai2019adversarial}
Q.~Dai, X.~Shen, L.~Zhang, Q.~Li, and D.~Wang.
\newblock Adversarial training methods for network embedding.
\newblock In \emph{TheWebConf}, 2019.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and
  Vandergheynst]{defferrard2016convolutional}
M.~Defferrard, X.~Bresson, and P.~Vandergheynst.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Deng et~al.(2019)Deng, Dong, and Zhu]{deng2019batch}
Z.~Deng, Y.~Dong, and J.~Zhu.
\newblock Batch virtual adversarial training for graph convolutional networks.
\newblock \emph{arXiv:1902.09192}, 2019.

\bibitem[Dong et~al.(2021)Dong, Chen, Feng, He, Bi, Ding, and
  Cui]{dong2021equivalence}
H.~Dong, J.~Chen, F.~Feng, X.~He, S.~Bi, Z.~Ding, and P.~Cui.
\newblock On the equivalence of decoupled graph convolution network and label
  propagation.
\newblock In \emph{TheWebConf}, 2021.

\bibitem[Duong et~al.(2011)Duong, Wellman, and Singh]{duong2011modeling}
Q.~Duong, M.~P. Wellman, and S.~Singh.
\newblock Modeling information diffusion in networks with unobserved links.
\newblock In \emph{IEEE PASSAT/SocialCom}, 2011.

\bibitem[Fang et~al.(2022)Fang, Xiao, Wang, Xu, Yang, and
  Yang]{fang2022dropmessage}
T.~Fang, Z.~Xiao, C.~Wang, J.~Xu, X.~Yang, and Y.~Yang.
\newblock Dropmessage: Unifying random dropping for graph neural networks.
\newblock \emph{arXiv:2204.10037}, 2022.

\bibitem[Fatemi et~al.(2021)Fatemi, El~Asri, and Kazemi]{fatemi2021slaps}
B.~Fatemi, L.~El~Asri, and S.~M. Kazemi.
\newblock Slaps: Self-supervision improves structure learning for graph neural
  networks.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Feng et~al.(2019)Feng, He, Tang, and Chua]{feng2019graph}
F.~Feng, X.~He, J.~Tang, and T.-S. Chua.
\newblock Graph adversarial training: Dynamically regularizing based on graph
  structure.
\newblock \emph{TKDE}, 2019.

\bibitem[Feng et~al.(2021)Feng, Gangal, Wei, Chandar, Vosoughi, Mitamura, and
  Hovy]{feng2021survey}
S.~Y. Feng, V.~Gangal, J.~Wei, S.~Chandar, S.~Vosoughi, T.~Mitamura, and
  E.~Hovy.
\newblock A survey of data augmentation approaches for nlp.
\newblock \emph{arXiv:2105.03075}, 2021.

\bibitem[Feng et~al.(2020)Feng, Zhang, Dong, Han, Luan, Xu, Yang, Kharlamov,
  and Tang]{feng2020graph}
W.~Feng, J.~Zhang, Y.~Dong, Y.~Han, H.~Luan, Q.~Xu, Q.~Yang, E.~Kharlamov, and
  J.~Tang.
\newblock Graph random neural networks for semi-supervised learning on graphs.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Franceschi et~al.(2019)Franceschi, Niepert, Pontil, and
  He]{franceschi2019learning}
L.~Franceschi, M.~Niepert, M.~Pontil, and X.~He.
\newblock Learning discrete structures for graph neural networks.
\newblock In \emph{ICML}, 2019.

\bibitem[Gao et~al.(2021)Gao, Bhattacharya, Zhang, Blum, Ribeiro, and
  Sadler]{gao2021training}
Z.~Gao, S.~Bhattacharya, L.~Zhang, R.~S. Blum, A.~Ribeiro, and B.~M. Sadler.
\newblock Training robust graph neural networks with topology adaptive edge
  dropping.
\newblock \emph{arXiv:2106.02892}, 2021.

\bibitem[Garrison et~al.(2015)Garrison, Scheinost, Finn, Shen, and
  Constable]{garrison2015stability}
K.~A. Garrison, D.~Scheinost, E.~S. Finn, X.~Shen, and R.~T. Constable.
\newblock The (in) stability of functional brain network measures across
  thresholds.
\newblock \emph{Neuroimage}, 2015.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
J.~Gilmer, S.~S. Schoenholz, P.~F. Riley, O.~Vinyals, and G.~E. Dahl.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{ICML}, 2017.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv:1412.6572}, 2014.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar,
  et~al.]{grill2020bootstrap}
J.-B. Grill, F.~Strub, F.~Altch{\'e}, C.~Tallec, P.~Richemond, E.~Buchatskaya,
  C.~Doersch, B.~Avila~Pires, Z.~Guo, M.~Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Grover and Leskovec(2016)]{grover2016node2vec}
A.~Grover and J.~Leskovec.
\newblock node2vec: Scalable feature learning for networks.
\newblock In \emph{KDD}, 2016.

\bibitem[Gui et~al.(2022)Gui, Li, Wang, and Ji]{gui2022good}
S.~Gui, X.~Li, L.~Wang, and S.~Ji.
\newblock Good: A graph out-of-distribution benchmark.
\newblock \emph{NeurIPS}, 2022.

\bibitem[G{\"u}nnemann(2022)]{gunnemann2022graph}
S.~G{\"u}nnemann.
\newblock Graph neural networks: Adversarial robustness.
\newblock In \emph{Graph Neural Networks: Foundations, Frontiers, and
  Applications}. 2022.

\bibitem[Guo and Mao(2021)]{guo2021intrusion}
H.~Guo and Y.~Mao.
\newblock Intrusion-free graph mixup.
\newblock \emph{arXiv:2110.09344}, 2021.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017inductive}
W.~Hamilton, Z.~Ying, and J.~Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Han et~al.(2022)Han, Jiang, Liu, and Hu]{han2022G}
X.~Han, Z.~Jiang, N.~Liu, and X.~Hu.
\newblock G-mixup: Graph data augmentation for graph classification.
\newblock In \emph{ICML}, 2022.

\bibitem[Hassani and Khasahmadi(2020)]{hassani2020contrastive}
K.~Hassani and A.~H. Khasahmadi.
\newblock Contrastive multi-view representation learning on graphs.
\newblock In \emph{ICML}, 2020.

\bibitem[Hassani and Khasahmadi(2022)]{hassani2022learning}
K.~Hassani and A.~H. Khasahmadi.
\newblock Learning graph augmentations to learn graph representations.
\newblock \emph{arXiv:2201.09830}, 2022.

\bibitem[Holland et~al.(1983)Holland, Laskey, and
  Leinhardt]{holland1983stochastic}
P.~W. Holland, K.~B. Laskey, and S.~Leinhardt.
\newblock Stochastic blockmodels: First steps.
\newblock \emph{Social networks}, 1983.

\bibitem[Hu et~al.(2021{\natexlab{a}})Hu, Chen, Chang, Zheng, and
  Du]{hu2021robust}
W.~Hu, C.~Chen, Y.~Chang, Z.~Zheng, and Y.~Du.
\newblock Robust graph convolutional networks with directional graph
  adversarial training.
\newblock \emph{Applied Intelligence}, 2021{\natexlab{a}}.

\bibitem[Hu et~al.(2021{\natexlab{b}})Hu, Fey, Ren, Nakata, Dong, and
  Leskovec]{hu2021ogb}
W.~Hu, M.~Fey, H.~Ren, M.~Nakata, Y.~Dong, and J.~Leskovec.
\newblock Ogb-lsc: A large-scale challenge for machine learning on graphs.
\newblock \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Hwang et~al.(2021)Hwang, Thost, Dasgupta, and Ma]{hwang2021revisiting}
E.~Hwang, V.~Thost, S.~S. Dasgupta, and T.~Ma.
\newblock Revisiting virtual nodes in graph neural networks for link
  prediction.
\newblock 2021.

\bibitem[Ishiguro et~al.(2019)Ishiguro, Maeda, and Koyama]{ishiguro2019graph}
K.~Ishiguro, S.-i. Maeda, and M.~Koyama.
\newblock Graph warp module: an auxiliary module for boosting the power of
  graph neural networks in molecular graph analysis.
\newblock \emph{arXiv preprint arXiv:1902.01020}, 2019.

\bibitem[Jiang et~al.(2019)Jiang, Zhang, Lin, Tang, and Luo]{jiang2019semi}
B.~Jiang, Z.~Zhang, D.~Lin, J.~Tang, and B.~Luo.
\newblock Semi-supervised learning with graph learning-convolutional networks.
\newblock In \emph{CVPR}, 2019.

\bibitem[Jiao et~al.(2020)Jiao, Xiong, Zhang, Zhang, Zhang, and
  Zhu]{jiao2020sub}
Y.~Jiao, Y.~Xiong, J.~Zhang, Y.~Zhang, T.~Zhang, and Y.~Zhu.
\newblock Sub-graph contrast for scalable self-supervised graph representation
  learning.
\newblock In \emph{ICDM}, 2020.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Li, Xu, Wang, Ji, Aggarwal, and
  Tang]{jin2020adversarial}
W.~Jin, Y.~Li, H.~Xu, Y.~Wang, S.~Ji, C.~Aggarwal, and J.~Tang.
\newblock Adversarial attacks and defenses on graphs: A review, a tool and
  empirical studies.
\newblock \emph{arXiv:2003.00653}, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Ma, Liu, Tang, Wang, and
  Tang]{jin2020graph}
W.~Jin, Y.~Ma, X.~Liu, X.~Tang, S.~Wang, and J.~Tang.
\newblock Graph structure learning for robust graph neural networks.
\newblock In \emph{KDD}, 2020{\natexlab{b}}.

\bibitem[Jin et~al.(2021)Jin, Li, Xu, Wang, Ji, Aggarwal, and
  Tang]{jin2021adversarial}
W.~Jin, Y.~Li, H.~Xu, Y.~Wang, S.~Ji, C.~Aggarwal, and J.~Tang.
\newblock Adversarial attacks and defenses on graphs.
\newblock \emph{SIGKDD Explorations}, 2021.

\bibitem[Jin et~al.(2022{\natexlab{a}})Jin, Tang, Jiang, Li, Zhang, Tang, and
  Yin]{jin2022condensing}
W.~Jin, X.~Tang, H.~Jiang, Z.~Li, D.~Zhang, J.~Tang, and B.~Yin.
\newblock Condensing graphs via one-step gradient matching.
\newblock In \emph{KDD}, 2022{\natexlab{a}}.

\bibitem[Jin et~al.(2022{\natexlab{b}})Jin, Zhao, Zhang, Liu, Tang, and
  Shah]{jin2022graph}
W.~Jin, L.~Zhao, S.~Zhang, Y.~Liu, J.~Tang, and N.~Shah.
\newblock Graph condensation for graph neural networks.
\newblock In \emph{ICLR}, 2022{\natexlab{b}}.

\bibitem[Jin et~al.(2023)Jin, Zhao, Ding, Liu, Tang, and
  Shah]{jin2022empowering}
W.~Jin, T.~Zhao, J.~Ding, Y.~Liu, J.~Tang, and N.~Shah.
\newblock Empowering graph representation learning with test-time graph
  transformation.
\newblock \emph{ICLR}, 2023.

\bibitem[Joachims and Swaminathan(2016)]{joachims2016counterfactual}
T.~Joachims and A.~Swaminathan.
\newblock Counterfactual evaluation and learning for search, recommendation and
  ad placement.
\newblock In \emph{SIGIR}, 2016.

\bibitem[Ju et~al.(2023)Ju, Zhao, Wen, Yu, Shah, Ye, and Zhang]{ju2022multi}
M.~Ju, T.~Zhao, Q.~Wen, W.~Yu, N.~Shah, Y.~Ye, and C.~Zhang.
\newblock Multi-task self-supervised graph neural networks enable stronger task
  generalization.
\newblock \emph{ICLR}, 2023.

\bibitem[Kipf and Welling(2016{\natexlab{a}})]{kipf2016semi}
T.~N. Kipf and M.~Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{arXiv:1609.02907}, 2016{\natexlab{a}}.

\bibitem[Kipf and Welling(2016{\natexlab{b}})]{kipf2016variational}
T.~N. Kipf and M.~Welling.
\newblock Variational graph auto-encoders.
\newblock \emph{arXiv:1611.07308}, 2016{\natexlab{b}}.

\bibitem[Klicpera et~al.(2018)Klicpera, Bojchevski, and
  G{\"u}nnemann]{klicpera2018predict}
J.~Klicpera, A.~Bojchevski, and S.~G{\"u}nnemann.
\newblock Predict then propagate: Graph neural networks meet personalized
  pagerank.
\newblock \emph{arXiv:1810.05997}, 2018.

\bibitem[Klicpera et~al.(2019)Klicpera, Wei{\ss}enberger, and
  G{\"u}nnemann]{klicpera2019diffusion}
J.~Klicpera, S.~Wei{\ss}enberger, and S.~G{\"u}nnemann.
\newblock Diffusion improves graph learning.
\newblock \emph{NeurIPS}, 2019.

\bibitem[Kondor and Lafferty(2002)]{kondor2002diffusion}
R.~I. Kondor and J.~Lafferty.
\newblock Diffusion kernels on graphs and other discrete structures.
\newblock In \emph{ICML}, 2002.

\bibitem[Kong et~al.(2020)Kong, Li, Ding, Wu, Zhu, Ghanem, Taylor, and
  Goldstein]{kong2020flag}
K.~Kong, G.~Li, M.~Ding, Z.~Wu, C.~Zhu, B.~Ghanem, G.~Taylor, and T.~Goldstein.
\newblock Flag: Adversarial data augmentation for graph neural networks.
\newblock \emph{arXiv:2010.09891}, 2020.

\bibitem[Kong et~al.(2022)Kong, Li, Ding, Wu, Zhu, Ghanem, Taylor, and
  Goldstein]{kong2022robust}
K.~Kong, G.~Li, M.~Ding, Z.~Wu, C.~Zhu, B.~Ghanem, G.~Taylor, and T.~Goldstein.
\newblock Robust optimization as data augmentation for large-scale graphs.
\newblock In \emph{CVPR}, 2022.

\bibitem[Kose and Shen(2022)]{kose2022fair}
O.~D. Kose and Y.~Shen.
\newblock Fair node representation learning via adaptive data augmentation.
\newblock \emph{arXiv:2201.08549}, 2022.

\bibitem[Kumar and Shah(2018)]{kumar2018false}
S.~Kumar and N.~Shah.
\newblock False information on web and social media: A survey.
\newblock \emph{arXiv:1804.08559}, 2018.

\bibitem[Lee et~al.(2021)Lee, Lee, and Park]{lee2021augmentation}
N.~Lee, J.~Lee, and C.~Park.
\newblock Augmentation-free self-supervised learning on graphs.
\newblock In \emph{AAAI}, 2021.

\bibitem[Levie et~al.(2018)Levie, Monti, Bresson, and
  Bronstein]{levie2018cayleynets}
R.~Levie, F.~Monti, X.~Bresson, and M.~M. Bronstein.
\newblock Cayleynets: Graph convolutional neural networks with complex rational
  spectral filters.
\newblock \emph{IEEE Transactions on Signal Processing}, 2018.

\bibitem[Li et~al.(2022{\natexlab{a}})Li, Zhang, Wang, and Zhu]{li2022learning}
H.~Li, Z.~Zhang, X.~Wang, and W.~Zhu.
\newblock Learning invariant graph representations for out-of-distribution
  generalization.
\newblock In \emph{NeurIPS}, 2022{\natexlab{a}}.

\bibitem[Li et~al.(2017)Li, Cai, and He]{li2017learning}
J.~Li, D.~Cai, and X.~He.
\newblock Learning graph-level representation for drug discovery.
\newblock \emph{arXiv preprint arXiv:1709.03741}, 2017.

\bibitem[Li et~al.(2022{\natexlab{b}})Li, Wen, Deng, Feng, Hu, Wang, and
  Fan]{li2022graph}
X.~Li, L.~Wen, Y.~Deng, F.~Feng, X.~Hu, L.~Wang, and Z.~Fan.
\newblock Graph neural network with curriculum learning for imbalanced node
  classification.
\newblock \emph{arXiv:2202.02529}, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Zhao, Xu, Luo, and
  Jiang]{liu2022graph}
G.~Liu, T.~Zhao, J.~Xu, T.~Luo, and M.~Jiang.
\newblock Graph rationalization with environment-based augmentations.
\newblock In \emph{KDD}, 2022{\natexlab{a}}.

\bibitem[Liu et~al.(2023)Liu, Inae, Zhao, Xu, Luo, and Jiang]{liu2023data}
G.~Liu, E.~Inae, T.~Zhao, J.~Xu, T.~Luo, and M.~Jiang.
\newblock Data-centric learning from unlabeled graphs with diffusion model.
\newblock \emph{arXiv preprint arXiv:2303.10108}, 2023.

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Li, Chen, and Song]{liu2022graphc}
M.~Liu, S.~Li, X.~Chen, and L.~Song.
\newblock Graph condensation via receptive field distribution matching.
\newblock \emph{arXiv:2206.13697}, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2022{\natexlab{c}})Liu, Wang, Wu, Chen, Guo, and
  Shi]{liu2022compact}
N.~Liu, X.~Wang, L.~Wu, Y.~Chen, X.~Guo, and C.~Shi.
\newblock Compact graph structure learning via mutual information compression.
\newblock In \emph{TheWebConf}, 2022{\natexlab{c}}.

\bibitem[Liu et~al.(2021)Liu, Dong, Li, Xu, Rong, Zhao, Huang, and
  Wu]{liu2021local}
S.~Liu, H.~Dong, L.~Li, T.~Xu, Y.~Rong, P.~Zhao, J.~Huang, and D.~Wu.
\newblock Local augmentation for graph neural networks.
\newblock \emph{arXiv:2109.03856}, 2021.

\bibitem[Liu et~al.(2022{\natexlab{d}})Liu, Zhang, Liu, and
  Zhu]{liu2022gatsmote}
Y.~Liu, Z.~Zhang, Y.~Liu, and Y.~Zhu.
\newblock Gatsmote: Improving imbalanced node classification on graphs via
  attention and homophily.
\newblock \emph{Mathematics}, 2022{\natexlab{d}}.

\bibitem[Liu et~al.(2022{\natexlab{e}})Liu, Zheng, Zhang, Chen, Peng, and
  Pan]{liu2022towards}
Y.~Liu, Y.~Zheng, D.~Zhang, H.~Chen, H.~Peng, and S.~Pan.
\newblock Towards unsupervised deep graph structure learning.
\newblock In \emph{TheWebConf}, 2022{\natexlab{e}}.

\bibitem[Luo et~al.(2021)Luo, Cheng, Yu, Zong, Ni, Chen, and
  Zhang]{luo2021learning}
D.~Luo, W.~Cheng, W.~Yu, B.~Zong, J.~Ni, H.~Chen, and X.~Zhang.
\newblock Learning to drop: Robust graph neural network via topological
  denoising.
\newblock In \emph{WSDM}, 2021.

\bibitem[Luo et~al.(2022)Luo, McThrow, Au, Komikado, Uchino, Maruhash, and
  Ji]{luo2022automated}
Y.~Luo, M.~McThrow, W.~Y. Au, T.~Komikado, K.~Uchino, K.~Maruhash, and S.~Ji.
\newblock Automated data augmentations for graph classification.
\newblock \emph{arXiv:2202.13248}, 2022.

\bibitem[Ma et~al.(2021)Ma, Liu, Zhao, Liu, Tang, and Shah]{ma2021unified}
Y.~Ma, X.~Liu, T.~Zhao, Y.~Liu, J.~Tang, and N.~Shah.
\newblock A unified view on graph neural networks as graph signal denoising.
\newblock In \emph{CIKM}, 2021.

\bibitem[Miao et~al.(2022)Miao, Liu, and Li]{miao2022interpretable}
S.~Miao, M.~Liu, and P.~Li.
\newblock Interpretable and generalizable graph learning via stochastic
  attention mechanism.
\newblock In \emph{ICML}, 2022.

\bibitem[Mujkanovic et~al.(2022)Mujkanovic, Geisler, G{\"u}nnemann, and
  Bojchevski]{mujkanovic2022defenses}
F.~Mujkanovic, S.~Geisler, S.~G{\"u}nnemann, and A.~Bojchevski.
\newblock Are defenses for graph neural networks robust?
\newblock \emph{NeurIPS}, 2022.

\bibitem[Niu and Bansal(2019)]{niu2019automatically}
T.~Niu and M.~Bansal.
\newblock Automatically learning data augmentation policies for dialogue tasks.
\newblock \emph{arXiv:1909.12868}, 2019.

\bibitem[Page et~al.(1999)Page, Brin, Motwani, and Winograd]{page1999pagerank}
L.~Page, S.~Brin, R.~Motwani, and T.~Winograd.
\newblock The pagerank citation ranking: Bringing order to the web.
\newblock Technical report, Stanford InfoLab, 1999.

\bibitem[Park et~al.(2021{\natexlab{a}})Park, Lee, Kim, Park, Jeong, Kim, Ha,
  and Kim]{park2021metropolis}
H.~Park, S.~Lee, S.~Kim, J.~Park, J.~Jeong, K.-M. Kim, J.-W. Ha, and H.~J. Kim.
\newblock Metropolis-hastings data augmentation for graph neural networks.
\newblock \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Park et~al.(2021{\natexlab{b}})Park, Shim, and Yang]{park2021graph}
J.~Park, H.~Shim, and E.~Yang.
\newblock Graph transplant: Node saliency-guided graph mixup with local
  structure preservation.
\newblock \emph{arXiv:2111.05639}, 2021{\natexlab{b}}.

\bibitem[Perozzi et~al.(2014)Perozzi, Al-Rfou, and Skiena]{perozzi2014deepwalk}
B.~Perozzi, R.~Al-Rfou, and S.~Skiena.
\newblock Deepwalk: Online learning of social representations.
\newblock In \emph{KDD}, 2014.

\bibitem[Rong et~al.(2019)Rong, Huang, Xu, and Huang]{rong2019dropedge}
Y.~Rong, W.~Huang, T.~Xu, and J.~Huang.
\newblock Dropedge: Towards deep graph convolutional networks on node
  classification.
\newblock \emph{arXiv:1907.10903}, 2019.

\bibitem[Sennrich et~al.(2015)Sennrich, Haddow, and
  Birch]{sennrich2015improving}
R.~Sennrich, B.~Haddow, and A.~Birch.
\newblock Improving neural machine translation models with monolingual data.
\newblock \emph{arXiv:1511.06709}, 2015.

\bibitem[Shah et~al.(2014)Shah, Beutel, Gallagher, and
  Faloutsos]{shah2014spotting}
N.~Shah, A.~Beutel, B.~Gallagher, and C.~Faloutsos.
\newblock Spotting suspicious link behavior with fbox: An adversarial
  perspective.
\newblock In \emph{ICDM}, 2014.

\bibitem[Shang et~al.(2021)Shang, Chen, and Bi]{shang2021discrete}
C.~Shang, J.~Chen, and J.~Bi.
\newblock Discrete graph structure learning for forecasting multiple time
  series.
\newblock In \emph{ICLR}, 2021.

\bibitem[Shiao et~al.(2022)Shiao, Guo, Zhao, Papalexakis, Liu, and
  Shah]{shiao2022link}
W.~Shiao, Z.~Guo, T.~Zhao, E.~E. Papalexakis, Y.~Liu, and N.~Shah.
\newblock Link prediction with non-contrastive learning.
\newblock \emph{arXiv:2211.14394}, 2022.

\bibitem[Shorten and Khoshgoftaar(2019)]{shorten2019survey}
C.~Shorten and T.~M. Khoshgoftaar.
\newblock A survey on image data augmentation for deep learning.
\newblock \emph{Journal of Big Data}, 2019.

\bibitem[Song et~al.(2021)Song, Giunchiglia, Zhao, and Xu]{song2021topological}
R.~Song, F.~Giunchiglia, K.~Zhao, and H.~Xu.
\newblock Topological regularization for graph neural networks augmentation.
\newblock \emph{arXiv:2104.02478}, 2021.

\bibitem[Spinelli et~al.(2021)Spinelli, Scardapane, Hussain, and
  Uncini]{spinelli2021fairdrop}
I.~Spinelli, S.~Scardapane, A.~Hussain, and A.~Uncini.
\newblock Fairdrop: Biased edge dropout for enhancing fairness in graph
  representation learning.
\newblock \emph{TAI}, 2021.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{JMLR}, 2014.

\bibitem[Sui et~al.(2022)Sui, Wang, Wu, Zhang, and He]{sui2022adversarial}
Y.~Sui, X.~Wang, J.~Wu, A.~Zhang, and X.~He.
\newblock Adversarial causal augmentation for graph covariate shift.
\newblock \emph{arXiv preprint arXiv:2211.02843}, 2022.

\bibitem[Sun et~al.(2021{\natexlab{a}})Sun, Wang, and Wu]{sun2021automated}
J.~Sun, B.~Wang, and B.~Wu.
\newblock Automated graph representation learning for node classification.
\newblock In \emph{IJCNN}, 2021{\natexlab{a}}.

\bibitem[Sun et~al.(2021{\natexlab{b}})Sun, Xing, Wang, Chen, and
  Zhou]{sun2021mocl}
M.~Sun, J.~Xing, H.~Wang, B.~Chen, and J.~Zhou.
\newblock Mocl: data-driven molecular fingerprint via knowledge-aware
  contrastive learning from molecular graph.
\newblock In \emph{KDD}, 2021{\natexlab{b}}.

\bibitem[Sun et~al.(2022)Sun, Li, Peng, Wu, Fu, Ji, and Philip]{sun2022graph}
Q.~Sun, J.~Li, H.~Peng, J.~Wu, X.~Fu, C.~Ji, and S.~Y. Philip.
\newblock Graph structure learning with variational information bottleneck.
\newblock In \emph{AAAI}, 2022.

\bibitem[Suresh et~al.(2021)Suresh, Li, Hao, and
  Neville]{suresh2021adversarial}
S.~Suresh, P.~Li, C.~Hao, and J.~Neville.
\newblock Adversarial graph augmentation to improve graph contrastive learning.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Thakoor et~al.(2022)Thakoor, Tallec, Azar, Azabou, Dyer, Munos,
  Veli{\v{c}}kovi{\'c}, and Valko]{thakoor2022largescale}
S.~Thakoor, C.~Tallec, M.~G. Azar, M.~Azabou, E.~L. Dyer, R.~Munos,
  P.~Veli{\v{c}}kovi{\'c}, and M.~Valko.
\newblock Large-scale representation learning on graphs via bootstrapping.
\newblock In \emph{ICLR}, 2022.

\bibitem[Tishby et~al.(2000)Tishby, Pereira, and Bialek]{tishby2000information}
N.~Tishby, F.~C. Pereira, and W.~Bialek.
\newblock The information bottleneck method.
\newblock \emph{arXiv}, 2000.

\bibitem[Topping et~al.(2022)Topping, Di~Giovanni, Chamberlain, Dong, and
  Bronstein]{topping2021understanding}
J.~Topping, F.~Di~Giovanni, B.~P. Chamberlain, X.~Dong, and M.~M. Bronstein.
\newblock Understanding over-squashing and bottlenecks on graphs via curvature.
\newblock In \emph{ICLR}, 2022.

\bibitem[Trivedi et~al.(2021)Trivedi, Lubana, Yan, Yang, and
  Koutra]{trivedi2021augmentations}
P.~Trivedi, E.~S. Lubana, Y.~Yan, Y.~Yang, and D.~Koutra.
\newblock Augmentations in graph contrastive learning: Current methodological
  flaws \& towards better practices.
\newblock \emph{arXiv:2111.03220}, 2021.

\bibitem[Trivedi et~al.(2022)Trivedi, Lubana, Heimann, Koutra, and
  Thiagarajan]{trivedianalyzing}
P.~Trivedi, E.~S. Lubana, M.~Heimann, D.~Koutra, and J.~J. Thiagarajan.
\newblock Analyzing data-centric properties for graph contrastive learning.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2017)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Lio, and Bengio]{velivckovic2017graph}
P.~Veli{\v{c}}kovi{\'c}, G.~Cucurull, A.~Casanova, A.~Romero, P.~Lio, and
  Y.~Bengio.
\newblock Graph attention networks.
\newblock \emph{arXiv:1710.10903}, 2017.

\bibitem[Velickovic et~al.(2019)Velickovic, Fedus, Hamilton, Li{\`o}, Bengio,
  and Hjelm]{velickovic2019deep}
P.~Velickovic, W.~Fedus, W.~L. Hamilton, P.~Li{\`o}, Y.~Bengio, and R.~D.
  Hjelm.
\newblock Deep graph infomax.
\newblock \emph{ICLR}, 2019.

\bibitem[Verma et~al.(2019{\natexlab{a}})Verma, Lamb, Beckham, Najafi,
  Mitliagkas, Lopez-Paz, and Bengio]{verma2019manifold}
V.~Verma, A.~Lamb, C.~Beckham, A.~Najafi, I.~Mitliagkas, D.~Lopez-Paz, and
  Y.~Bengio.
\newblock Manifold mixup: Better representations by interpolating hidden
  states.
\newblock In \emph{ICML}, 2019{\natexlab{a}}.

\bibitem[Verma et~al.(2019{\natexlab{b}})Verma, Qu, Lamb, Bengio, Kannala, and
  Tang]{verma2019graphmix}
V.~Verma, M.~Qu, A.~Lamb, Y.~Bengio, J.~Kannala, and J.~Tang.
\newblock Graphmix: Regularized training of graph neural networks for
  semi-supervised learning.
\newblock \emph{arXiv:1909.11715}, 2019{\natexlab{b}}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Mou, Wang, Xiao, Ju, Shi, and
  Xie]{wang2021graph}
R.~Wang, S.~Mou, X.~Wang, W.~Xiao, Q.~Ju, C.~Shi, and X.~Xie.
\newblock Graph structure estimation neural networks.
\newblock In \emph{TheWebConf}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2019)Wang, Liu, and
  Hsieh]{wang2019graphdefense-adv-training}
X.~Wang, X.~Liu, and C.-J. Hsieh.
\newblock Graphdefense: Towards robust graph convolutional networks, 2019.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Wang, Liang, Cai, and
  Hooi]{wang2020graphcrop}
Y.~Wang, W.~Wang, Y.~Liang, Y.~Cai, and B.~Hooi.
\newblock Graphcrop: Subgraph cropping for graph classification.
\newblock \emph{arXiv:2009.10564}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Wang, Liang, Cai, Liu, and
  Hooi]{wang2020nodeaug}
Y.~Wang, W.~Wang, Y.~Liang, Y.~Cai, J.~Liu, and B.~Hooi.
\newblock Nodeaug: Semi-supervised node classification with data augmentation.
\newblock In \emph{KDD}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Min, Chen, and Wu]{wang2021multi}
Y.~Wang, Y.~Min, X.~Chen, and J.~Wu.
\newblock Multi-view graph contrastive representation learning for drug-drug
  interaction prediction.
\newblock In \emph{TheWebConf}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2021{\natexlab{c}})Wang, Min, Shao, and
  Wu]{wang2021molecular}
Y.~Wang, Y.~Min, E.~Shao, and J.~Wu.
\newblock Molecular graph contrastive learning with parameterized explainable
  augmentations.
\newblock In \emph{BIBM}, 2021{\natexlab{c}}.

\bibitem[Wang et~al.(2021{\natexlab{d}})Wang, Wang, Liang, Cai, and
  Hooi]{wang2021mixup}
Y.~Wang, W.~Wang, Y.~Liang, Y.~Cai, and B.~Hooi.
\newblock Mixup for node and graph classification.
\newblock In \emph{TheWebConf}, 2021{\natexlab{d}}.

\bibitem[Wei and Zou(2019)]{wei2019eda}
J.~Wei and K.~Zou.
\newblock Eda: Easy data augmentation techniques for boosting performance on
  text classification tasks.
\newblock \emph{arXiv:1901.11196}, 2019.

\bibitem[Wu et~al.(2020{\natexlab{a}})Wu, Zhang, Valiant, and
  R{\'e}]{wu2020generalization}
S.~Wu, H.~Zhang, G.~Valiant, and C.~R{\'e}.
\newblock On the generalization effects of linear transformations in data
  augmentation.
\newblock In \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Wu et~al.(2021)Wu, Wang, Zhang, He, and Chua]{wu2021discovering}
Y.~Wu, X.~Wang, A.~Zhang, X.~He, and T.-S. Chua.
\newblock Discovering invariant rationales for graph neural networks.
\newblock In \emph{ICLR}, 2021.

\bibitem[Wu et~al.(2020{\natexlab{b}})Wu, Pan, Chen, Long, Zhang, and
  Philip]{wu2020comprehensive}
Z.~Wu, S.~Pan, F.~Chen, G.~Long, C.~Zhang, and S.~Y. Philip.
\newblock A comprehensive survey on graph neural networks.
\newblock \emph{TNNLS}, 2020{\natexlab{b}}.

\bibitem[Xie et~al.(2022)Xie, Xu, Zhang, Wang, and Ji]{xie2022self}
Y.~Xie, Z.~Xu, J.~Zhang, Z.~Wang, and S.~Ji.
\newblock Self-supervised learning of graph neural networks: A unified review.
\newblock \emph{TPAMI}, 2022.

\bibitem[Xu et~al.(2021{\natexlab{a}})Xu, Cheng, Luo, Chen, and
  Zhang]{xu2021infogcl}
D.~Xu, W.~Cheng, D.~Luo, H.~Chen, and X.~Zhang.
\newblock Infogcl: Information-aware graph contrastive learning.
\newblock \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Xu et~al.(2021{\natexlab{b}})Xu, Xiang, Yu, Cao, and
  Wang]{xu2021speedup}
H.~Xu, L.~Xiang, J.~Yu, A.~Cao, and X.~Wang.
\newblock Speedup robust graph structure learning with low-rank information.
\newblock In \emph{CIKM}, 2021{\natexlab{b}}.

\bibitem[Xu et~al.(2019)Xu, Chen, Liu, Chen, Weng, Hong, and
  Lin]{xu2019topology}
K.~Xu, H.~Chen, S.~Liu, P.-Y. Chen, T.-W. Weng, M.~Hong, and X.~Lin.
\newblock Topology attack and defense for graph neural networks: An
  optimization perspective.
\newblock \emph{arXiv:1906.04214}, 2019.

\bibitem[Yang et~al.(2019)Yang, Kang, Cao, Jin, Yang, and
  Guo]{yang2019topology}
L.~Yang, Z.~Kang, X.~Cao, D.~Jin, B.~Yang, and Y.~Guo.
\newblock Topology optimization based graph convolutional network.
\newblock In \emph{IJCAI}, 2019.

\bibitem[Ying et~al.(2021)Ying, Cai, Luo, Zheng, Ke, He, Shen, and
  Liu]{ying2021transformers}
C.~Ying, T.~Cai, S.~Luo, S.~Zheng, G.~Ke, D.~He, Y.~Shen, and T.-Y. Liu.
\newblock Do transformers really perform badly for graph representation?
\newblock \emph{NeurIPS}, 2021.

\bibitem[Ying et~al.(2018)Ying, He, Chen, Eksombatchai, Hamilton, and
  Leskovec]{ying2018graph}
R.~Ying, R.~He, K.~Chen, P.~Eksombatchai, W.~L. Hamilton, and J.~Leskovec.
\newblock Graph convolutional neural networks for web-scale recommender
  systems.
\newblock In \emph{KDD}, 2018.

\bibitem[You et~al.(2020)You, Chen, Sui, Chen, Wang, and Shen]{you2020graph}
Y.~You, T.~Chen, Y.~Sui, T.~Chen, Z.~Wang, and Y.~Shen.
\newblock Graph contrastive learning with augmentations.
\newblock \emph{NeurIPS}, 2020.

\bibitem[You et~al.(2021)You, Chen, Shen, and Wang]{you2021graph}
Y.~You, T.~Chen, Y.~Shen, and Z.~Wang.
\newblock Graph contrastive learning automated.
\newblock In \emph{ICML}, 2021.

\bibitem[Yu et~al.(2020)Yu, Xu, Rong, Bian, Huang, and He]{yu2020graph}
J.~Yu, T.~Xu, Y.~Rong, Y.~Bian, J.~Huang, and R.~He.
\newblock Graph information bottleneck for subgraph recognition.
\newblock \emph{arXiv:2010.05563}, 2020.

\bibitem[Yuan et~al.(2021)Yuan, Yu, Cao, Xu, Xie, and Wang]{yuan2021semi}
J.~Yuan, H.~Yu, M.~Cao, M.~Xu, J.~Xie, and C.~Wang.
\newblock Semi-supervised and self-supervised classification with multi-view
  graph neural networks.
\newblock In \emph{CIKM}, 2021.

\bibitem[Zbontar et~al.(2021)Zbontar, Jing, Misra, LeCun, and
  Deny]{zbontar2021barlow}
J.~Zbontar, L.~Jing, I.~Misra, Y.~LeCun, and S.~Deny.
\newblock Barlow twins: Self-supervised learning via redundancy reduction.
\newblock In \emph{ICML}, 2021.

\bibitem[Zhang and Ma(2020)]{zhang2020defensevgae}
A.~Zhang and J.~Ma.
\newblock Defensevgae: Defending against adversarial attacks on graph data via
  a variational graph autoencoder.
\newblock \emph{arXiv:2006.08900}, 2020.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv:1710.09412}, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Wu, Yan, Wipf, and Yu]{zhang2021canonical}
H.~Zhang, Q.~Wu, J.~Yan, D.~Wipf, and P.~S. Yu.
\newblock From canonical correlation analysis to self-supervised graph neural
  networks.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Zhang and Zitnik(2020)]{zhang2020gnnguard}
X.~Zhang and M.~Zitnik.
\newblock Gnnguard: Defending graph neural networks against adversarial
  attacks.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Pal, Coates, and Ustebay]{zhang2019bayesian}
Y.~Zhang, S.~Pal, M.~Coates, and D.~Ustebay.
\newblock Bayesian graph convolutional neural networks for semi-supervised
  classification.
\newblock In \emph{AAAI}, 2019.

\bibitem[Zhang et~al.(2022)Zhang, Zhu, Song, Koniusz, and King]{zhang2022costa}
Y.~Zhang, H.~Zhu, Z.~Song, P.~Koniusz, and I.~King.
\newblock Costa: Covariance-preserving feature augmentation for graph
  contrastive learning.
\newblock In \emph{KDD}, 2022.

\bibitem[Zhao et~al.(2021{\natexlab{a}})Zhao, Liu, Neves, Woodford, Jiang, and
  Shah]{zhao2021data}
T.~Zhao, Y.~Liu, L.~Neves, O.~Woodford, M.~Jiang, and N.~Shah.
\newblock Data augmentation for graph neural networks.
\newblock In \emph{AAAI}, 2021{\natexlab{a}}.

\bibitem[Zhao et~al.(2021{\natexlab{b}})Zhao, Ni, Yu, Guo, Shah, and
  Jiang]{zhao2021action}
T.~Zhao, B.~Ni, W.~Yu, Z.~Guo, N.~Shah, and M.~Jiang.
\newblock Action sequence augmentation for early graph-based anomaly detection.
\newblock In \emph{CIKM}, 2021{\natexlab{b}}.

\bibitem[Zhao et~al.(2021{\natexlab{c}})Zhao, Zhang, and
  Wang]{zhao2021graphsmote}
T.~Zhao, X.~Zhang, and S.~Wang.
\newblock Graphsmote: Imbalanced node classification on graphs with graph
  neural networks.
\newblock In \emph{WSDM}, 2021{\natexlab{c}}.

\bibitem[Zhao et~al.(2022{\natexlab{a}})Zhao, Liu, Wang, Yu, and
  Jiang]{zhao2021counterfactual}
T.~Zhao, G.~Liu, D.~Wang, W.~Yu, and M.~Jiang.
\newblock Learning from counterfactual links for link prediction.
\newblock In \emph{ICML}, 2022{\natexlab{a}}.

\bibitem[Zhao et~al.(2022{\natexlab{b}})Zhao, Tang, Zhang, Jiang, Rao, Song,
  Agrawal, Subbian, Yin, and Jiang]{zhao2022autogda}
T.~Zhao, X.~Tang, D.~Zhang, H.~Jiang, N.~Rao, Y.~Song, P.~Agrawal, K.~Subbian,
  B.~Yin, and M.~Jiang.
\newblock Autogda: Automated graph data augmentation for node classification.
\newblock In \emph{LoG}, 2022{\natexlab{b}}.

\bibitem[Zheng et~al.(2020)Zheng, Zong, Cheng, Song, Ni, Yu, Chen, and
  Wang]{zheng2020robust}
C.~Zheng, B.~Zong, W.~Cheng, D.~Song, J.~Ni, W.~Yu, H.~Chen, and W.~Wang.
\newblock Robust graph representation learning via neural sparsification.
\newblock In \emph{ICML}, 2020.

\bibitem[Zhong et~al.(2020)Zhong, Zheng, Kang, Li, and Yang]{zhong2020random}
Z.~Zhong, L.~Zheng, G.~Kang, S.~Li, and Y.~Yang.
\newblock Random erasing data augmentation.
\newblock In \emph{AAAI}, 2020.

\bibitem[Zhou et~al.(2020)Zhou, Shen, and Xuan]{zhou2020data}
J.~Zhou, J.~Shen, and Q.~Xuan.
\newblock Data augmentation for graph classification.
\newblock In \emph{CIKM}, 2020.

\bibitem[Zhu et~al.(2022)Zhu, Shen, Zhang, Pang, and Wei]{zhu2022data}
S.~Zhu, Q.~Shen, Y.~Zhang, Y.~Pang, and Z.~Wei.
\newblock Data-augmented counterfactual learning for bundle recommendation.
\newblock \emph{arXiv:2210.10555}, 2022.

\bibitem[Zhu(2005)]{zhu2005semi}
X.~Zhu.
\newblock \emph{Semi-supervised learning with graphs}.
\newblock Carnegie Mellon University, 2005.

\bibitem[Zhu and Ghahramani(2002)]{zhu2002learning}
X.~Zhu and Z.~Ghahramani.
\newblock Learning from labeled and unlabeled data with label propagation.
\newblock 2002.

\bibitem[Zhu et~al.(2020)Zhu, Xu, Yu, Liu, Wu, and Wang]{zhu2020deep}
Y.~Zhu, Y.~Xu, F.~Yu, Q.~Liu, S.~Wu, and L.~Wang.
\newblock Deep graph contrastive representation learning.
\newblock \emph{arXiv:2006.04131}, 2020.

\bibitem[Zhu et~al.(2021{\natexlab{a}})Zhu, Xu, Zhang, Du, Zhang, Liu, Yang,
  and Wu]{zhu2021survey}
Y.~Zhu, W.~Xu, J.~Zhang, Y.~Du, J.~Zhang, Q.~Liu, C.~Yang, and S.~Wu.
\newblock A survey on graph structure learning: Progress and opportunities.
\newblock \emph{arXiv}, 2021{\natexlab{a}}.

\bibitem[Zhu et~al.(2021{\natexlab{b}})Zhu, Xu, Yu, Liu, Wu, and
  Wang]{zhu2021graph}
Y.~Zhu, Y.~Xu, F.~Yu, Q.~Liu, S.~Wu, and L.~Wang.
\newblock Graph contrastive learning with adaptive augmentation.
\newblock In \emph{TheWebConf}, 2021{\natexlab{b}}.

\bibitem[Z{\"u}gner and
  G{\"u}nnemann(2019{\natexlab{a}})]{zugner2019adversarial}
D.~Z{\"u}gner and S.~G{\"u}nnemann.
\newblock Adversarial attacks on graph neural networks via meta learning.
\newblock \emph{arXiv preprint arXiv:1902.08412}, 2019{\natexlab{a}}.

\bibitem[Z{\"u}gner and
  G{\"u}nnemann(2019{\natexlab{b}})]{zugner2019certifiable}
D.~Z{\"u}gner and S.~G{\"u}nnemann.
\newblock Certifiable robustness and robust training for graph convolutional
  networks.
\newblock In \emph{KDD}, 2019{\natexlab{b}}.

\bibitem[Z{\"u}gner and G{\"u}nnemann(2020)]{zugner2020certifiable}
D.~Z{\"u}gner and S.~G{\"u}nnemann.
\newblock Certifiable robustness of graph convolutional networks under
  structure perturbations.
\newblock In \emph{KDD}, 2020.

\bibitem[Z{\"u}gner et~al.(2018)Z{\"u}gner, Akbarnejad, and
  G{\"u}nnemann]{zugner2018adversarial}
D.~Z{\"u}gner, A.~Akbarnejad, and S.~G{\"u}nnemann.
\newblock Adversarial attacks on neural networks for graph data.
\newblock In \emph{KDD}, 2018.

\end{small}
\end{thebibliography}

\end{document}