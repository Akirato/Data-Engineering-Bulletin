%\newpage
\section{Applications}
\label{sec:sourav_:application}
We describe the explainers methods that are relevant for specific applications in different domains such as in social networks, biology, and computer security.% as well as the methods that are designed for temporal or dynamic graphs.
%\subsection{Applications}



\paragraph{Computer Security.} This work \cite{he2022illuminati} focuses on designing an explanation framework for cybersecurity applications using GNN models by identifying the important nodes, edges, and attributes that are contributing to the prediction. The applications include code vulnerability detection and smart contract vulnerability detection. Another work \cite{herath2022cfgexplainer} proposes CFGExplainer for GNN oriented malware classification and identifies a subgraph of the malware control flow graph that is most important for the classification. Some other work focuses on the problem of botnet detection. The first method BD-GNNExplainer \cite{zhu2022interpretability} extracts the explainer subgraph by reducing the loss between the classification results generated by the input subgraph and the entire input graph. The XG-BoT detector proposed in \cite{lo2023xg} detects malicious botnet nodes in botnet communication graphs. The explainer is based on the GNNExplainer and saliency map in the XG-BoT. %The 
%vulnerability detection in software has also been studied with explainable GNN method \cite{}.



\paragraph{Social Networks.}A recent work \cite{rath2021scarlet} studies the problem of detecting fake news spreaders in social networks. The proposed method SCARLET is a user-centric model that uses a GNN with attention mechanism. The attention scores help in computing the importance of the neighbors. The findings include that a person’s decision to spread false information is dependent on its perception (or trust dynamics) of neighbor’s credibility. On the other hand, \textbf{GCAN} \cite{GCAN} uses sequence models. The aim is to find a fake tweet based on the user profile and the sequence of its retweets. The sequence models and GNNs help to learn representation of retweet propagation and representation of user interactions respectively. A co-attention mechanism is further used to learn the correlation between source tweet and retweet propagation and make prediction. In \cite{ma2021understanding}, a GNN model has been proposed along with the explanation of its prediction for the problem on drug abuse in social networks.




\paragraph{Computational Biology.} 
One of the long standing problems in neuroscience is the understanding of Brain networks, especially understanding the Regions of Interests (ROIs) and the connectivity between them. These regions and their connectivity can be modelled as a graph. A recent work on explainability, IBGNN \cite{cui2022interpretable} explores the explainable GNN methods to solve the task of identifying ROIs and their connectivity that are indicative of brain disorders. It uses a perturbation matrix to create an edge mask, and extracts important edges and nodes. A few more works also focus on the same task of identifying ROIs, but use different explanation techniques. In \cite{mauri2022accurate}, the method uses a perturbation matrix with feature masks and optimizes mutual information to find the explanations. This work \cite{zhou2022interpretable} uses Grad-CAM \cite{Excitation-BP} to find important ROIs. \cite{abrate2021counterfactual} uses a search-based method to extract counterfactuals, which can serve as good candidates for important ROIs. The method uses graph edit operations to navigate from input graph to a counterfactual, but it optimizes this by using a lookup database to select edges that are the most effective in discriminating between different predicted classes. As another interesting application, this work \cite{pfeifer2022gnn} explores is related to the extraction of subgraphs in protein-protein interaction (PPI) network, where the downstream task is to detect the relevance of a protein to cancer. 
%In this a slightly modified version of GNNExplainer \cite{ying2019gnnexplainer} is used to extract important subgraphs as explanations.


%One of the long standing problems in neuroscience is the understanding of Brain networks, especially understanding the Regions of Interests (ROIs) and the connectivity between them. This can be modelled as a graph. A recent work on explainability, IBGNN \cite{cui2022interpretable} explores the use of explainable GNN techniques to model the task of identifying ROIs and their connectivity that are indicative of Brain disorders. 



%It uses a perturbation matrix to create an edge mask, and extracts important edges and nodes. A few more works also focusses on the same task of identifying ROIs, but uses different explanation techniques. This work \cite{mauri2022accurate} uses perturbation matrix with feature mask and optimizes mutual information to find the explanations. This work \cite{zhou2022interpretable} uses Grad-CAM \cite{Excitation-BP} to find important ROIs. Another interesting application this work \cite{pfeifer2022gnn} explores is related to the extraction of subgraphs in protein-protein interaction(PPI) network, where the downstream task is to detect a protein's relevance to cancer. In this a slightly modified version of GNNExplainer \cite{ying2019gnnexplainer} is used to extract important subgraphs as explanations.

\paragraph{Chemistry.} GNNs are being used to study molecular properties extensively and often requires explanations to better understand the model's predictions. A recent work \cite{henderson2021improving} focuses on improving self-interpretability of GCNs by imposing orthogonality of node features and sparsity of the GCN's weights using Gini regularization. The intuition behind the orthogonality of features is driven by the assumption that atoms in a molecule can be represented by a linear combination of orthonormal basis of wavefunctions. Another method, APRILE \cite{xu2021aprile} aims at finding the parts of a drug molecule responsible for side effects. It uses perturbation techniques to extract an explanation. In drug design, the method in \cite{jimenez2021coloring} uses integrated gradients \cite{sundararajan2017axiomatic} to assign importance to atoms (nodes) and the atomic properties (node features) to understand the properties of a drug.



\paragraph{Pathology.} In medical diagnosis a challenging task is to understand the reason behind a particular diagnosis, whether it is made by a human or a machine learning system. To this end, explainer frameworks for the machine learning models become useful. In many cases the diagnosis data can be represented by graphs. This work \cite{wu2021counterfactual} builds a graph using words, entities, clauses and sentences extracted from a patient's electronic medical record (EMR). The objective is to extract the entities most relevant for the diagnosis by training an edge mask, and is achieved by minimizing the sum of the elements in the mask matrix. Another method~\cite{jaume2021quantifying} focuses on generating explanations for histology (micro-anatomy) images. It first converts the image into a graph of biological entities, where the nodes could be cells, tissues or some other task specific biological features. Afterwards the standard explainer techniques described in gradient \ref{sec:sourav_:gradient-based} or perturbation \ref{sec:sourav_:perturbation} based methods are used to generate the explanations. Another work \cite{yu2021towards} in this field modifies the objective to optimise for both necessity and sufficiency (Sec. \ref{sec:sourav_:eval}). The explanation is generated in such a way that the mutual information between explanation subgraph and the prediction is maximized. Additionally, the mutual information between the remaining graph after removing the explanation subgraph and the prediction is minimized. %Here the mutual information computation is slightly modified from the standard version.




% \paragraph{Others.} Other applications include predictions in physical systems. The work \textbf{NodeSel} \cite{bonet2022explaining}, for instance, focuses on generating explanations for GNN models used in air quality prediction of various places connected in a graph like manner. It uses interpretable HSIC Lasso model to extract the most influential neighbourhood of a node that is relevant for the prediction. In the field of robotics one could model the actions and movements of a robot as a graph and use a GNN to model this task. This work \cite{} does the same and uses an explainer to analyse the decisions made by the GNN. The explainer method involves the usage of an edge and a feature mask to generate the subgraph. It uses mutual information to optimize this subgraph.







