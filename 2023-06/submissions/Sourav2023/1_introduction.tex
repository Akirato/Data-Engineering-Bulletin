\section{Introduction}
Recent years have seen a tremendous rise in the use of Graph Neural Networks (GNNs) for real-world applications, ranging from healthcare~\cite{zitnik2018modeling,zitnik2019machine}, drug design~\cite{xiong2021graph,drug-design, drug-ligand}, recommender systems~\cite{chen2022grease}, and fraud detection~\cite{fraud-detect}. Predictions made in these domains have a substantial impact and therefore require to be highly trustworthy. In the realm of deep learning, one effective approach to enhance trust in these predictions is to provide an explanation supporting them~\cite{trust-AI}. These explanations elucidate the model's predictions for human understanding and can be generated through various methods. For instance, they may involve identifying important substructures within the input data~\cite{pgexplainer, Graph-mask, subgraphX}, providing additional examples from the training data~\cite{SE-GNN}, or constructing counterfactual examples by perturbing the input to produce a different prediction outcome~\cite{cfgnnex, cf^2-counter, chen2022grease}.



The interpretability of deep learning models is influenced by the characteristics of the input domain as the content and the complexity of explanations can vary depending on the inputs. When it comes to explaining predictions made by graph neural networks (GNNs), several challenges arise. First, since graphs are combinatorial data structures, finding important substructures by evaluating different combinations that maximize a certain prediction becomes difficult. Second, attributed graphs contain both node attributes and edge connectivity, which can influence the predictions and they should be considered together in explanations. Third, explanations must be adaptable to different existing GNN architectures. Lastly, explanations for the local tasks (e.g., node or edge level) may differ from those for global tasks (e.g., graph level). Due to these challenges, explaining graph neural networks is non-trivial and a large variety of methods have been proposed in the literature to tackle it~\cite{ying2019gnnexplainer, GSAT, D_invariant_rationale, Excitation-BP, guided-bp, pgm-ex, pgexplainer, xgnn, cf^2-counter, robust-counter, meg-counter}. With the increasing use of GNNs in critical applications such as healthcare and recommender systems, and the consequent rise in their explainability methods, we provide an updated survey of the explainability of GNNs. Additionally, we propose a novel taxonomy that categorizes the explainability methods for GNNs, providing a comprehensive overview of the field.


% write-up on comparison with existing surveys
Existing surveys on GNN explainability predominantly focus on either factual methods \cite{First-survey,survey2-prfe,survey5-li} or counterfactual methods \cite{survey-counter} but not both. 
% Factual methods aim to identify features that are sufficient to make the same prediction as on the input graph, while counterfactual methods seek minimal changes in input data to change the prediction. 
These surveys thus lack a comprehensive overview of the different methods in the literature by limiting the discussion to specific methods~\cite{First-survey,survey5-li} or only discussing them under the broad umbrella of trustworthy GNNs~\cite{survey2-prfe, survey3-gl}.
% In addition to limiting the scope of their discussion to a specific set of methods, they lack a comprehensive overview of the many explainability methods in the literature.
% only to post-hoc explainers and exclude discussion on other methods while others  only discuss explainability the broad umbrella of trustworthy GNNs. Thus, we believe that existing surveys  
Our survey aims to bridge this gap by providing a comprehensive and detailed summary of existing explainability methods for GNNs. We include both factual as well as counterfactual methods of explainability in GNNs. To enhance clarity and organization, we introduce a novel taxonomy to categorize these methods for a more systematic understanding of their nuances and characteristics. 