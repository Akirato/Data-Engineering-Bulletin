\section{Future Directions}
\label{sec:sourav_future_work}

\textbf{Combinatorial problems: } Most of the existing explanation frameworks are for prediction tasks such as node and graph classification. However, graphs are prevalent in various domains, such as in social networks~\cite{kempe2003maximizing,medya2020approximate}, healthcare~\cite{wilder2018optimizing}, and infrastructure development~\cite{medya2018noticeable,medya2016towards}. Solving combinatorial optimization problems on graphs is a common requirement in these domains. Several architectures based on Graph Neural Networks (GNNs) \cite{khalil2017learning,manchanda2020gcomb,ranjan2022greed} have been proposed to tackle these problems that are usually computationally hard. However, the explainability of these methods for such combinatorial problems is largely missing. One potential direction is to build frameworks that can explain the behavior of the solution set in such problems. \\
\noindent 
\textbf{Global methods: } 
Most explainers primarily adopt a local perspective by generating examples specific to individual input graphs. From global explanations, we can extract higher-level insights that complement the understanding gained from local explanations (see details on global methods in Sec. \ref{sec:sourav_:global}). Moreover, global explanations can be easily understood by humans even for large datasets.
Real-world graph datasets often consist of millions of nodes. When generating explanations specific to each instance, the number of explanations increases proportionally with the size of the dataset. As a result, the sheer volume of explanations becomes overwhelming for human cognitive capabilities to process effectively. Global approaches can immensely help in these scenarios. \\
\noindent
% \textbf{Scalability: } Most of the graph benchmarks consider a small size of graphs 
\textbf{Visualization and HCI tools: } Graph data, unlike textual and visual data, cannot be perceived by human senses. Thus, qualitative evaluation of explanation becomes a non-trivial problem and often requires expert guidance~\cite{ying2019gnnexplainer,pgm-ex}. This makes crowdsourcing evaluations difficult and not scalable. Other ways to qualitatively assess graph structures for explanation of a certain prediction can be explored. Additionally, since explainability is human-centric, it is crucial that explainers are influenced by human cognition and behavior, particularly those of domain experts~\cite{liao2021human} while using GNNs in making important decisions \cite{medya2022exploratory}.
 HCI research can help in designing the interface for the experts to assess the generated explanation graphs~\cite{cfgnnex}. \\
\textbf{Temporal GNNs: } Temporal graph models are designed to predict the graph structure and labels in the future by exploiting how the graph has evolved in the past. This increases the complexity of explanations significantly as they now involve combinations of graph structures at different time intervals. Existing methods~\cite{fan2021gcn,xie2022explaining,he2022explainer,kosan2021event} mostly focus on discrete-time models where graphs are provided at different points in time. Future works can explore ways to explain the prediction of a continuous-time dynamic graph model, where interactions happen in real time~\cite{xiaexplaining}. One direction could be to optimize over a parameterized temporal point process~\cite{trivedi2019dyrep}. 