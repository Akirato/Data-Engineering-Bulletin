%\section{Background}

\subsection{Graph Neural Networks}
Graph neural networks (GNNs) have been used to learn powerful representations of graphs~\cite{kipf2016semi,velivckovic2017graph,hamilton2017inductive}. Consider a graph $\CG$ given by $\CG = (\CV, \CE)$, where $\CV$ denotes the set of $n$ nodes and $\CE$ denotes the set of $m$ edges. We can create an adjacency matrix $\Abf \in {[0, 1]}^{n \times n}$ such that $A_{ij} = 1$ if $(i,j) \in \CE$ and $0$ otherwise. Each node may have attributes, given by the matrix $\Xbf \in \mathbb{R}^{n \times F}$ such that each row $i$ stores the $F$-dimensional attribute vector for node $i$. A GNN model $\CM$ embeds each node $v \in \CV$ into a low-dimensional space $\Zbf: \mathbb{R}^{n \times d}$ by following this message passing rule for $k$ steps as

\begin{equation}
    \Zbf_v^{(k+1)} = \textsc{Update}_{\Phi}(\Zbf_v^{(k)}, \textsc{Agg}(\{\textsc{Msg}_{\Theta}(\Zbf_v^{(k)}, \Zbf_u^{(k)}): (u, v) \in \CE\})),
\end{equation}

such that $\Zbf^{0} = \Xbf$ and $\Zbf := \Zbf^{(k)}$. Different instances of the update $\textsc{Update}_{\Phi}$, aggregation $\textsc{Agg}_{\Phi}$ and message generator $\textsc{Msg}_{\Theta}$ functions give rise to different GNN architectures. For example, GCN~\cite{kipf2016semi} has an identity message, a mean aggregation, and weighted update functions, while GAT~\cite{velivckovic2017graph} learns an attention-based message generation instead. These embeddings are trained for a specific task $\CT$ that can be either supervised (e.g., node classification, graph classification, etc.) or unsupervised (e.g., self-supervised link prediction, clustering, etc.). 

\subsection{Explainability in ML}
\begin{prob}[Explainability~\cite{burkart2021survey}]
    Consider a supervised task $\CT$ with the aim of learning a mapping from $\CX$ to $\CY$, and a model $\CM$ trained for this task. Given a set of $(\xbf, y)$ pairs $\subseteq (\CX, \CY)$ and the model $\CM$, generate an explanation $\ebf$ from a given set $\CD_{E}$ such that $\ebf$ ``explains'' the prediction $\hat{y} = \CM(\xbf)$. 
\end{prob}

These explanations can be either \textit{local} to a single test input $(\xbf, y)$ or \textit{global} when they explain prediction over a specific dataset $\CD' \subseteq (\CX, \CY)$. Further, the explanation can be generated either \textit{post-hoc} (\ie, after the model training) or \textit{ante-hoc} where the model itself is \textit{self-interpretable}, \ie, it explains its predictions. With some exceptions, post-hoc explanations usually consider a black-box access to the model while self-interpretable methods update the model architecture and/or training itself. We can further differentiate the explanation methods based on their content, \ie, the explanation set $\CD_E$. \textit{Local explanations} only consider the local neighborhood of the given data instance while \textit{global explanations} are concerned about the model's overall behavior and thus, searches for patterns in the model's predictions. On the other hand, explanations can also be \textit{counterfactual}, where the aim is to explain a prediction by providing a contrasting example that changes it. 