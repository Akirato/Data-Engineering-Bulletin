\section{Others}

\label{sec:sourav_:Others}
In this section we describe the explainer methods in three special categories: \textbf{explainer for temporal GNNs, global explainers} and \textbf{causality-based explainers} in this section.


\subsection{Explainers for Temporal GNNs}

\iffalse
\begin{table}[htbp]
  \centering
  \scriptsize
  \caption{Temporal....}
    \begin{tabular}{ccccc}
    \hline
          Method & Approach & GNN model & Task     & Application \\
    \hline
GCN-SE \cite{fan2021gcn} & & GCN &  Node Classification &  \\
 
    \hline
    \end{tabular}%
  \label{tab::dataset}%
\end{table}%
\fi


In temporal or dynamic graphs, the graph topology and node attributes evolve over time. For instance, in social networks the relationships can be dynamic, or in the citation networks co-authorships change over time. There has been effort towards explaining the GNN models that are specifically designed for such structures. 

One of the earlier explainer methods on dynamic graphs is GCN-SE \cite{fan2021gcn}. GCN-SE learns the attention weights in the form of linear combination of the representations of the nodes over multiple snapshots (i.e., over time). To quantify the explanatory power of the proposed method, importance of different snapshots are evaluated via these learned weights. Another method \cite{he2022explainer} designs a two-step process. It uses static explainer such as PGM-explaine~\cite{pgm-ex} to explain the underlying temporal GNN (TGNN) model (such as TGCN~\cite{zhao2019tgcn}) for each time step separately and then it aims to discover the dominant explanations from the explanations identified by the static one. DGExplainer \cite{xie2022explaining} also generates
explanations for dynamic GNNs by computing the
relevance scores that capture the contributions of each component for a dynamic graph. More specifically, it redistributes the output activation score to the relevance of the neurons of its previous layer in the model. This process iterates until the relevance scores of the input neuron are obtained. Recently, T-GNNExplainer~\cite{xiaexplaining} has been proposed for temporal graph explanation where a temporal graph constituted by a sequence of temporal events. T-GNNExplainer solves the problem of finding a small set of previous events that are responsible for the model's prediction of the target event. In \cite{liudifferential}, the approach involves a smooth parameterization of the GNN
predicted distributions using axiomatic attribution. These distributions are assumed to be on
a low-dimensional manifold. The approach models the distributional evolution as
smooth curves on the manifold and reparameterize families of curves by designing a convex optimization problem. The aim is to find a unique curve that
approximates the distributional evolution and will be useful for human interpretation.



The following ones also design explainers of temporal GNNs but with specific objectives or applications. \cite{vu2022limit} studies the limit of perturbation-based explanation methods. The approach constructs some specific instances of TGNNs and evaluate how reliably node-perturbation, edge-perturbation or both can reliably identify specific graph components carrying out the temporal aggregation in temporal GNNs.
In \cite{yang2023interpretable}, a novel interpretable model on temporal heterogeneous graphs has been proposed. The method constructs
temporal heterogeneous graphs which represent the research interests of the target authors. After the detection task, a deep neural network has been used for the generation process of interpretation on the predicted results. This method has been applied to research interest shift detection of researchers. 
Another related work \cite{han2022interpretable} is on explaining GraphRC which is a special type of GNN and popular because of its training efficiency.  
The proposed method explores the specific role played by each reservoir node (neuron) of GraphRC by using attention mechanism on the distinct temporal patterns in the reservoir nodes.
%For evaluation, to quantify the explanatory power of the proposed method, metrics to evaluate the correlation
%between the importance of different timesteps and the
%importance indicated by the learned attention weights,
%thus quantifying the explanatory power of a given classification model;




\subsection{Global Explainers}
\label{sec:sourav_:global}
% Just writing the Global methods, we can fit them later or make a different section
% White box ; Post-hoc
%Most explainers provide explanation for specific instance and can be termed as local explainers. However, global explainers try to explain general model behaviour by finding input patterns that explain certain predictions like predictions for a specific class \cite{xgnn}. Global explainers provide high-level and generic explanation compared to local explainers. However, local explainers are more accurate compared to global explainers \cite{xgnn}. We categorize global explainers into three different types:
Majority of the explainers provide explanation for specific instances and can be seen as \textit{local explainers}. However, global explainers aim to explain the overall behavior of the model by finding common input patterns that explain certain predictions \cite{xgnn}. Global explainers provide a high-level and generic explanation compared to local explainers. However, local explainers can be more accurate compared to global explainers \cite{xgnn} especially for individual instances. We categorize global explainers into following three types.\\
\noindent
\textbf{(1) Generation-based:} These post-hoc methods use either a generator or generative modeling to find explanations. For instance, \textbf{XGNN} \cite{xgnn} uses a reinforcement learning (RL) based graph generator optimized using policy gradient. In contrast, \textbf{GNNInterpreter} \cite{gnninterpreter} is a generative global explainer that maximizes the likelihood of explanation graph being predicted as the target class by the model (the details are in Sec. \ref{sec:sourav_:generation-based}).\\
%\textbf{Generation-based:} These are post-facto methods that use either a generator or generative modeling to find explanations. There are two generation-based approaches XGNN \cite{xgnn} and GNNInterpreter \cite{gnninterpreter}. XGNN \cite{xgnn} uses a Reinforcement learning (RL) based graph generator optimized using policy gradient. On the other hand, GNNInterpreter \cite{gnninterpreter} is a generative global explainer that maximizes the likelihood of explanation graph being predicted as target  class by the model. We have already discussed these in detail in Sec. \ref{sec:sourav_:generation-based}.\\
\noindent 
%\textbf{Concept-based}: These methods provide concept based explanations. Concepts are small higher level units of information that can be interpreted by humans \cite{automatic_concept}. The methods differ in the approaches to find concepts. One method \textbf{GCExplainer} \cite{global_concept_ex} adapts an image explanation framework known as automated concept based explanation (ACE) \cite{automatic_concept} to find global explanation for graphs. Concepts are small units in the input that can be understood by humans. GCExplainer finds Concepts by clustering the embeddings from the last layer. Each cluster represents a concept. The importance of a concept is decided by the number of nodes it represents. Another method \textbf{GCneuron } \cite{Global_neuron} is inspired by Compositional Explanations of Neurons \cite{Neuron-compositional}, this method finds global explanation for GNNs by finding compositional concepts aligned with neurons. A base concept is a function \(C: G \longrightarrow \{0,1 \} ^ {|V|} \) on Graph \(G\) that produces a binary mask over all the input nodes \(V\). A compositional concept is logical combination of base concepts. This method uses beam search to find compositional concept that minimizes the divergence between the concept and the neuron activation. Lastly, \textbf{GLGExplainer} \cite{global_logic} uses local explanations from PGExplainer \cite{pgexplainer} and projects them to a set of learned prototype or concepts (similar to ProtGNN \cite{protgnn}) to derive a concept vector. A concept vector is vector of distance between graph explanation and each prototype. This concept vector is then used to train an Entropy based logic explainable network (E-LEN) \cite{logic-LEN} to match the prediction of the class. The logic formula from entropy layer for each class acts as explanations.\\
\textbf{(2) Concept-based}: These methods provide concept based explanations. Concepts are small higher level units of information that can be interpreted by humans \cite{automatic_concept}. The methods differ in the approaches to find concepts. \textbf{GCExplainer} \cite{global_concept_ex} adapts an image explanation framework known as automated concept based explanation (ACE) \cite{automatic_concept} to find global explanation for graphs. It finds concepts by clustering the embeddings from the last layer of the GNN. A concept and its importance are represented by a cluster and the number of nodes in it respectively. Another method \textbf{GCneuron } \cite{Global_neuron}, which is inspired by Compositional Explanations of Neurons \cite{Neuron-compositional}, finds global explanation for GNNs by finding compositional concepts aligned with neurons. A base concept is a function \(C\) on Graph \(G\) that produces a binary mask over all the input nodes \(V\). A compositional concept is logical combination of base concepts. This method uses beam search to find compositional concept that minimizes the divergence between the concept and the neuron activation. Lastly, \textbf{GLGExplainer} \cite{global_logic} uses local explanations from PGExplainer \cite{pgexplainer} and projects them to a set of learned prototype or concepts (similar to ProtGNN \cite{protgnn}) to derive a concept vector. A concept vector is vector of distances between graph explanation and each prototype. This concept vector is then used to train an Entropy based logic explainable network (E-LEN) \cite{logic-LEN} to match the prediction of the class. The logic formula from the entropy layer for each class acts as explanations.\\
\noindent
\textbf{(3) Counterfactual}: \textbf{Global counterfactual explainer} \cite{Global-counter} finds a candidate set of counterfactuals using vertex re-inforced random walk. It then uses a greedy strategy to select the top $k$ counterfactuals from the candidate set as global explanations. We explain it in more detail in Sec. \ref{sec:sourav_:Counterfactual-search}. %To the best of our knowledge, there is only one Global counterfactual explainer \cite{Global-counter} and we explain it in more detail in section \ref{sec:sourav_:Counterfactual}.

\subsection{Causality-based Explainers}
Most of the GNN classifiers learn all statistical correlation between the label and input features. As a result, these may not distinguish between causal and non-causal features and may make prediction using shortcut features \cite{causal-attention}. Shortcut features serve as confounders between the causal features and the prediction. Methods in this category attempt to reduce the confounding effect so that the model exploits causal substructure for prediction and these substructures also act as explanations. They can be categorized into the followings.

\textbf{Self-interpretable methods.} Methods in this category have the explainer architecture inbuilt into the model. 
One of the methods, \textbf{DIR} \cite{D_invariant_rationale} creates multiple interventional distribution by conducting intervention on the training distribution. The invariant part across these distributions is considered as the causal part (see details in Sec. \ref{subsec:sourav_: SE- structural-constr}). \textbf{CAL} \cite{causal-attention} uses edge and node attention to estimate causal and shortcut features of the graph. Two classifiers are used to make prediction on causal and shortcut features respectively. Loss on causal features is used as classification loss. Moreover, KL divergence is used to push the prediction based on shortcut features to have uniform distribution across classes. Finally, CAL creates an intervention graph in the representation space via random additions. The loss on this intervened graph classification is considered as the causal loss. These three loss terms are used to reduce the confounding effect and find the causal substructure that acts as explanation. \textbf{DisC} \cite{causal-debiasing} uses a disentangled GNN framework to separate causal and shortcut substructures. It first learns a edge mask generator that divides the input into causal and shortcut substructures. Two separate GNNs are trained to produce disentangled representation of these substructures. Finally, these representations are used to generate unbiased counterfactual samples by randomly permuting the shortcut representation with the causal representation. %With this the causal and shortcut representations are both disentangled and uncorrelated.

\textbf{Generation-based methods.} These methods use generative modeling to find explanations and are post-hoc. \textbf{GEM~\cite{Gen-causal}} trains a generative auto-encoder by finding the causal contribution of each edge in the computation graph (details are in Sec. \ref{sec:sourav_:generation-based}). While GEM focuses on the graph space, \textbf{OrphicX} \cite{causal-orphicx} identifies the causal factors in the embedding space. It trains a variational graph autoencoder (VGAE) that has an encoder and a generator. The encoder outputs latent representations of causal and shortcut substructures of input. Generator uses both of these representations to generate the original graph and the causal representation to produce a causal mask on original graph. The information flow between the latent representation of the causal substructure and the prediction is maximized to train the explainer. The causal substructure also acts as an explanation.

