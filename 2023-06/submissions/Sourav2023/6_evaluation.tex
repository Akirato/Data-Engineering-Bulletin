
\section{Evaluation}
\label{sec::eval}
The evaluation of the explainer methods is based on the quality of the explainer's ability to generate human-intelligible explanations about the model prediction. As this might be subjective depending on the applications in hand, the evaluation measures consider both quantitative and qualitative metrics.
%The evaluation of the explainer methods is based on explainability or interpretability which measures the quality of the explainer's ability to generate human-intelligible explanations about the model prediction. As this might be subjective depending on the applications in hand, the evaluation measures consider both quantitative and qualitative metrics.


\subsection{Quantitative Evaluation}
Quantitative evaluation metrics help in having a standardized evaluation that is free of human bias. 
% Some of the commonly used quantitative metrics are described here. 
For this, explainability is posed as a binary classification problem. The explainers assign a score to the \textit{node features}, \textit{edges}, and \textit{motifs}, which are the most responsible for the prediction according to the explainer. We are also provided with the ground-truth binary labels for the features and structures based, denoting whether they are responsible for the prediction or not. The explainer is then evaluated by comparing these scores to the ground-truth explanation labels using different methods: 
% Area Under Curve(AUC) can serve as a good measure of the accuracy. 
% A few methods use input instances to generate subgraphs which are used as input to another classifier to predict the label for the original instance. Here the explainer can be evaluated based on the accuracy of this classifier. This method of evaluation is used specifically for the graph classification task.

%Since qualitative evaluation is subject to human bias, in order to have a standardized evaluation, quantitative evaluation methods have been proposed in the literature. Some of the commonly used quantitative metrics are described here.

%For quantitative evaluation, interpretability is posed as a binary classification problem. In this setup, generally, the explainers assign a score to \textit{node features} or \textit{edges} or \textit{motifs}, which are the most responsible for the prediction, according to the explainer. The ground-truth explanations are just True/False labels assigned to \textit{node features} or \textit{edges} or \textit{motifs} that are actually the most responsible for the prediction. The explainer is then evaluated by comparing these scores to the ground-truth explanations. 

%In cases where the number of edges or the size of motifs are considered as a tunable parameters, Area Under Curve(AUC) can serve as a good accuracy measure.

%Some papers use input instances to generate sub-graph which are used as input to another classifier to predict the label for the original instance from which this sub-graph was generated. In this case the explainer can be evaluated based on the accuracy of this classifier. This method of evaluation is used generally for graph classification tasks.

% \begin{itemize}
 
% =========================================================
% ========================Accuracy=========================
    %\item \textbf{Accuracy \cite{cfgnnex, cf^2-counter}:} In this metric, generally, top \textit{k} edges, as scored by the explainer, are chosen and these edges are consider positive, and the rest negative. In ground truth explanations, we already have positive and negative labeled edges according to their relevance to the target label. These top-k edges and labels are used to compare accuracy.

% \item 
\noindent\textbf{Accuracy \cite{cfgnnex, cf^2-counter}:} To find the accuracy, the top-$k$ edges produced by the explainer are set to be positive, and the rest are negative. These top-k edges and the ground-truth labels are compared to compute the accuracy.
% =========================================================
% ========================AUC=========================
% \item 

\noindent\textbf{Area Under Curve (AUC) \cite{RELex, agnostic-counter, robust-counter}:} We compare the top-$k$ raw scores directly against the ground-truth labels by computing the area under the ROC curve. %Here the score is used to rank the explainers, to sort the predictions.
%\item \textbf{Area Under Curve (AUC) \cite{robust-counter}:} In the same set up as above, a better method than accuracy will be to simply compute accuracy by choosing a "arbitrary" \textit{k} is to plot ROC curve and compute AUC. % Here the score is used to rank the explainers, to sort the predictions.    
% =========================================================
% ========================Fidelity=========================

\noindent\textbf{Fidelity \cite{VGIB, cfgnnex, robust-counter}:} This is used for explainers that generate a subgraph as the explanation. It compares the performances of the base GNN model on the input graph and the explainer subgraph. Let $N$ be the number of samples, $y_i$ is the true label of sample $i$, $\hat{y}_i$ is the predicted label of sample $i$, $\hat{y}^k$ is the predicted label after choosing the subgraph formed by nodes with top-$k$\% nodes, and $\mathbbm{1}[\cdot]$ is the indicator function. Fidelity measures how close the predictions of the explanation sub-graph are to the input graph. For factual explainers, the lower this value, the better is the explanation. It is formally defined as follows:
    \begin{equation*}
        \text{Fidelity} = \frac{1}{N} \sum_{i=1}^{N} {\mathbbm{1}}[y_i = \hat{y}_i] - \mathbbm{1}[y_i = \hat{y}_i^k]
    \end{equation*}    
    %For this metric the explainer is evaluated by comparing the performance of the base GNN model when using the full graph to the performance when using the explanation sub-graph.
    %Let $N$ be the number of samples, $y_i$ is the true label of sample $i$, $\hat{y}_i$ is the predicted label of sample $i$,  $\hat{y}^k$ is the predicted label when the input is limited to the explanation sub-graph.
    %And $\mathbbm{1}(y_i = \hat{y}_i)$ is the indicator function. The following equation for fidelity is used in the paper  \cite{VGIB}
    %\begin{equation}
        %{Fidelity} = \frac{1}{N} \sum_{i=1}^{N} \left[{\mathbbm{1}}\left(y_i = \hat{y}_i\right) - \mathbbm{1}\left(y_i = \hat{y}_i^k\right)\right]
    %\end{equation}
    %Fidelity measures how close the predictions of explanation sub-graph are to the input graph. The lower this value, the better is the explanation.
    % Reference - https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_Improving_Subgraph_Recognition_With_Variational_Graph_Information_Bottleneck_CVPR_2022_paper.pdf
% =========================================================
% ========================Sparsity=========================

\noindent\textbf{Sparsity  \cite{VGIB, cfgnnex}:} It measures the conciseness of explanations (e.g., the sub-graphs) that are responsible for the final prediction. Let $|p_i|$ and $|\CG_i|$ denote the number of edges in the explanation, and the same in the original input graph, respectively. The sparsity is then defined as follows:
    \begin{equation*}
        \text{Sparsity} = 1 - \frac{1}{N} \sum_{i=1}^{N} \frac{|p_i|}{|\CG_i|}
    \end{equation*}
    % Example, identifying only those motifs/sub-structures responsible for a prediction. It has an inverse relation to fidelity
    % Eq-10, 11 in https://openreview.net/pdf?id=0YXmOFLb1wQ
% =========================================================
% =======================Infidelity=========================
%\noindent\textbf{Infidelity \cite{yeh2019fidelity}:} It quantifies how much of the base model's response to a perturbation is captured by the explanation. 
%Given a base model $f$, explainer $\Phi$, input graph's adjacency matrix $\CA_G$, a random variable $\mathbf{I} \in \mathbbm{R}^d$ with probability measure $\mu_I$, which represents meaningful perturbations of interest, the infidelity of $\Phi$ is defined as:
%\begin{equation*}
%\text{Infidelity} (\Phi, f, \CA_G) = \mathbbm{E}_{I\sim\mu_I}\left[(\mathbf{I}^T\Phi(f, \CA_G) - {(f(\CA_G) - f(\CA_G - \mathbf{I}))})^2\right]
%\end{equation*}
%Intuitively, $\mathbf{I}^T\phi(f, \CA_G)$ can be understood as the weighted sum of the perturbation vector ($\mathbf{I}$) components, where weights given by $\Phi(f, \CA_G)$ represent the importance of the vector components (features), and $f(\CA_G) - f(\CA_G - \mathbf{I})$ is the base model's response to the perturbation. 
% (https://arxiv.org/pdf/2006.00305.pdf, https://arxiv.org/pdf/1901.09392.pdf)
% =========================================================

% =====================Robustness==========================
\noindent\textbf{Robustness \cite{robust-counter}:} It quantifies how resistant an explainer is to perturbations on input graph. Here perturbations are addition or deletion of edges randomly such that it does not change the prediction of the underlying GNN. The robustness is the percentage of graphs for which these perturbations do not change the explanation.
% =========================================================

% ===============Sufficiency & Necessity====================
\noindent \textbf{Probability of Sufficiency (PS) \cite{cf^2-counter, chen2022grease}:} It is the percentage of graphs for which the explanation graph is sufficient to generate the same prediction as the original input graph.

\noindent \textbf{Probability of Necessity (PN) \cite{cf^2-counter, chen2022grease}:} It is the percentage of graphs for which the explanation graph when removed from the original input graph will alter the prediction made by the GNN.
% =========================================================

% =====================Generalization=======================

\noindent \textbf{Generalization \cite{RL-enhanced}:} This measures the capability of generalization of the explainer method in an inductive setting. To measure this, the training dataset size is usually varied and the AUC scores are computed for these tests. Generalisation plays an important role in explanability as the good generalizable models are generally sparse in terms of inputs. This metric is highly relevant for the self-interpretable models.
    % Get this statement reviewed
    % https://openreview.net/pdf?id=nUtLCcV24hL
    % Train the explainer on limited number of samples to see, which generalizes better in this inductive setting
% =========================================================

% =====================Efficiency==========================
    % \item  \textbf{Efficiency \cite{RC-ex, chen2022grease}:}

    
% =====================Others==========================
%\noindent \textbf{Others:} Other measures to evaluate explanations include \textbf{base model effect}, \textbf{validity}, \textbf{computational efficiency \cite{robust-counter, chen2022grease}}, \textbf{fairness}. \textit{Base model effect} quantifies how the explanations change when the underlying GNN is changed. \textit{Validity} measures what percentage of the generated explanations/counterfactuals are valid graphs under domain constraints, as in case of molecules all possible sub-graphs are not valid molecules. \textit{Fairness} measures if the explainer is sensitive to protected features. Less sensitiveness implies better fairness, and vice-versa.

\subsection{Qualitative Evaluation}
Explanations can also be evaluated qualitatively using expert domain knowledge. This mode of evaluation is crucial especially while working with real-world datasets that do not have ground truth labels.
% Include an example of molecules from a paper?
% Include some example graphs for qualitative measure from some paper on popular datasets to highlight this point
% \begin{itemize}
% =========================================================
% ===================Interpretability=======================

\noindent \textbf{Domain Knowledge \cite{ying2019gnnexplainer}:} Generated explanations can be evaluated for their meaning in the application domain. For example, GNNExplainer~\cite{ying2019gnnexplainer} correctly identifies the carbon ring as well as chemical groups NH2 and NO2, which are known to be mutagenic. 
% In addition to capturing this relationship, the explanation should be easy to understand, i.e., the explanation is desired to be compact. Compactness can be measured by the size of the explanation (e.g., a subgraph). 
    % or the class “mutagenic”, we can observe that carbon circles and NO2 are some common patterns, and this is consistent with the chemical fact that carbon rings and NO2 chemical groups are mutagenic
    % \item \textbf{Completeness:} Not including as, this would be a redundant measure
    % Some explanations cover only partial motifs, others are more comphrehensive
    % https://arxiv.org/pdf/2102.05152.pdf
% =========================================================
% =====================Manual Scoring=======================

\noindent \textbf{Manual Scoring \cite{pgm-ex}:} Another method of evaluating the explanations is by asking users (e.g., domain experts) to score, say on a scale of 1-10, the explanations generated by various explainers and compare them. One can also use RMSE scores to quantitatively compare these explainers based on the scores.
    % https://arxiv.org/pdf/2010.05788.pdf en end-users were asked to give a score on a scale of 0 to 10 for each explanation based on the their thoughts on the importance of the highlighted nodes to the GNN prediction.

% \end{itemize}
