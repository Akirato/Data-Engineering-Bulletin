
\newpage
\section{Method}
\subsection{Conditional Calibration}
\subsubsection{In-distribution Detection}
Given an arbitrary query $q=(h,r)$ and a candidate set $\gT_q = \{t_1^q, \dots, t_n^q\}$, we want to have a better sense of how each tail in $\gT_q$ really is an answer to the query.

\begin{enumerate}[noitemsep,topsep=0pt,parsep=1pt,partopsep=0pt, leftmargin=*]
    \item Calculate the distance $d(q,t)$ between $q$ and each $t \in \gT_q$
    \item Fit a normal distribution over $d(q,t_1^q), \dots, d(q,t_n^q)$
    \item Quantify the uncertainty by 1/2/3 standard deviations
\end{enumerate}

This may be limited because of the small size of the $\gT_q$, the idea is to find nearest neighbors of query $q$, (in this case will be finding nearest neighbor of $h$: $\{h_1,\dots, h_k\}$), each query $q_i=(h_i, r), i\in[1,k]$ will have a different candidate set $\gT_{q_i}$. 

Denote $q_0 = q$, the second step will be aggregating all the distance $d(q_i, t_j^{q_i}),\: i\in[0, k],\: j\in[1, |\gT_{q_i}|]$


\begin{enumerate}[noitemsep,topsep=0pt,parsep=1pt,partopsep=0pt, leftmargin=*]
    \item Calculate the distance $d(q_i,t_j^{q_i})$ between {\color{red} $q_i$ and each $t_j^{q_i} \in \gT_{q_i}$}
    \item Fit a normal distribution over $d(q_i, t_j^{q_i}),\: i\in[0, k],\: j\in[1, |\gT_{q_i}|]$
    \item Quantify the uncertainty by 1/2/3 standard deviations
\end{enumerate}


\subsubsection{Outlier Detection}
Given an arbitrary query $q=(h,r)$ and a candidate set $\gT_q = \{t_1^q, \dots, t_n^q\}$, we want to have a better sense of how each tail in $\gT_q$ really is an answer to the query.

\begin{enumerate}[noitemsep,topsep=0pt,parsep=1pt,partopsep=0pt, leftmargin=*]
    \item {\color{blue} Random sample negatives of same type $\gN_q=\{n_1^q, \dots, n_m^q\}$}
    \item {\color{blue} Calculate the distance $d(q,n)$ between $q$ and each $n \in \gN_q$}
    \item Calculate the distance $d(q,t)$ between $q$ and each $t \in \gT_q$
    \item Fit a normal distribution over {\color{red} $d(q,n_1^q), \dots, d(q,n_m^q)$}
    \item Quantify the uncertainty by 1/2/3 standard deviations
\end{enumerate}

\newpage
\begin{itemize}
    \item Add tf-idf
    \item Test random examples
    \item Sample negatives by finding close entities of the answers.
\end{itemize}

\begin{itemize}
    \item Guided fact curation
    \item Fact importance, assume everything is correct, use the top rankings to guided the user experience. It requires stability, comparison is local.
    \item Guided fact verification, it requires calibration across queries. Comparison is global.
    \item How much will we cost comparison between weird question and random question.
    \item (1) how surprise is it, (2) add the importance of the entity (pagerank stuff) for calculation of the total utility.
\end{itemize}

Baselines and our methods for proposal
\begin{itemize}
    \item First baseline: a unified threshold across all queries
    \item Second baseline: random proposal
    \item Ours: adaptive threshold, consider the popularity of the entity, tf-idf, size of the answer set, sampling from the probability
\end{itemize}

\begin{itemize}
    \item Pick $k$ entities above 0.99. (k~1,000-10,000)
    \item $k\times\alpha$ binary examples, marked as correct or wrong.
    \item List of answers on wikidata + additional answers (KG ontology, nearest neighbor using embeddings).
    \item Assumption: Wrong answers are super small, $W$ out of $k\times \alpha$.
    \item Proxy between wikidata KG and the actual fact (verified by human curators).
    \item To get the wrong answers using small number of queries
    \item One na\"ive way: go over all $k\times \alpha$ queries. Expectation: easily run out of Budget
    \item Baseline Policy two: uniformly random within budget $B$ queries. Expectation: cannot find the wrong facts.
    \item Ours: weighted sampling. 
    
\end{itemize}