\subsection{Challenges in Machine Learning}
%说一下机器学习发展，引用一下之前的文章
%说一下机器学习的问题，因此需要联邦学习

Machine learning has achieved great success and gradually entered people's daily lives~\cite{Jindong-sarker2021machine, Jindong-paluru2021anam, Jindong-wang2019deep}.
It has been applied to many fields, e.g. human activity recognition~\cite{Jindong-lu2021cross}, face recognition~\cite{Jindong-liu2022sphereface}, and healthcare~\cite{Jindong-chen2022metafed}.
Successful machine learning applications, especially deep learning based applications, often require a large amount of data and lots of computational resources.
In most cases, a deluge of data and computing resources can lead to easy success, such as ChatGPT~\cite{Jindong-van2023chatgpt}.
However, data and computation also mean money and resources.
In reality, it is impossible to aggregate all data together in some situations.
There seems to be a contradiction between the massive resource requirements of traditional methods and the limited real environment. %\wjd{This part is unnecessary and should be removed. You should add a section to introduce OOD related work}

Generalization is another challenging problem caused by data distribution shifts.
Its goal is to learn a generalized model with limited data and it expects that the learned model can work well on unseen targets with unknown distributions.
% Generalization is more difficult.
% For more details on generalization, please refer to the survey~\cite{Jindong-wang2022generalizing}.
\cite{Jindong-wang2022generalizing} gives a survey on domain generalization and first groups existing methods into three categories, including data manipulation~\cite{Jindong-lu2022semantic}, representation learning~\cite{Jindong-lu2022local}, and learning strategy~\cite{Jindong-huang2020self}.

\subsection{Federated Learning}
Data is often scatted everywhere and cannot be aggregated together due to some factors, such as laws and regulations~\cite{Jindong-voigt2017eu} and the awakening of people's awareness of data security and privacy protection.
In such an environment, federated learning came into being~\cite{Jindong-yang2019federated,Jindong-roy2019braintorrent}.
According to \cite{Jindong-yang2019federated}, federated learning can be grouped into three categories, including horizontal federated learning, vertical federated learning, and federated transfer learning.
Most deep learning based methods belong to horizontal federated learning and so is this paper.
For a more detailed introduction, please refer to the survey~\cite{Jindong-liu2022distributed}.

FedAVG is a traditional horizontal federated learning method~\cite{Jindong-mcmahan2017communication}.
Although it is simple, it was applied in many applications.
When meeting data distribution heterogeneity, FedAVG appeared powerless~\cite{Jindong-sattler2019robust}.
And many researchers proposed various methods to solve the above problems.
FedProx~\cite{Jindong-li2018federated} added a proximal regularized term to FedAVG and it allowed slight model gaps between clients and the server.
In FedBN~\cite{Jindong-li2021fedbn}, the authors thought that parameters in batch normalization layers can represent data distribution, and keeping specific batch normalization layers for each client could make local models personalized.
Another latest method, FedAP~\cite{Jindong-lu2022personalized}, learned the similarity between clients based on the statistics of the batch normalization layers while preserving the specificity of each client with different local batch normalization.
The above methods all achieve satisfactory results in their corresponding scenarios.
However, most of them focused on personalization and ignored generalization issues~\cite{Jindong-chenbridging}.

Generalization in federated learning is a novel problem.
In recent two years, some papers tried to solve this problem.
\cite{Jindong-Honglinyuan2022} first discussed the generalization in federated learning and it proposed a framework to disentangle performance gaps, including out-of-sample gaps and participation gaps.
FED-DRO~\cite{Jindong-chenbridging} proposed a novel federated learning framework to explicitly decouple a model's dual duties with two prediction tasks and it mainly focused on label shifts.
Some other work tried to adapt existing domain generalization methods to generalization in federated learning~\cite{Jindong-gupta2022fl,Jindong-tenison2022gradient,Jindong-qu2022generalized,Jindong-caldarola2022improving}.
FL Games~\cite{Jindong-gupta2022fl} utilized Nash equilibrium to learn causal features that were invariant across clients which is similar to Invariant Risk Minimization (IRM)~\cite{Jindong-arjovsky2020invariant}.
FedSAM~\cite{Jindong-qu2022generalized} proposed a general effective algorithm based on Sharpness Aware Minimization (SAM) local optimizer~\cite{Jindong-foretsharpness}.
Although these methods can bring generalization, they were not designed for large models and could not make full use of knowledge brought by pretrained models.

%说一下联邦学习发展，三种
%fedavg、fedprox、fedbn
%说说个性化和泛化
%大模型上表现不好

\subsection{CLIP and Large Models}
From perceptron~\cite{Jindong-gardner1998artificial} to AlexNet~\cite{Jindong-alom2018history} to ResNet~\cite{Jindong-he2016deep} to Vision transformer~\cite{Jindong-yuan2021tokens} to CLIP~\cite{Jindong-radford2021learning}, pretrained models have become larger and larger.
The importance of pretrained models has been increasing and pretrained models contain a growing amount of knowledge.
For specific applications, researchers usually choose suitable backbone models and then adopt some techniques, e.g. finetune~\cite{Jindong-sun2019fine}, to slightly adapt pretrained models.
Since pretrained models are trained via a large amount of data, features extracted from them are often generalized and insightful.
Few works pay attention to large models in federated learning and high demands of computational costs and communication costs hinder the development of this field.
In this paper, we focus on CLIP in federated learning.

CLIP~\cite{Jindong-radford2021learning} learned SOTA image representations from scratch on a dataset of 400 million(image, text) pairs collected from the internet.
The natural language was used to reference learned visual concepts.
It has been applied in many fields and demonstrated its superiority~\cite{Jindong-ramesh2021zero, Jindong-lee2023image}.
However, in federated learning, CLIP is still in its infancy.
PromptFL~\cite{Jindong-guo2022promptfl} replaced the federated model training with the federated prompt training to simultaneously achieve efficient global aggregation and local training by exploiting the power of foundation models in a distributed way.
However, it still requires certain computational costs and it is not designed for data distribution heterogeneity problems.
Moreover, it is hard to tune the hyperparameters for the prompt techniques in Transformer.% are often hard to tune the .
In this paper, we focus on fast personalization and generalization for CLIP.
%从感知机到alexnet到resnet到clip，预训练模型越来越不可或缺，包含的知识越来越多。
%这些知识往往是泛化的，特征提取的。
%说说现有的大模型
%重点介绍CLIP