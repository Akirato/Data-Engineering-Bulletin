@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM}
}




@unpublished{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  note = {Manuscript},
}

@article{yuan2020federated,
  title={Federated Accelerated Stochastic Gradient Descent},
  author={Yuan, Honglin and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{reisizadeh2020fedpaq,
  title={Fedpaq: A communication-efficient federated learning method with periodic averaging and quantization},
  author={Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Jadbabaie, Ali and Pedarsani, Ramtin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2021--2031},
  year={2020},
  organization={PMLR}
}
@inproceedings{haddadpour2021federated,
  title={Federated learning with compression: Unified analysis and sharp guarantees},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2350--2358},
  year={2021},
  organization={PMLR}
}
@inproceedings{haddadpour2019local,
  title={Local sgd with periodic averaging: Tighter analysis and adaptive synchronization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  booktitle={Advances in Neural Information Processing Systems},
  pages={11082--11094},
  year={2019}
}
@article{deng2020distributionally,
  title={Distributionally Robust Federated Averaging},
  author={Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{deng2020adaptive,
  title={Adaptive Personalized Federated Learning},
  author={Deng, Yuyang and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad},
  journal={arXiv preprint arXiv:2003.13461},
  year={2020}
}
@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}
@inproceedings{karimireddy2020scaffold,
  title={Scaffold: Stochastic controlled averaging for federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
  booktitle={International Conference on Machine Learning},
  pages={5132--5143},
  year={2020},
  organization={PMLR}
}
@inproceedings{khaled2020tighter,
  title={Tighter theory for local SGD on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}
@article{stich2018local,
  title={Local SGD converges fast and communicates little},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1805.09767},
  year={2018}
}
@article{wang2018cooperative,
  title={Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms},
  author={Wang, Jianyu and Joshi, Gauri},
  journal={arXiv preprint arXiv:1808.07576},
  year={2018}
}
@inproceedings{woodworth2020local,
  title={Is local SGD better than minibatch SGD?},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Stich, Sebastian and Dai, Zhen and Bullins, Brian and Mcmahan, Brendan and Shamir, Ohad and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={10334--10343},
  year={2020},
  organization={PMLR}
}
@inproceedings{yu2019parallel,
  title={Parallel restarted sgd with faster convergence and less communication: Demystifying why model averaging works for deep learning},
  author={Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  pages={5693--5700},
  year={2019}
}
@article{dieuleveut2019communication,
  title={Communication trade-offs for Local-SGD with large step size},
  author={Dieuleveut, Aymeric and Patel, Kumar Kshitij},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={13601--13612},
  year={2019}
}
@article{zhou2017convergence,
  title={On the convergence properties of a $ K $-step averaging stochastic gradient descent algorithm for nonconvex optimization},
  author={Zhou, Fan and Cong, Guojing},
  journal={arXiv preprint arXiv:1708.01012},
  year={2017}
}
@article{li2019convergence,
  title={On the convergence of fedavg on non-iid data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1907.02189},
  year={2019}
}
@article{haddadpour2019convergence,
  title={On the convergence of local descent methods in federated learning},
  author={Haddadpour, Farzin and Mahdavi, Mehrdad},
  journal={arXiv preprint arXiv:1910.14425},
  year={2019}
}
@inproceedings{haddadpour2019trading,
  title={Trading redundancy for communication: Speeding up distributed sgd for non-convex optimization},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  booktitle={International Conference on Machine Learning},
  pages={2545--2554},
  year={2019},
  organization={PMLR}
}
@article{stich2019error,
  title={The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication},
  author={Stich, Sebastian U and Karimireddy, Sai Praneeth},
  journal={arXiv preprint arXiv:1909.05350},
  year={2019}
}
@article{li2018federated,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={arXiv preprint arXiv:1812.06127},
  year={2018}
}
@article{kairouz2019advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Kallista and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
  journal={arXiv preprint arXiv:1912.04977},
  year={2019}
}
@article{wang2021field,
  title={A field guide to federated optimization},
  author={Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H Brendan and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and others},
  journal={arXiv preprint arXiv:2107.06917},
  year={2021}
}
@article{alistarh2017qsgd,
  title={QSGD: Communication-efficient SGD via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  pages={1709--1720},
  year={2017}
}
@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}
@article{konevcny2016federated,
  title={Federated learning: Strategies for improving communication efficiency},
  author={Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
  journal={arXiv preprint arXiv:1610.05492},
  year={2016}
}
@inproceedings{rothchild2020fetchsgd,
  title={Fetchsgd: Communication-efficient federated learning with sketching},
  author={Rothchild, Daniel and Panda, Ashwinee and Ullah, Enayat and Ivkin, Nikita and Stoica, Ion and Braverman, Vladimir and Gonzalez, Joseph and Arora, Raman},
  booktitle={International Conference on Machine Learning},
  pages={8253--8265},
  year={2020},
  organization={PMLR}
}
@inproceedings{suresh2017distributed,
  title={Distributed mean estimation with limited communication},
  author={Suresh, Ananda Theertha and Felix, X Yu and Kumar, Sanjiv and McMahan, H Brendan},
  booktitle={International Conference on Machine Learning},
  pages={3329--3337},
  year={2017},
  organization={PMLR}
}
@article{vogels2019powersgd,
  title={PowerSGD: Practical low-rank gradient compression for distributed optimization},
  author={Vogels, Thijs and Karinireddy, Sai Praneeth and Jaggi, Martin},
  journal={Advances In Neural Information Processing Systems 32 (Nips 2019)},
  volume={32},
  number={CONF},
  year={2019},
  publisher={NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)}
}
@article{wang2018atomo,
  title={Atomo: Communication-efficient learning via atomic sparsification},
  author={Wang, Hongyi and Sievert, Scott and Charles, Zachary and Liu, Shengchao and Wright, Stephen and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:1806.04090},
  year={2018}
}
@article{wangni2017gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  journal={arXiv preprint arXiv:1710.09854},
  year={2017}
}
@article{horvath2019natural,
  title={Natural compression for distributed deep learning},
  author={Horvath, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu, Atal Narayan and Canini, Marco and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:1905.10988},
  year={2019}
}
@article{basu2019qsparse,
  title={Qsparse-local-SGD: Distributed SGD with quantization, sparsification, and local computations},
  author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
  journal={arXiv preprint arXiv:1906.02367},
  year={2019}
}
@inproceedings{yu2019linear,
  title={On the linear speedup analysis of communication efficient momentum SGD for distributed non-convex optimization},
  author={Yu, Hao and Jin, Rong and Yang, Sen},
  booktitle={International Conference on Machine Learning},
  pages={7184--7193},
  year={2019},
  organization={PMLR}
}
@article{wang2021local,
  title={Local Adaptivity in Federated Learning: Convergence and Consistency},
  author={Wang, Jianyu and Xu, Zheng and Garrett, Zachary and Charles, Zachary and Liu, Luyang and Joshi, Gauri},
  journal={arXiv preprint arXiv:2106.02305},
  year={2021}
}
@article{karimireddy2020mime,
  title={Mime: Mimicking centralized stochastic algorithms in federated learning},
  author={Karimireddy, Sai Praneeth and Jaggi, Martin and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={arXiv preprint arXiv:2008.03606},
  year={2020}
}
@article{singh2021squarm,
  title={Squarm-sgd: Communication-efficient momentum sgd for decentralized optimization},
  author={Singh, Navjot and Data, Deepesh and George, Jemin and Diggavi, Suhas},
  journal={IEEE Journal on Selected Areas in Information Theory},
  year={2021},
  publisher={IEEE}
}
@article{li2020acceleration,
  title={Acceleration for compressed gradient descent in distributed and federated optimization},
  author={Li, Zhize and Kovalev, Dmitry and Qian, Xun and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.11364},
  year={2020}
}
@article{li2021canita,
  title={CANITA: Faster rates for distributed convex optimization with communication compression},
  author={Li, Zhize and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2107.09461},
  year={2021}
}
@article{bansal2019potential,
  title={Potential-function proofs for gradient methods},
  author={Bansal, Nikhil and Gupta, Anupam},
  journal={Theory of Computing},
  volume={15},
  number={1},
  pages={1--32},
  year={2019},
  publisher={Theory of Computing Exchange}
}
@article{li2020federated,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020},
  publisher={IEEE}
}
@article{lin2018don,
  title={Don't use large mini-batches, use local SGD},
  author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
  journal={arXiv preprint arXiv:1808.07217},
  year={2018}
}
@article{lecun1998mnist,
  title={The MNIST database of handwritten digits},
  author={LeCun, Yann},
  journal={http://yann. lecun. com/exdb/mnist/},
  year={1998}
}
@article{dua2017uci,
  title={UCI machine learning repository},
  author={Dua, Dheeru and Graff, Casey},
  journal={http://archive.ics.uci.edu/ml},
  year={2017}
}
@article{wang2022communication,
  title={Communication-Efficient Adaptive Federated Learning},
  author={Wang, Yujia and Lin, Lu and Chen, Jinghui},
  journal={arXiv preprint arXiv:2205.02719},
  year={2022}
}
@article{li2022distributed,
  title={On Distributed Adaptive Optimization with Gradient Compression},
  author={Li, Xiaoyun and Karimi, Belhal and Li, Ping},
  journal={arXiv preprint arXiv:2205.05632},
  year={2022}
}



















@book{Abiteboul1995Foundationsdatabases,
  title = {Foundations of Databases},
  author = {Abiteboul, S. and Hull, Richard and Vianu, Victor},
  year = {1995},
  publisher = {{Addison-Wesley}},
  address = {{Reading, Mass}},
  isbn = {978-0-201-53771-0},
  langid = {english},
  lccn = {QA76.9.D3 A26 1995},
  keywords = {Database management},
  annotation = {00000}
}

@inproceedings{Alivanistos2022QueryEmbedding,
  title = {Query {{Embedding}} on {{Hyper-Relational Knowledge Graphs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Alivanistos, Dimitrios and Berrendorf, Max and Cochez, Michael and Galkin, Mikhail},
  year = {2022},
  month = mar,
  abstract = {Multi-hop logical reasoning is an established problem in the field of representation learning on knowledge graphs (KGs). It subsumes both one-hop link prediction as well as other more complex types of logical queries. Existing algorithms operate only on classical, triple-based graphs, whereas modern KGs often employ a hyper-relational modeling paradigm. In this paradigm, typed edges may have several key-value pairs known as qualifiers that provide fine-grained context for facts. In queries, this context modifies the meaning of relations, and usually reduces the answer set. Hyper-relational queries are often observed in real-world KG applications, and existing approaches for approximate query answering cannot make use of qualifier pairs. In this work, we bridge this gap and extend the multi-hop reasoning problem to hyper-relational KGs allowing to tackle this new type of complex queries. Building upon recent advancements in Graph Neural Networks and query embedding techniques, we study how to embed and answer hyper-relational conjunctive queries. Besides that, we propose a method to answer such queries and demonstrate in our experiments that qualifiers improve query answering on a diverse set of query patterns.},
  langid = {english}
}

@inproceedings{Amayuelas2022NeuralMethods,
  ids = {amayuelasNeuralMethodsLogical2022},
  title = {Neural {{Methods}} for {{Logical Reasoning}} over {{Knowledge Graphs}}},
  booktitle = {The {{Tenth International Conference}} on {{Learning Representations}}, {{ICLR}} 2022, {{Virtual Event}}, {{April}} 25-29, 2022},
  author = {Amayuelas, Alfonso and Zhang, Shuai and Rao, Susie Xi and Zhang, Ce},
  year = {2022},
  publisher = {{OpenReview.net}}
}

@inproceedings{Arakelyan2021ComplexQuery,
  title = {Complex {{Query Answering}} with {{Neural Link Predictors}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Arakelyan, Erik and Daza, Daniel and Minervini, Pasquale and Cochez, Michael},
  year = {2021},
  abstract = {Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions (\$\textbackslash land\$), disjunctions (\$\textbackslash lor\$) and existential quantifiers (\$\textbackslash exists\$), while accounting for missing edges. In this work, we propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods --- black-box neural models trained on millions of generated queries --- without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8\% up to 40\% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online, at https://github.com/uclnlp/cqd.},
  langid = {english}
}

@inproceedings{Auer2007DBpediaNucleus,
  title = {{{DBpedia}}: {{A Nucleus}} for a {{Web}} of {{Open Data}}},
  shorttitle = {{{DBpedia}}},
  booktitle = {The {{Semantic Web}}},
  author = {Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  editor = {Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and {Cudr{\'e}-Mauroux}, Philippe},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {722--735},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-76298-0_52},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  isbn = {978-3-540-76298-0},
  langid = {english},
  keywords = {Open Dataset,Relational Database Table,Sophisticated Query,SPARQL Endpoint,Triple Pattern}
}

@inproceedings{Bai2022Query2ParticlesKnowledge,
  ids = {baiQuery2ParticlesKnowledgeGraph2022a},
  title = {{{Query2Particles}}: {{Knowledge Graph Reasoning}} with {{Particle Embeddings}}},
  shorttitle = {{{Query2Particles}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2022},
  author = {Bai, Jiaxin and Wang, Zihao and Zhang, Hongming and Song, Yangqiu},
  year = {2022},
  month = jul,
  pages = {2703--2714},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.findings-naacl.207},
  abstract = {Answering complex logical queries on incomplete knowledge graphs (KGs) with missing edges is a fundamental and important task for knowledge graph reasoning. The query embedding method is proposed to answer these queries by jointly encoding queries and entities to the same embedding space. Then the answer entities are selected according to the similarities between the entity embeddings and the query embedding. As the answers to a complex query are obtained from a combination of logical operations over sub-queries, the embeddings of the answer entities may not always follow a uni-modal distribution in the embedding space. Thus, it is challenging to simultaneously retrieve a set of diverse answers from the embedding space using a single and concentrated query representation such as a vector or a hyper-rectangle. To better cope with queries with diversified answers, we propose Query2Particles (Q2P), a complex KG query answering method. Q2P encodes each query into multiple vectors, named particle embeddings. By doing so, the candidate answers can be retrieved from different areas over the embedding space using the maximal similarities between the entity embeddings and any of the particle embeddings. Meanwhile, the corresponding neural logic operations are defined to support its reasoning over arbitrary first-order logic queries. The experiments show that Query2Particles achieves state-of-the-art performance on the complex query answering tasks on FB15k, FB15K-237, and NELL knowledge graphs.}
}

@inproceedings{Bollacker2008Freebasecollaboratively,
  title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
  shorttitle = {Freebase},
  booktitle = {Proceedings of the {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}, {{SIGMOD}} 2008, {{Vancouver}}, {{BC}}, {{Canada}}, {{June}} 10-12, 2008},
  author = {Bollacker, Kurt D. and Evans, Colin and Paritosh, Praveen K. and Sturge, Tim and Taylor, Jamie},
  editor = {Wang, Jason Tsong-Li},
  year = {2008},
  pages = {1247--1250},
  publisher = {{ACM}},
  doi = {10.1145/1376616.1376746}
}

@inproceedings{Bordes2013TranslatingEmbeddings,
  title = {Translating {{Embeddings}} for {{Modeling Multi-relational Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bordes, Antoine and Usunier, Nicolas and {Garcia-Duran}, Alberto and Weston, Jason and Yakhnenko, Oksana},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  annotation = {04160}
}

@article{Bosselut2021DynamicNeuroSymbolic,
  title = {Dynamic {{Neuro-Symbolic Knowledge Graph Construction}} for {{Zero-shot Commonsense Question Answering}}},
  author = {Bosselut, Antoine and Le Bras, Ronan and Choi, Yejin},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {6},
  pages = {4923--4931},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i6.16625},
  abstract = {Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it.},
  langid = {english}
}

@inproceedings{Carlson2010ArchitectureNeverEnding,
  title = {Toward an {{Architecture}} for {{Never-Ending Language Learning}}},
  booktitle = {Proceedings of the {{Twenty-Fourth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2010, {{Atlanta}}, {{Georgia}}, {{USA}}, {{July}} 11-15, 2010},
  author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Jr, Estevam R. Hruschka and Mitchell, Tom M.},
  editor = {Fox, Maria and Poole, David},
  year = {2010},
  publisher = {{AAAI Press}}
}

@inproceedings{Chen2022FuzzyLogic,
  title = {Fuzzy {{Logic Based Logical Query Answering}} on {{Knowledge Graphs}}},
  booktitle = {Thirty-{{Sixth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2022, {{Thirty-Fourth Conference}} on {{Innovative Applications}} of {{Artificial Intelligence}}, {{IAAI}} 2022, {{The Twelveth Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}, {{EAAI}} 2022 {{Virtual Event}}, {{February}} 22 - {{March}} 1, 2022},
  author = {Chen, Xuelu and Hu, Ziniu and Sun, Yizhou},
  year = {2022},
  pages = {3939--3948},
  publisher = {{AAAI Press}}
}

@inproceedings{Choudhary2021ProbabilisticEntity,
  ids = {choudharyProbabilisticEntityRepresentation2021a},
  title = {Probabilistic {{Entity Representation Model}} for {{Reasoning}} over {{Knowledge Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Choudhary, Nurendra and Rao, Nikhil and Katariya, Sumeet and Subbian, Karthik and Reddy, Chandan},
  year = {2021},
  volume = {34},
  pages = {23440--23451},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique that can provide an efficient querying mechanism over large and incomplete databases. Current approaches employ spatial geometries such as boxes to learn query representations that encompass the answer entities and model the logical operations of projection and intersection. However, their geometry is restrictive and leads to non-smooth strict boundaries, which further results in ambiguous answer entities. Furthermore, previous works propose transformation tricks to handle unions which results in non-closure and, thus, cannot be chained in a stream. In this paper, we propose a Probabilistic Entity Representation Model (PERM) to encode entities as a Multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. Additionally, we also define the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. On the logical query reasoning problem, we demonstrate that the proposed PERM significantly outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics. We also evaluate PERM's competence on a COVID-19 drug-repurposing case study and show that our proposed work is able to recommend drugs with substantially better F1 than current methods. Finally, we demonstrate the working of our PERM's query answering process through a low-dimensional visualization of the Gaussian representations.}
}

@inproceedings{Choudhary2021SelfSupervisedHyperboloidc,
  title = {Self-{{Supervised Hyperboloid Representations}} from {{Logical Queries}} over {{Knowledge Graphs}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Choudhary, Nurendra and Rao, Nikhil and Katariya, Sumeet and Subbian, Karthik and Reddy, Chandan K.},
  year = {2021},
  month = jun,
  series = {{{WWW}} '21},
  pages = {1373--1384},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3442381.3449974},
  abstract = {Knowledge Graphs (KGs) are ubiquitous structures for information storage in several real-world applications such as web search, e-commerce, social networks, and biology. Querying KGs remains a foundational and challenging problem due to their size and complexity. Promising approaches to tackle this problem include embedding the KG units (e.g., entities and relations) in a Euclidean space such that the query embedding contains the information relevant to its results. These approaches, however, fail to capture the hierarchical nature and semantic information of the entities present in the graph. Additionally, most of these approaches only utilize multi-hop queries (that can be modeled by simple translation operations) to learn embeddings and ignore more complex operations such as intersection, and union of simpler queries. To tackle such complex operations, in this paper, we formulate KG representation learning as a self-supervised logical query reasoning problem that utilizes translation, intersection and union queries over KGs. We propose Hyperboloid Embeddings (HypE), a novel self-supervised dynamic reasoning framework, that utilizes positive first-order existential queries on a KG to learn representations of its entities and relations as hyperboloids in a Poincar\'e ball. HypE models the positive first-order queries as geometrical translation, intersection, and union. For the problem of KG reasoning in real-world datasets, the proposed HypE model significantly outperforms the state-of-the art results. We also apply HypE to an anomaly detection task on a popular e-commerce website product taxonomy as well as hierarchically organized web articles and demonstrate significant performance improvements compared to existing baseline methods. Finally, we also visualize the learned HypE embeddings in a Poincar\'e ball to clearly interpret and comprehend the representation space.},
  isbn = {978-1-4503-8312-7},
  keywords = {hyperbolic space,knowledge graphs,reasoning queries,Representation learning}
}

@article{Cohen2020TensorLogProbabilistic,
  title = {{{TensorLog}}: {{A Probabilistic Database Implemented Using Deep-Learning Infrastructure}}},
  shorttitle = {{{TensorLog}}},
  author = {Cohen, William and Yang, Fan and Mazaitis, Kathryn Rivard},
  year = {2020},
  month = feb,
  journal = {Journal of Artificial Intelligence Research},
  volume = {67},
  pages = {285--325},
  issn = {1076-9757},
  doi = {10.1613/jair.1.11944},
  abstract = {We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.},
  copyright = {Copyright (c)},
  langid = {english}
}

@misc{Daza2020MessagePassing,
  title = {Message {{Passing Query Embedding}}},
  author = {Daza, Daniel and Cochez, Michael},
  year = {2020},
  month = jun,
  number = {arXiv:2002.02406},
  eprint = {2002.02406},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.02406},
  abstract = {Recent works on representation learning for Knowledge Graphs have moved beyond the problem of link prediction, to answering queries of an arbitrary structure. Existing methods are based on ad-hoc mechanisms that require training with a diverse set of query structures. We propose a more general architecture that employs a graph neural network to encode a graph representation of the query, where nodes correspond to entities and variables. The generality of our method allows it to encode a more diverse set of query types in comparison to previous work. Our method shows competitive performance against previous models for complex queries, and in contrast with these models, it can answer complex queries when trained for link prediction only. We show that the model learns entity embeddings that capture the notion of entity type without explicit supervision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{Galkin2020MessagePassing,
  title = {Message {{Passing}} for {{Hyper-Relational Knowledge Graphs}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Galkin, Mikhail and Trivedi, Priyansh and Maheshwari, Gaurav and Usbeck, Ricardo and Lehmann, Jens},
  year = {2020},
  month = nov,
  pages = {7346--7359},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.596},
  abstract = {Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.}
}

@inproceedings{Galkin2022InductiveLogical,
  title = {Inductive {{Logical Query Answering}} in {{Knowledge Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33: {{Annual Conference}} on {{Neural Information Processing Systems}} 2022 {{NeurIPS}} 2022, {{November}} 27- {{December}} 9, 2022, {{New Orleans}}},
  author = {Galkin, Mikhail and Zhu, Zhaocheng and Ren, Hongyu and Tang, Jian},
  year = {2022},
  pages = {25},
  abstract = {Formulating and answering logical queries is a standard communication interface for knowledge graphs (KGs). Alleviating the notorious incompleteness of realworld KGs, neural methods achieved impressive results in link prediction and complex query answering tasks by learning representations of entities, relations, and queries. Still, most existing query answering methods rely on transductive entity embeddings and cannot generalize to KGs containing new entities without retraining the entity embeddings. In this work, we study the inductive query answering task where inference is performed on a graph containing new entities with queries over both seen and unseen entities. To this end, we devise two mechanisms leveraging inductive node and relational structure representations powered by graph neural networks (GNNs). Experimentally, we show that inductive models are able to perform logical reasoning at inference time over unseen nodes generalizing to graphs up to 500\% larger than training ones. Exploring the efficiency\textendash effectiveness trade-off, we find the inductive relational structure representation method generally achieves higher performance, while the inductive node representation method is able to answer complex queries in the inference-only regime without any training on queries and scales to graphs of millions of nodes. Code is available at https://github.com/DeepGraphLearning/InductiveQE.},
  langid = {english}
}

@book{Hajek1998MetamathematicsFuzzy,
  title = {Metamathematics of {{Fuzzy Logic}}},
  author = {H{\'a}jek, Petr},
  editor = {W{\'o}jcicki, Ryszard and H{\'a}jek, Petr and Makinson, David and Mundici, Daniele and Segerberg, Krister and Urquhart, Alasdair and Malinowski, Jacek},
  year = {1998},
  series = {Trends in {{Logic}}},
  volume = {4},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-5300-3},
  isbn = {978-1-4020-0370-7 978-94-011-5300-3},
  langid = {english}
}

@inproceedings{Hamilton2018EmbeddingLogical,
  title = {Embedding {{Logical Queries}} on {{Knowledge Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31: {{Annual Conference}} on {{Neural Information Processing Systems}} 2018, {{NeurIPS}} 2018, {{December}} 3-8, 2018, {{Montr\'eal}}, {{Canada}}},
  author = {Hamilton, William L. and Bajaj, Payal and Zitnik, Marinka and Jurafsky, Dan and Leskovec, Jure},
  editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and {Cesa-Bianchi}, Nicol{\`o} and Garnett, Roman},
  year = {2018},
  pages = {2030--2041}
}

@inproceedings{Hu2020OpenGraph,
  title = {Open {{Graph Benchmark}}: {{Datasets}} for {{Machine Learning}} on {{Graphs}}},
  shorttitle = {Open {{Graph Benchmark}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  year = {2020},
  volume = {33},
  pages = {22118--22133},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .}
}

@inproceedings{Hu2022TypeawareEmbeddings,
  title = {Type-Aware {{Embeddings}} for {{Multi-Hop Reasoning}} over {{Knowledge Graphs}}},
  booktitle = {Proceedings of the {{Thirty-First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Hu, Zhiwei and Gutierrez Basulto, Victor and Xiang, Zhiliang and Li, Xiaoli and Li, Ru and Z. Pan, Jeff},
  year = {2022},
  month = jul,
  pages = {3078--3084},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Vienna, Austria}},
  doi = {10.24963/ijcai.2022/427},
  abstract = {Multi-hop reasoning over real-life knowledge graphs (KGs) is a highly challenging problem as traditional subgraph matching methods are not capable to deal with noise and missing information. To address this problem, it has been recently introduced a promising approach based on jointly embedding logical queries and KGs into a low-dimensional space to identify answer entities. However, existing proposals ignore critical semantic knowledge inherently available in KGs, such as type information. To leverage type information, we propose a novel TypE-aware Message Passing (TEMP) model, which enhances the entity and relation representations in queries, and simultaneously improves generalization, deductive and inductive reasoning. Remarkably, TEMP is a plug-and-play model that can be easily incorporated into existing embedding-based models to improve their performance. Extensive experiments on three real-world datasets demonstrate TEMP's effectiveness.},
  isbn = {978-1-956792-00-3},
  langid = {english}
}

@inproceedings{Huang2022LinELogical,
  ids = {huangLinELogicalQuery2022a,huangLinELogicalQuery2022b},
  title = {{{LinE}}: {{Logical Query Reasoning}} over {{Hierarchical Knowledge Graphs}}},
  shorttitle = {{{LinE}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Huang, Zijian and Chiang, Meng-Fen and Lee, Wang-Chien},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {615--625},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539338},
  abstract = {Logical reasoning over Knowledge Graphs (KGs) for first-order logic (FOL) queries performs the query inference over KGs with logical operators, including conjunction, disjunction, existential quantification and negation, to approximate true answers in embedding spaces. However, most existing work imposes strong distributional assumptions (e.g., Beta distribution) to represent entities and queries into presumed distributional shape, which limits their expressive power. Moreover, query embeddings are challenging due to the relational complexities in multi-relational KGs (e.g., symmetry, anti-symmetry and transitivity). To bridge the gap, we propose a logical query reasoning framework, Line Embedding (LinE), for FOL queries. To relax the distributional assumptions, we introduce the logic space transformation layer, which is a generic neural function that converts embeddings from probabilistic distribution space to LinE embeddings space. To tackle multi-relational and logical complexities, we formulate neural relation-specific projections and individual logical operators to truthfully ground LinE query embeddings on logical regularities and KG factoids. Lastly, to verify the LinE embedding quality, we generate a FOL query dataset from WordNet, which richly encompasses hierarchical relations. Extensive experiments show superior reasoning sensitivity of LinE on three benchmarks against strong baselines, particularly for multi-hop relational queries and negation-related queries.},
  isbn = {978-1-4503-9385-0},
  keywords = {knowledge representation learning,logical query reasoning}
}

@article{Ji2022Surveyknowledge,
  title = {A Survey on Knowledge Graphs: Representation, Acquisition, and Applications},
  shorttitle = {A Survey on Knowledge Graphs},
  author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
  year = {2022},
  month = feb,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {2},
  pages = {494--514},
  publisher = {{IEEE, Institute of Electrical and Electronics Engineers}},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2021.3070843},
  langid = {english},
  pmid = {33900922}
}

@inproceedings{Jia2021ComplexTemporal,
  title = {Complex {{Temporal Question Answering}} on {{Knowledge Graphs}}},
  booktitle = {{{CIKM}} '21: {{The}} 30th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}, {{Virtual Event}}, {{Queensland}}, {{Australia}}, {{November}} 1 - 5, 2021},
  author = {Jia, Zhen and Pramanik, Soumajit and Roy, Rishiraj Saha and Weikum, Gerhard},
  editor = {Demartini, Gianluca and Zuccon, Guido and Culpepper, J. Shane and Huang, Zi and Tong, Hanghang},
  year = {2021},
  pages = {792--802},
  publisher = {{ACM}},
  doi = {10.1145/3459637.3482416}
}

@article{Katsaras1977Fuzzyvector,
  title = {Fuzzy Vector Spaces and Fuzzy Topological Vector Spaces},
  author = {Katsaras, A. K and Liu, D. B},
  year = {1977},
  month = mar,
  journal = {Journal of Mathematical Analysis and Applications},
  volume = {58},
  number = {1},
  pages = {135--146},
  issn = {0022-247X},
  doi = {10.1016/0022-247X(77)90233-5},
  langid = {english}
}

@article{Kotnis2021AnsweringComplex,
  title = {Answering {{Complex Queries}} in {{Knowledge Graphs}} with {{Bidirectional Sequence Encoders}}},
  author = {Kotnis, Bhushan and Lawrence, Carolin and Niepert, Mathias},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {6},
  pages = {4968--4977},
  issn = {2374-3468},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Relational Learning}
}

@book{Kroenke2018Databaseprocessing,
  title = {Database Processing: Fundamentals, Design, and Implementation},
  shorttitle = {Database Processing},
  author = {Kroenke, David M. and Auer, David J. and Vandenberg, Scott L. and Yoder, Robert C.},
  year = {2018},
  edition = {15th edition, 40th anniversary edition},
  publisher = {{Pearson}},
  address = {{NY NY}},
  isbn = {978-0-13-480274-9},
  langid = {english},
  lccn = {QA76.9.D3 K7365 2018},
  keywords = {Database management}
}

@book{Libkin2004ElementsFinite,
  title = {Elements of {{Finite Model Theory}}},
  author = {Libkin, Leonid},
  year = {2004},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-07003-1},
  isbn = {978-3-642-05948-3 978-3-662-07003-1},
  langid = {english},
  annotation = {00000}
}

@inproceedings{Libkin2009OpenClosed,
  title = {Open and {{Closed World Assumptions}} in {{Data Exchange}}},
  booktitle = {Proceedings of the 22nd {{International Workshop}} on {{Description Logics}} ({{DL}} 2009), {{Oxford}}, {{UK}}, {{July}} 27-30, 2009},
  author = {Libkin, Leonid and Sirangelo, Cristina},
  editor = {Grau, Bernardo Cuenca and Horrocks, Ian and Motik, Boris and Sattler, Ulrike},
  year = {2009},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {477},
  publisher = {{CEUR-WS.org}}
}

@article{Lin2019KagNetKnowledgeAware,
  title = {{{KagNet}}: {{Knowledge-Aware Graph Networks}} for {{Commonsense Reasoning}}},
  shorttitle = {{{KagNet}}},
  author = {Lin, Bill Yuchen and Chen, Xinyue and Chen, Jamin and Ren, Xiang},
  year = {2019},
  journal = {EMNLP/IJCNLP},
  doi = {10.18653/v1/D19-1282},
  abstract = {Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.}
}

@inproceedings{Liu2021NeuralAnsweringLogical,
  ids = {liuNeuralAnsweringLogicalQueries2021a},
  title = {Neural-{{Answering Logical Queries}} on {{Knowledge Graphs}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Liu, Lihui and Du, Boxin and Ji, Heng and Zhai, ChengXiang and Tong, Hanghang},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {1087--1097},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3447548.3467375},
  abstract = {Logical queries constitute an important subset of questions posed in knowledge graph question answering systems. Yet, effectively answering logical queries on large knowledge graphs remains a highly challenging problem. Traditional subgraph matching based methods might suffer from the noise and incompleteness of the underlying knowledge graph, often with a prolonged online response time. Recently, an alternative type of method has emerged whose key idea is to embed knowledge graph entities and the query in an embedding space so that the embedding of answer entities is close to that of the query. Compared with subgraph matching based methods, it can better handle the noisy or missing information in knowledge graph, with a faster online response. Promising as it might be, several fundamental limitations still exist, including the linear transformation assumption for modeling relations and the inability to answer complex queries with multiple variable nodes. In this paper, we propose an embedding based method (NewLook) to address these limitations. Our proposed method offers three major advantages. First (Applicability), it supports four types of logical operations and can answer queries with multiple variable nodes. Second (Effectiveness), the proposed NewLook goes beyond the linear transformation assumption, and thus consistently outperforms the existing methods. Third (Efficiency), compared with subgraph matching based methods, NewLook is at least 3 times faster in answering the queries; compared with the existing embed-ding based methods, NewLook bears a comparable or even faster online response and offline training time.},
  isbn = {978-1-4503-8332-5},
  keywords = {knowledge graph embedding,knowledge graph question answering,logical query embedding}
}

@inproceedings{Liu2022JointKnowledge,
  ids = {liuJointKnowledgeGraph2022a},
  title = {Joint {{Knowledge Graph Completion}} and {{Question Answering}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Liu, Lihui and Du, Boxin and Xu, Jiejun and Xia, Yinglong and Tong, Hanghang},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {1098--1108},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539289},
  abstract = {Knowledge graph reasoning plays a pivotal role in many real-world applications, such as network alignment, computational fact-checking, recommendation, and many more. Among these applications, knowledge graph completion (KGC) and multi-hop question answering over knowledge graph (Multi-hop KGQA) are two representative reasoning tasks. In the vast majority of the existing works, the two tasks are considered separately with different models or algorithms. However, we envision that KGC and Multi-hop KGQA are closely related to each other. Therefore, the two tasks will benefit from each other if they are approached adequately. In this work, we propose a neural model named BiNet to jointly handle KGC and multi-hop KGQA, and formulate it as a multi-task learning problem. Specifically, our proposed model leverages a shared embedding space and an answer scoring module, which allows the two tasks to automatically share latent features and learn the interactions between natural language question decoder and answer scoring module. Compared to the existing methods, the proposed BiNet model addresses both multi-hop KGQA and KGC tasks simultaneously with superior performance. Experiment results show that BiNet outperforms state-of-the-art methods on a wide range of KGQA and KGC benchmark datasets.},
  isbn = {978-1-4503-9385-0},
  keywords = {knowledge graph completion,knowledge graph question answering,multi-task learning}
}

@inproceedings{Liu2022MaskReason,
  ids = {liuMaskReasonPreTraining2022a,liuMaskReasonPreTraining2022b},
  title = {Mask and {{Reason}}: {{Pre-Training Knowledge Graph Transformers}} for {{Complex Logical Queries}}},
  shorttitle = {Mask and {{Reason}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Liu, Xiao and Zhao, Shiyu and Su, Kai and Cen, Yukuo and Qiu, Jiezhong and Zhang, Mengdi and Wu, Wei and Dong, Yuxiao and Tang, Jie},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {1120--1130},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539472},
  abstract = {Knowledge graph (KG) embeddings have been a mainstream approach for reasoning over incomplete KGs. However, limited by their inherently shallow and static architectures, they can hardly deal with the rising focus on complex logical queries, which comprise logical operators, imputed edges, multiple source entities, and unknown intermediate entities. In this work, we present the Knowledge Graph Transformer (kgTransformer) with masked pre-training and fine-tuning strategies. We design a KG triple transformation method to enable Transformer to handle KGs, which is further strengthened by the Mixture-of-Experts (MoE) sparse activation. We then formulate the complex logical queries as masked prediction and introduce a two-stage masked pre-training strategy to improve transferability and generalizability.Extensive experiments on two benchmarks demonstrate that kgTransformer can consistently outperform both KG embedding-based baselines and advanced encoders on nine in-domain and out-of-domain reasoning tasks. Additionally, kgTransformer can reason with explainability via providing the full reasoning paths to interpret given answers.},
  isbn = {978-1-4503-9385-0},
  keywords = {graph neural networks,knowledge graph,pre-training}
}

@article{Luus2021LogicEmbeddings,
  title = {Logic {{Embeddings}} for {{Complex Query Answering}}},
  author = {Luus, Francois and Sen, Prithviraj and Kapanipathi, Pavan and Riegel, Ryan and Makondo, Ndivhuwo and Lebese, Thabang and Gray, Alexander},
  year = {2021},
  month = feb,
  journal = {arXiv:2103.00418 [cs]},
  eprint = {2103.00418},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Answering logical queries over incomplete knowledge bases is challenging because: 1) it calls for implicit link prediction, and 2) brute force answering of existential first-order logic queries is exponential in the number of existential variables. Recent work of query embeddings provides fast querying, but most approaches model set logic with closed regions, so lack negation. Query embeddings that do support negation use densities that suffer drawbacks: 1) only improvise logic, 2) use expensive distributions, and 3) poorly model answer uncertainty. In this paper, we propose Logic Embeddings, a new approach to embedding complex queries that uses Skolemisation to eliminate existential variables for efficient querying. It supports negation, but improves on density approaches: 1) integrates well-studied t-norm logic and directly evaluates satisfiability, 2) simplifies modeling with truth values, and 3) models uncertainty with truth bounds. Logic Embeddings are competitively fast and accurate in query answering over large, incomplete knowledge graphs, outperform on negation queries, and in particular, provide improved modeling of answer uncertainty as evidenced by a superior correlation between answer set size and embedding entropy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Logic in Computer Science,Computer Science - Machine Learning}
}

@inproceedings{Lv2020GraphBasedReasoning,
  title = {Graph-{{Based Reasoning}} over {{Heterogeneous External Knowledge}} for {{Commonsense Question Answering}}},
  booktitle = {The {{Thirty-Fourth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2020, {{The Thirty-Second Innovative Applications}} of {{Artificial Intelligence Conference}}, {{IAAI}} 2020, {{The Tenth AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}, {{EAAI}} 2020, {{New York}}, {{NY}}, {{USA}}, {{February}} 7-12, 2020},
  author = {Lv, Shangwen and Guo, Daya and Xu, Jingjing and Tang, Duyu and Duan, Nan and Gong, Ming and Shou, Linjun and Jiang, Daxin and Cao, Guihong and Hu, Songlin},
  year = {2020},
  pages = {8449--8456},
  publisher = {{AAAI Press}}
}

@inproceedings{Ma2022QueryNeighborAware,
  title = {Query and {{Neighbor-Aware Reasoning Based Multi-hop Question Answering}} over {{Knowledge Graph}}},
  booktitle = {Knowledge {{Science}}, {{Engineering}} and {{Management}} - 15th {{International Conference}}, {{KSEM}} 2022, {{Singapore}}, {{August}} 6-8, 2022, {{Proceedings}}, {{Part I}}},
  author = {Ma, Biao and Chen, Xiaoying and Xiong, Shengwu},
  editor = {Memmi, G{\'e}rard and Yang, Baijian and Kong, Linghe and Zhang, Tianwei and Qiu, Meikang},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13368},
  pages = {133--145},
  publisher = {{Springer}},
  doi = {10.1007/978-3-031-10983-6_11}
}

@book{Marker2002Modeltheory,
  title = {Model Theory: An Introduction},
  shorttitle = {Model Theory},
  author = {Marker, D.},
  year = {2002},
  series = {Graduate Texts in Mathematics},
  number = {217},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-98760-6},
  langid = {english},
  lccn = {QA9.7 .M367 2002},
  keywords = {Model theory},
  annotation = {00000}
}

@article{Miller1995WordNetlexical,
  title = {{{WordNet}}: A Lexical Database for {{English}}},
  shorttitle = {{{WordNet}}},
  author = {Miller, George A.},
  year = {1995},
  month = nov,
  journal = {Communications of the ACM},
  volume = {38},
  number = {11},
  pages = {39--41},
  issn = {0001-0782},
  doi = {10.1145/219717.219748},
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].}
}

@inproceedings{PellissierTanon2016FreebaseWikidata,
  title = {From {{Freebase}} to {{Wikidata}}: {{The Great Migration}}},
  shorttitle = {From {{Freebase}} to {{Wikidata}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{World Wide Web}}},
  author = {Pellissier Tanon, Thomas and Vrande{\v c}i{\'c}, Denny and Schaffert, Sebastian and Steiner, Thomas and Pintscher, Lydia},
  year = {2016},
  month = apr,
  series = {{{WWW}} '16},
  pages = {1419--1428},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Republic and Canton of Geneva, CHE}},
  doi = {10.1145/2872427.2874809},
  abstract = {Collaborative knowledge bases that make their data freely available in a machine-readable form are central for the data strategy of many projects and organizations. The two major collaborative knowledge bases are Wikimedia's Wikidata and Google's Freebase. Due to the success of Wikidata, Google decided in 2014 to offer the content of Freebase to the Wikidata community. In this paper, we report on the ongoing transfer efforts and data mapping challenges, and provide an analysis of the effort so far. We describe the Primary Sources Tool, which aims to facilitate this and future data migrations. Throughout the migration, we have gained deep insights into both Wikidata and Freebase, and share and discuss detailed statistics on both knowledge bases.},
  isbn = {978-1-4503-4143-1},
  keywords = {crowdsourcing systems,freebase,semantic web,wikidata}
}

@inproceedings{Ren2020BetaEmbeddings,
  title = {Beta {{Embeddings}} for {{Multi-Hop Logical Reasoning}} in {{Knowledge Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33: {{Annual Conference}} on {{Neural Information Processing Systems}} 2020, {{NeurIPS}} 2020, {{December}} 6-12, 2020, Virtual},
  author = {Ren, Hongyu and Leskovec, Jure},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  year = {2020}
}

@inproceedings{Ren2020Query2boxReasoning,
  title = {Query2box: {{Reasoning}} over {{Knowledge Graphs}} in {{Vector Space Using Box Embeddings}}},
  shorttitle = {Query2box},
  booktitle = {8th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2020, {{Addis Ababa}}, {{Ethiopia}}, {{April}} 26-30, 2020},
  author = {Ren, Hongyu and Hu, Weihua and Leskovec, Jure},
  year = {2020},
  publisher = {{OpenReview.net}}
}

@inproceedings{Ren2021LEGOLatent,
  ids = {renLEGOLatentExecutionGuided2021a},
  title = {{{LEGO}}: {{Latent Execution-Guided Reasoning}} for {{Multi-Hop Question Answering}} on {{Knowledge Graphs}}},
  shorttitle = {{{LEGO}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Ren, Hongyu and Dai, Hanjun and Dai, Bo and Chen, Xinyun and Yasunaga, Michihiro and Sun, Haitian and Schuurmans, Dale and Leskovec, Jure and Zhou, Denny},
  year = {2021},
  month = jul,
  pages = {8959--8970},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Answering complex natural language questions on knowledge graphs (KGQA) is a challenging task. It requires reasoning with the input natural language questions as well as a massive, incomplete heterogeneous KG. Prior methods obtain an abstract structured query graph/tree from the input question and traverse the KG for answers following the query tree. However, they inherently cannot deal with missing links in the KG. Here we present LEGO, a Latent Execution-Guided reasOning framework to handle this challenge in KGQA. LEGO works in an iterative way, which alternates between (1) a Query Synthesizer, which synthesizes a reasoning action and grows the query tree step-by-step, and (2) a Latent Space Executor that executes the reasoning action in the latent embedding space to combat against the missing information in KG. To learn the synthesizer without step-wise supervision, we design a generic latent execution guided bottom-up search procedure to find good execution traces efficiently in the vast query space. Experimental results on several KGQA benchmarks demonstrate the effectiveness of our framework compared with previous state of the art.},
  langid = {english}
}

@inproceedings{Ren2022SMOREKnowledgea,
  title = {{{SMORE}}: {{Knowledge Graph Completion}} and {{Multi-hop Reasoning}} in {{Massive Knowledge Graphs}}},
  shorttitle = {{{SMORE}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ren, Hongyu and Dai, Hanjun and Dai, Bo and Chen, Xinyun and Zhou, Denny and Leskovec, Jure and Schuurmans, Dale},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {1472--1482},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539405},
  abstract = {Knowledge graphs (KGs) capture knowledge in the form of head--relation--tail triples and are a crucial component in many AI systems. There are two important reasoning tasks on KGs: (1) single-hop knowledge graph completion, which involves predicting individual links in the KG; and (2), multi-hop reasoning, where the goal is to predict which KG entities satisfy a given logical query. Embedding-based methods solve both tasks by first computing an embedding for each entity and relation, then using them to form predictions. However, existing scalable KG embedding frameworks only support single-hop knowledge graph completion and cannot be applied to the more challenging multi-hop reasoning task. Here we present Scalable Multi-hOp REasoning (SMORE), the first general framework for both single-hop and multi-hop reasoning in KGs. Using a single machine SMORE can perform multi-hop reasoning in Freebase KG (86M entities, 338M edges), which is 1,500x larger than previously considered KGs. The key to SMORE's runtime performance is a novel bidirectional rejection sampling that achieves a square root reduction of the complexity of online training data generation. Furthermore, SMORE exploits asynchronous scheduling, overlapping CPU-based data sampling, GPU-based embedding computation, and frequent CPU--GPU IO. SMORE increases throughput (i.e., training speed) over prior multi-hop KG frameworks by 2.2x with minimal GPU memory requirements (2GB for training 400-dim embeddings on 86M-node Freebase) and achieves near linear speed-up with the number of GPUs. Moreover, on the simpler single-hop knowledge graph completion task SMORE achieves comparable or even better runtime performance to state-of-the-art frameworks on both single GPU and multi-GPU settings.},
  isbn = {978-1-4503-9385-0},
  keywords = {knowledge graph embeddings,multi-hop reasoning,scalable system}
}

@inproceedings{Ruffinelli2020YouCAN,
  title = {You {{CAN Teach}} an {{Old Dog New Tricks}}! {{On Training Knowledge Graph Embeddings}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ruffinelli, Daniel and Broscheit, Samuel and Gemulla, Rainer},
  year = {2020},
  month = mar,
  abstract = {Knowledge graph embedding (KGE) models learn algebraic representations of the entities and relations in a knowledge graph. A vast number of KGE techniques for multi-relational link prediction have been proposed in the recent literature, often with state-of-the-art performance. These approaches differ along a number of dimensions, including different model architectures, different training strategies, and different approaches to hyperparameter optimization. In this paper, we take a step back and aim to summarize and quantify empirically the impact of each of these dimensions on model performance. We report on the results of an extensive experimental study with popular model architectures and training strategies across a wide range of hyperparameter settings. We found that when trained appropriately, the relative performance differences between various model architectures often shrinks and sometimes even reverses when compared to prior results. For example, RESCAL\textasciitilde\textbackslash citep\{nickel2011three\}, one of the first KGE models, showed strong performance when trained with state-of-the-art techniques; it was competitive to or outperformed more recent architectures. We also found that good (and often superior to prior studies) model configurations can be found by exploring relatively few random samples from a large hyperparameter space. Our results suggest that many of the more advanced architectures and techniques proposed in the literature should be revisited to reassess their individual benefits. To foster further reproducible research, we provide all our implementations and experimental results as part of the open source LibKGE framework.},
  langid = {english}
}

@inproceedings{Saxena2021QuestionAnswering,
  title = {Question {{Answering Over Temporal Knowledge Graphs}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}}, {{ACL}}/{{IJCNLP}} 2021, ({{Volume}} 1: {{Long Papers}}), {{Virtual Event}}, {{August}} 1-6, 2021},
  author = {Saxena, Apoorv and Chakrabarti, Soumen and Talukdar, Partha P.},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  year = {2021},
  pages = {6663--6676},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.acl-long.520}
}

@inproceedings{Schlichtkrull2018ModelingRelational,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}}},
  booktitle = {The {{Semantic Web}}},
  author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and {van~den Berg}, Rianne and Titov, Ivan and Welling, Max},
  editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Rapha{\"e}l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {593--607},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-93417-4_38},
  abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
  isbn = {978-3-319-93417-4},
  langid = {english}
}

@inproceedings{Sun2022JointLKJoint,
  title = {{{JointLK}}: {{Joint Reasoning}} with {{Language Models}} and {{Knowledge Graphs}} for {{Commonsense Question Answering}}},
  shorttitle = {{{JointLK}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{NAACL}} 2022, {{Seattle}}, {{WA}}, {{United States}}, {{July}} 10-15, 2022},
  author = {Sun, Yueqing and Shi, Qi and Qi, Le and Zhang, Yu},
  editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Ru{\'i}z, Iv{\'a}n Vladimir Meza},
  year = {2022},
  pages = {5049--5060},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2022.naacl-main.372}
}

@inproceedings{Talmor2018WebKnowledgeBase,
  title = {The {{Web}} as a {{Knowledge-Base}} for {{Answering Complex Questions}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Talmor, Alon and Berant, Jonathan},
  year = {2018},
  month = jun,
  pages = {641--651},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1059},
  abstract = {Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, ComplexWebQuestions, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.}
}

@inproceedings{Teru2020InductiveRelation,
  title = {Inductive {{Relation Prediction}} by {{Subgraph Reasoning}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Teru, Komal and Denis, Etienne and Hamilton, William L},
  year = {2020},
  month = nov,
  pages = {9448--9457},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The dominant paradigm for relation prediction in knowledge graphs involves learning and operating on latent representations (i.e., embeddings) of entities and relations. However, these embedding-based methods do not explicitly capture the compositional logical rules underlying the knowledge graph, and they are limited to the transductive setting, where the full set of entities must be known during training. Here, we propose a graph neural network based relation prediction framework, GraIL, that reasons over local subgraph structures and has a strong inductive bias to learn entity-independent relational semantics. Unlike embedding-based models, GraIL is naturally inductive and can generalize to unseen entities and graphs after training. We provide theoretical proof and strong empirical evidence that GraIL can rep-resent a useful subset of first-order logic and show that GraIL outperforms existing rule-induction baselines in the inductive setting. We also demonstrate significant gains obtained by ensembling GraIL with various knowledge graph embedding methods in the transductive setting, highlighting the complementary inductive bias of our method.},
  langid = {english}
}

@inproceedings{Toutanova2015Observedlatent,
  title = {Observed versus Latent Features for Knowledge Base and Text Inference},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Continuous Vector Space Models}} and Their {{Compositionality}}},
  author = {Toutanova, Kristina and Chen, Danqi},
  year = {2015},
  month = jul,
  pages = {57--66},
  publisher = {{Association for Computational Linguistics}},
  address = {{Beijing, China}},
  doi = {10.18653/v1/W15-4007}
}

@inproceedings{Trouillon2016ComplexEmbeddings,
  title = {Complex {{Embeddings}} for {{Simple Link Prediction}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Trouillon, Th{\'e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, Eric and Bouchard, Guillaume},
  year = {2016},
  month = jun,
  pages = {2071--2080},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
  langid = {english}
}

@article{VandenBroeck2017QueryProcessing,
  title = {Query {{Processing}} on {{Probabilistic Data}}: {{A Survey}}},
  shorttitle = {Query {{Processing}} on {{Probabilistic Data}}},
  author = {{Van den Broeck}, Guy and Suciu, Dan},
  year = {2017},
  journal = {Foundations and Trends\textregistered{} in Databases},
  volume = {7},
  number = {3-4},
  pages = {197--341},
  issn = {1931-7883, 1931-7891},
  doi = {10.1561/1900000052},
  langid = {english}
}

@article{Vrandecic2014Wikidatafree,
  title = {Wikidata: A Free Collaborative Knowledgebase},
  shorttitle = {Wikidata},
  author = {Vrande{\v c}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  year = {2014},
  month = sep,
  journal = {Communications of the ACM},
  volume = {57},
  number = {10},
  pages = {78--85},
  issn = {0001-0782},
  doi = {10.1145/2629489},
  abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.}
}

@inproceedings{Wang2021BenchmarkingCombinatorial,
  title = {Benchmarking the {{Combinatorial Generalizability}} of {{Complex Query Answering}} on {{Knowledge Graphs}}},
  booktitle = {Proceedings of the {{Neural Information Processing Systems Track}} on {{Datasets}} and {{Benchmarks}} 1, {{NeurIPS Datasets}} and {{Benchmarks}} 2021, {{December}} 2021, Virtual},
  author = {Wang, Zihao and Yin, Hang and Song, Yangqiu},
  editor = {Vanschoren, Joaquin and Yeung, Sai-Kit},
  year = {2021}
}

@inproceedings{Xu2022NeuralSymbolicEntangleda,
  title = {Neural-{{Symbolic Entangled Framework}} for {{Complex Query Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xu, Zezhong and Zhang, Wen and Ye, Peng and Chen, Hui and Chen, Huajun},
  year = {2022},
  month = oct,
  abstract = {Answering complex queries over knowledge graphs (KG) is an important yet challenging task because of the KG incompleteness issue and cascading errors during reasoning. Recent query embedding (QE) approaches embed the entities and relations in a KG and the first-order logic (FOL) queries into a low dimensional space, making the query can be answered by dense similarity searching. However, previous works mainly concentrate on the target answers, ignoring intermediate entities' usefulness, which is essential for relieving the cascading error problem in logical query answering. In addition, these methods are usually designed with their own geometric or distributional embeddings to handle logical operators like union, intersection, and negation, with the sacrifice of the accuracy of the basic operator -- projection, and they could not absorb other embedding methods to their models. In this work, we propose a Neural and Symbolic Entangled framework (ENeSy) for complex query answering, which enables the neural and symbolic reasoning to enhance each other to alleviate the cascading error and KG incompleteness. The projection operator in ENeSy could be any embedding method with the capability of link prediction, and the other FOL operators are handled without parameters. With both neural and symbolic reasoning results contained, ENeSy answers queries in ensembles. We evaluate ENeSy on complex query answering benchmarks, and ENeSy achieves the state-of-the-art, especially in the setting of training model only with the link prediction task.},
  langid = {english}
}

@misc{Yang2022GammaEGamma,
  title = {{{GammaE}}: {{Gamma Embeddings}} for {{Logical Queries}} on {{Knowledge Graphs}}},
  shorttitle = {{{GammaE}}},
  author = {Yang, Dong and Qing, Peijun and Li, Yang and Lu, Haonan and Lin, Xiaodong},
  year = {2022},
  month = oct,
  number = {arXiv:2210.15578},
  eprint = {2210.15578},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.15578},
  abstract = {Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a challenging problem due to massive and complicated structures in many KGs. Recently, many promising works projected entities and queries into a geometric space to efficiently find answers. However, it remains challenging to model the negation and union operator. The negation operator has no strict boundaries, which generates overlapped embeddings and leads to obtaining ambiguous answers. An additional limitation is that the union operator is non-closure, which undermines the model to handle a series of union operators. To address these problems, we propose a novel probabilistic embedding model, namely Gamma Embeddings (GammaE), for encoding entities and queries to answer different types of FOL queries on KGs. We utilize the linear property and strong boundary support of the Gamma distribution to capture more features of entities and queries, which dramatically reduces model uncertainty. Furthermore, GammaE implements the Gamma mixture method to design the closed union operator. The performance of GammaE is validated on three large logical query datasets. Experimental results show that GammaE significantly outperforms state-of-the-art models on public benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning}
}

@inproceedings{Yasunaga2021QAGNNReasoning,
  title = {{{QA-GNN}}: {{Reasoning}} with {{Language Models}} and {{Knowledge Graphs}} for {{Question Answering}}},
  shorttitle = {{{QA-GNN}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{NAACL-HLT}} 2021, {{Online}}, {{June}} 6-11, 2021},
  author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
  editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and {Hakkani-T{\"u}r}, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  year = {2021},
  pages = {535--546},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.naacl-main.45}
}

@inproceedings{Zaheer2017DeepSets,
  title = {Deep {{Sets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.}
}

@inproceedings{Zhang2021ConECone,
  ids = {zhangConEConeEmbeddings2021a},
  title = {{{ConE}}: {{Cone Embeddings}} for {{Multi-Hop Reasoning}} over {{Knowledge Graphs}}},
  shorttitle = {{{ConE}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Zhanqiu and Wang, Jie and Chen, Jiajun and Ji, Shuiwang and Wu, Feng},
  year = {2021},
  volume = {34},
  pages = {19172--19183},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Query embedding (QE)---which aims to embed entities and first-order logical (FOL) queries in low-dimensional spaces---has shown great power in multi-hop reasoning over knowledge graphs. Recently, embedding entities and queries with geometric shapes becomes a promising direction, as geometric shapes can naturally represent answer sets of queries and logical relationships among them. However, existing geometry-based models have difficulty in modeling queries with negation, which significantly limits their applicability. To address this challenge, we propose a novel query embedding model, namely \textbackslash textbf\{Con\}e \textbackslash textbf\{E\}mbeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunction operations. By further noticing that the closure of complement of cones remains cones, we design geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets.}
}

@article{Zhang2021Drugrepurposing,
  title = {Drug Repurposing for {{COVID-19}} via Knowledge Graph Completion},
  author = {Zhang, Rui and Hristovski, Dimitar and Schutte, Dalton and Kastrin, Andrej and Fiszman, Marcelo and Kilicoglu, Halil},
  year = {2021},
  month = mar,
  journal = {Journal of Biomedical Informatics},
  volume = {115},
  pages = {103696},
  issn = {1532-0480},
  doi = {10.1016/j.jbi.2021.103696},
  abstract = {OBJECTIVE: To discover candidate drugs to repurpose for COVID-19 using literature-derived knowledge and knowledge graph completion methods. METHODS: We propose a novel, integrative, and neural network-based literature-based discovery (LBD) approach to identify drug candidates from PubMed and other COVID-19-focused research literature. Our approach relies on semantic triples extracted using SemRep (via SemMedDB). We identified an informative and accurate subset of semantic triples using filtering rules and an accuracy classifier developed on a BERT variant. We used this subset to construct a knowledge graph, and applied five state-of-the-art, neural knowledge graph completion algorithms (i.e., TransE, RotatE, DistMult, ComplEx, and STELP) to predict drug repurposing candidates. The models were trained and assessed using a time slicing approach and the predicted drugs were compared with a list of drugs reported in the literature and evaluated in clinical trials. These models were complemented by a discovery pattern-based approach. RESULTS: Accuracy classifier based on PubMedBERT achieved the best performance (F1 = 0.854) in identifying accurate semantic predications. Among five knowledge graph completion models, TransE outperformed others (MR~=~0.923, Hits@1~=~0.417). Some known drugs linked to COVID-19 in the literature were identified, as well as others that have not yet been studied. Discovery patterns enabled identification of additional candidate drugs and generation of plausible hypotheses regarding the links between the candidate drugs and COVID-19. Among them, five highly ranked and novel drugs (i.e., paclitaxel, SB 203580, alpha 2-antiplasmin, metoclopramide, and oxymatrine) and the mechanistic explanations for their potential use are further discussed. CONCLUSION: We showed that a LBD approach can be feasible not only for discovering drug candidates for COVID-19, but also for generating mechanistic explanations. Our approach can be generalized to other diseases as well as to other clinical questions. Source code and data are available at https://github.com/kilicogluh/lbd-covid.},
  langid = {english},
  pmcid = {PMC7869625},
  pmid = {33571675},
  keywords = {Algorithms,Antiviral Agents,COVID-19,COVID-19 Drug Treatment,Drug Repositioning,Drug repurposing,Humans,Knowledge Discovery,Knowledge graph completion,Literature-based discovery,Neural Networks; Computer,SARS-CoV-2,Text mining}
}

@inproceedings{Zhang2021MultiHopReasoning,
  title = {Multi-{{Hop Reasoning}} for {{Question Answering}} with {{Knowledge Graph}}},
  booktitle = {19th {{IEEE}}/{{ACIS International Conference}} on {{Computer}} and {{Information Science}}, {{ICIS}} 2021, {{Shanghai}}, {{China}}, {{June}} 23-25, 2021},
  author = {Zhang, Jiayuan and Cai, Yifei and Zhang, Qian and Cao, Zehao and Cheng, Zhenrong and Li, Dongmei and Meng, Xianghao},
  year = {2021},
  pages = {121--125},
  publisher = {{IEEE}},
  doi = {10.1109/ICIS51600.2021.9516865}
}

@inproceedings{Zhang2022FactTreeReasoning,
  title = {Fact-{{Tree Reasoning}} for {{N-ary Question Answering}} over {{Knowledge Graphs}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022, {{Dublin}}, {{Ireland}}, {{May}} 22-27, 2022},
  author = {Zhang, Yao and Li, Peiyao and Liang, Hongru and Jatowt, Adam and Yang, Zhenglu},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = {2022},
  pages = {788--802},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2022.findings-acl.66}
}

@article{Zhang2022KnowledgeGraph,
  title = {Knowledge {{Graph Reasoning}} with {{Logics}} and {{Embeddings}}: {{Survey}} and {{Perspective}}},
  shorttitle = {Knowledge {{Graph Reasoning}} with {{Logics}} and {{Embeddings}}},
  author = {Zhang, Wen and Chen, Jiaoyan and Li, Juan and Xu, Zezhong and Pan, Jeff Z. and Chen, Huajun},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.07412 [cs]},
  eprint = {2202.07412},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Knowledge graph (KG) reasoning is becoming increasingly popular in both academia and industry. Conventional KG reasoning based on symbolic logic is deterministic, with reasoning results being explainable, while modern embedding-based reasoning can deal with uncertainty and predict plausible knowledge, often with high efficiency via vector computation. A promising direction is to integrate both logic-based and embedding-based methods, with the vision to have advantages of both. It has attracted wide research attention with more and more works published in recent years. In this paper, we comprehensively survey these works, focusing on how logics and embeddings are integrated. We first briefly introduce preliminaries, then systematically categorize and discuss works of logic and embedding-aware KG reasoning from different perspectives, and finally conclude and discuss the challenges and further directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {00000}
}

@article{Zhao2022Improvingquestion,
  title = {Improving Question Answering over Incomplete Knowledge Graphs with Relation Prediction},
  author = {Zhao, Fen and Li, Yinguo and Hou, Jie and Bai, Ling},
  year = {2022},
  journal = {Neural Computing and Applications},
  issn = {1433-3058},
  doi = {10.1007/s00521-021-06736-7},
  abstract = {Large-scale knowledge graphs (KGs) play a critical role in question answering over KGs (KGs-QA). Despite of large scale, KGs suffer from incompleteness, which has fueled a lot of research on relation prediction. Since existing researches of relation prediction process each triple independently, the hidden relations which are inherently present can not be captured. Complementarily, to simultaneously capture both entity features and relation features in a given entity's neighborhood, an entity importance estimation network of attention-based graph embedding is proposed, which consists of the attention-based graph embedding module and the entity importance estimation module. Firstly, the new embedding of an entity from its n-hop neighbor is learned by an attention-based graph embedding module. Then, the learned new embedding is integrated into the entity importance estimation module to find entities of high importance in n-hop neighbors of the central entity. Finally, multi-hop relations are encapsulated and an auxiliary edge of n-hop neighbors is introduced, which realizes the relation prediction task. To the best our knowledge, we are the first to realize KGs-QA while realizing relation prediction, which alleviates the phenomenon of missing relations and the low-precision problem of KGs-QA. On the SQ datasets, the proposed method obtains a high F1 score (49.3\%) in 10\% missing relation, compared to QASE and MCCNNs with F1 scores of 44.2\% and 46.3\%, respectively.},
  langid = {english},
  annotation = {00000}
}

@inproceedings{Zhao2022SimulateHuman,
  title = {Simulate {{Human Thinking}}: {{Cognitive Knowledge Graph Reasoning}} for {{Complex Question Answering}}},
  shorttitle = {Simulate {{Human Thinking}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}} - 26th {{Pacific-Asia Conference}}, {{PAKDD}} 2022, {{Chengdu}}, {{China}}, {{May}} 16-19, 2022, {{Proceedings}}, {{Part I}}},
  author = {Zhao, Hong and Fu, Yao and Jiang, Weihao and Pu, Shiliang and Cai, Xiaoyu},
  editor = {Gama, Jo{\~a}o and Li, Tianrui and Yu, Yang and Chen, Enhong and Zheng, Yu and Teng, Fei},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13280},
  pages = {522--534},
  publisher = {{Springer}},
  doi = {10.1007/978-3-031-05933-9_41}
}

@inproceedings{Zhu2021NeuralBellmanForda,
  title = {Neural {{Bellman-Ford Networks}}: {{A General Graph Neural Network Framework}} for {{Link Prediction}}},
  shorttitle = {Neural {{Bellman-Ford Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhu, Zhaocheng and Zhang, Zuobai and Xhonneux, Louis-Pascal and Tang, Jian},
  year = {2021},
  volume = {34},
  pages = {29476--29490},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in this paper we propose a general and flexible representation learning framework based on paths for link prediction. Specifically, we define the representation of a pair of nodes as the generalized sum of all path representations, with each path representation as the generalized product of the edge representations in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that the proposed path formulation can be efficiently solved by the generalized Bellman-Ford algorithm. To further improve the capacity of the path formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general graph neural network framework that solves the path formulation with learned operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes the generalized Bellman-Ford algorithm with 3 neural components, namely Indicator, Message and Aggregate functions, which corresponds to the boundary condition, multiplication operator, and summation operator respectively. The NBFNet covers many traditional path-based methods, and can be applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings. Experiments on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing methods by a large margin in both transductive and inductive settings, achieving new state-of-the-art results.}
}

@misc{Zhu2022NeuralSymbolicModelsa,
  title = {Neural-{{Symbolic Models}} for {{Logical Queries}} on {{Knowledge Graphs}}},
  author = {Zhu, Zhaocheng and Galkin, Mikhail and Zhang, Zuobai and Tang, Jian},
  year = {2022},
  month = may,
  number = {arXiv:2205.10128},
  eprint = {2205.10128},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Answering complex first-order logic (FOL) queries on knowledge graphs is a fundamental task for multi-hop reasoning. Traditional symbolic methods traverse a complete knowledge graph to extract the answers, which provides good interpretation for each step. Recent neural methods learn geometric embeddings for complex queries. These methods can generalize to incomplete knowledge graphs, but their reasoning process is hard to interpret. In this paper, we propose Graph Neural Network Query Executor (GNN-QE), a neural-symbolic model that enjoys the advantages of both worlds. GNN-QE decomposes a complex FOL query into relation projections and logical operations over fuzzy sets, which provides interpretability for intermediate variables. To reason about the missing links, GNN-QE adapts a graph neural network from knowledge graph completion to execute the relation projections, and models the logical operations with product fuzzy logic. Extensive experiments on 3 datasets show that GNN-QE significantly improves over previous state-of-the-art models in answering FOL queries. Meanwhile, GNN-QE can predict the number of answers without explicit supervision, and provide visualizations for intermediate variables.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}

@incollection{emerson1990temporal,
  title={Temporal and modal logic},
  author={Emerson, E Allen},
  booktitle={Formal Models and Semantics},
  pages={995--1072},
  year={1990},
  publisher={Elsevier}
}

@inproceedings{DBLP:conf/iclr/BelloPL0B17,
  author    = {Irwan Bello and
               Hieu Pham and
               Quoc V. Le and
               Mohammad Norouzi and
               Samy Bengio},
  title     = {Neural Combinatorial Optimization with Reinforcement Learning},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Bk9mxlSFx},
  timestamp = {Thu, 04 Apr 2019 13:20:08 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BelloPL0B17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{petroni2019language,
  title={Language Models as Knowledge Bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2463--2473},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{xiong2017deeppath,
  title={DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning},
  author={Xiong, Wenhan and Hoang, Thien and Wang, William Yang},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={564--573},
  year={2017}
}

@inproceedings{LMPNN,
  author    = {Zihao Wang and
               Yangqiu Song and
               Ginny Y. Wong and
               Simon See},
  title     = {Logical Message Passing Networks with One-hop Inference on Atomic Formulas},
  booktitle = {The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali Rwanda, May 1-5, 2023},
  publisher = {OpenReview.net},
  year      = {2023},
  url       = {https://openreview.net/forum?id=SoyOsp7i_l},
}

@article{shi2016survey,
  title={A survey of heterogeneous information network analysis},
  author={Shi, Chuan and Li, Yitong and Zhang, Jiawei and Sun, Yizhou and Philip, S Yu},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={29},
  number={1},
  pages={17--37},
  year={2016},
  publisher={IEEE}
}

@book{rossi2006handbook,
  title={Handbook of constraint programming},
  author={Rossi, Francesca and Van Beek, Peter and Walsh, Toby},
  year={2006},
  publisher={Elsevier}
}

