\section{Requirements for a Multi-modal Data Science Platform}
\label{sec:requ-holist-data}

In this section we outline the requirements of a multi-modal data science platform and then introduce Vizier as a solution fulfilling these requirements. We start by discussing existing modalities that are widely used to interact with data in data science and their advantages and disadvantages. Converging on a set of modalities we deem to be essential, we then cover several cross-cutting concerns such as reproducibility and dealing with errors that are important no matter what modality is used to interact with data.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Modalities for Interacting with Data}
\label{sec:modal-inter-with}

Data scientists often employ several tools during the life-cycle of building a data pipelines. During data discovery, search engines (e.g., open data repositories) and dataset discovery tools (e.g., metadata management tools for data lakes) are used to identify data internal to an organization, data available for purchase, or openly available data that could be used to fulfill the goal the data scientist has in mind. The next step is typically to profile the data and understand its semantics and fitness for the task at hand. At this stage, data visualization is used, either in the form of specialized visualization systems or by employing visualization libraries written in, e.g., Python. This is often done from within notebook environments like Jupyter which can show visualizations inline with the users code. Afterwards, data is curated and integrated. Spreadsheets are often used for manual inspection of smaller datasets, repair of one-off errors, and to calculate basic statistics. Task-specific data cleaning systems or libraries written in a general purpose programming language are employed to (partially) automate this process. The cleaned and integrated data is then used in data analysis (e.g., building and evaluating machine learning models, running analytical queries, \ldots) and the final results of the analysis are visualized. Building a data pipeline is often an iterative process where the user revisits previous steps in the pipeline to deal with errors and to refine the processing.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Notebooks for Interactive Pipeline Development with Immediate Feedback}
\label{sec:noteb-inter-pipel}
%
Notebook systems like Jupyter and Apache Zeppelin to name just a few have become the quasi-standard for developing data science pipelines. One major advantage of notebooks is their highly interactive nature: users can run pieces of code and immediately observe their results. This makes it easy to debug steps in the pipeline and aids iterative development of pipelines. The outputs recorded in a notebook serve as a documentation of the data preparation and analysis process. Furthermore, most notebook system allow support documentation (typically written in a markup language such as markdown) to be interleaved with code. Even notebook interfaces have become prevalent, most existing implementations are suffer from poor reproducibility, do not support iterative development well, and are not suited well for large and complex pipelines. A recent study~\cite{PM19} on Jupyter notebooks collected from github observed that only 4\% of these notebooks are reproducibly in the sense that they can be rerun without errors and produce the same result as recorded in the notebook. We have argued in \cite{BS20, DG22} that these shortcomings are not inherent to the notebook model, but rather are the result of the architecture of notebook systems which use a long running kernel (e.g., a Python interpreter) and when the user runs a cell in a notebook send the cell's code to the kernel for execution. The kernels state, however, is hidden from the user and except for cell outputs is not encoded in the notebook itself. This leads to unreproducible behavior where the results recorded in the notebook no longer agree with a serial (top-down) execution of the notebook's cells. Furthermore, this can lead to stale results, if the user forgets to rerun cells whose code does dependent on the code in a cell that has been changed. Given the many benefits of and broad use of notebooks, support for a notebook interface is a must for any data science platform. We argue that by providing a notebook interface on top of a specialized workflow engine we can avoid the pitfalls of notebook systems which are thin wrappers around a kernel (Python interpreter). Specifically, we want a solution that supports a notebook interface (\textbf{requirement (N)} which fulfills the following conditions:
\begin{itemize}
\item \textbf{(N1) Serial Execution Semantics:} At any point in time, the results for the cells of the notebook agree with the results produced by a serial execution of the notebook.
\item \textbf{(N2) No Stale Outputs:} The outputs of each cell in the notebook should correctly reflect the current version of the notebook's code.
\item \textbf{(N3) Reproducibility:} The execution of a pipeline (notebook) does not depend on any hidden state. That is, as long as the code in the notebook is deterministic, rerunning a pipeline produces the same output as the original execution.
\end{itemize}

Note that any system that guarantees serial execution of notebooks as defined above, automatically guarantees that there are no stale outputs and that the execution of a notebook is reproducible.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Spreadsheets for Manual Curation and Exploration}
\label{sec:spre-manu-curat}
%
While notebooks are suited well for programmatic transformation and exploration of data, spreadsheet interfaces enable manual exploration and one-off transformations and fixes~\cite{FG16}. For instance, a user may search for and correct a few mistyped attributes values through a spreadsheet interface. Spreadsheets are also suited well for testing simple data transformations on a few rows using formulas and then once the transformation is satisfactory apply the transformation to a large number of rows by applying the same formula to many rows. Many spreadsheet systems have support for tracking changes made to the spreadsheet, but lack mechanisms to navigate the version history of a spreadsheet and to create branches, e.g., to try out a crazy idea without breaking a deployed version of a data science pipeline. As observed elsewhere~\cite{bendre-19-fhs}, spreadsheet systems do not typically scale to large datasets. While it is anyways infeasible to manually clean and curate large datasets, developing manual fixes on a sample / subset of a large dataset and then deploying the fixes to the dataset can be quite effective. Most spreadsheet systems use a data model that is incompatible with other structured data models such as the relational data model or data frames (which are essentially relations with row indexes) in that (i) columns do not have to be declared but are used by inserting a value into one cell of a column, (ii) the values of a column do not need to belong to a single datatype (e.g., some cells in a column may store strings while others are integers), and (iii) some operations  change the row and column positions rather than their content (e.g., inserting a row, changes the positions of all following rows). To support spreadsheets as a modality (\textbf{requirement (S)}, data science platforms should:
\begin{itemize}
\item \textbf{(S1) Translating between Relations and Spreadsheets:} It should be possible to manipulate standard relations through a spreadsheet view as well as process spreadsheets in other modalities.
\item \textbf{(S2) Supporting Spreadsheet Operations over Relations:} It should be possible to apply spreadsheet operations such as inserting / deleting rows / columns.
\item \textbf{(S3) Scalability:} It should be possible access large datasets through a spreadsheet interface.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Profiling and Interactive Visualizations}
\label{sec:inter-visu}
%
Data profiling and visualization are important tools for data scientists to explore data, understands its semantics, and identify problems with the data.
Data science platforms should support visualizations (\textbf{requirement (V)}). For example, a typical task in the initial data exploration phase of constructing a pipeline is to analyze and visualize the data distributions of columns in a dataset, e.g., by computing histograms for individual columns or by calculating the number of null values per column. Another common approach is to visualize correlations between columns. Both the spreadsheet and notebook modalities discussed so far, do support creation of plots and other data visualizations. Visualization recommendation~\cite{lee-21-l, hu-19-v} guide the user in exploring visualizations that help them to understand their data. A data science platform should support out-of-the-box visualizations  for common tasks (\textbf{requirement (V1)}, e.g., accessing data distributions as histograms as well as provide access to more general visualization tools (\textbf{requirement (V2)} to enable creation of custom visualization of, e.g., analysis results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsubsection{Semi-automated Data Cleaning, Curation, and Integration}
\label{sec:semi-automated-data}
%
As has been observed repeatedly in the past~\cite{nyt:wrangling}, data scientists spend the majority of their time in data discovery, preparation, and curation. Data cleaning, integration, and curation are complex tasks that are time consuming and error-prone. While full automation of these tasks is typically not an option, a plethora of semi-automated tools (e.g., constraint-based data cleaning~\cite{ilyas-15-tcrd}, schema matching~\cite{RB01}, and data integration~\cite{HR06}) which rely on heuristics and often involve the user in curation decisions have been proposed and exist in the form of stand-alone tools and libraries available for languages such as Python. Data science platforms should enable users to use such tools and algorithms in their data science pipelines (\textbf{requirement (C)}. Most approaches for automating data wrangling rely on heuristics to clean data. This is due to the fact that typically insufficient information is available to determine what the correct repair for a dataset is. For instance, when repairing a primary key constraint by tuple deletion~\cite{ilyas-15-tcrd}, one has to retain at most one tuple from each group of tuples with the same primary key value, however, we typically lack information to decide which tuple is the correct one to keep. Thus, data repair algorithms instead use heuristics such as preferring tuples with values that are common in the dataset or optimizing a global metric~\cite{RC17}. Data science platforms should track changes made based on heuristics and how they affect downstream operations as well as provide the user with an overview of what other choices did exist and how different choices would have affected the user's analysis results (\textbf{requirement (U)}):
\begin{itemize}
\item \textbf{(C) (Semi-)automated Data Curation and Integration:} Data science platforms should empower users to access (automated) data curation, cleaning, and integration techniques.
\item \textbf{(U) Tracking the Impact of Uncertain Choices:}  The impact of heuristic choices should be tracked through the operations in a data pipeline to aide the user in understanding how these choices have affected their analysis results and how the results would be affected if another alternative would have been chosen during data cleaning.
\end{itemize}

In summary, all modalities we have discussed so far have in common that they provide ways for the user to interact with data by applying transformations that create new data artifacts or update existing data artifacts and by viewing artifacts through user interfaces. For instance, plotting a histogram of the value distribution of an attribute in a csv file involves several transformations: (i) load the CSV file into a suitable in-memory data structure (e.g., a Pandas dataframe); (ii) aggregate the data to create a histogram; (iii) transform the histogram into a plot. We argue that it is feasible to build multiple modalities as ``views'' on top of a single dataflow platform. This approach has the advantage that the user does not have to to manually transform data between different formats expected by systems implementing the different modalities. Even more important, critical orthogonal functionality  such as versioning (that we will discuss next) has to only be implemented once.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Cross-cutting Concerns}
\label{sec:cross-cutt-conc}

Independent of which modality is used to interact with the data, there are important cross-cutting concerns such as reproducibility, supporting the user in iterative development of their data pipelines, dealing with data errors and uncertainty, and how to document data which need to be addressed. Some of the issues with implementing these cross-cutting functionality for specific modalities was already discussing in \Cref{sec:modal-inter-with}, but here we

\begin{itemize}
\item \textbf{(R) Reproducibility and Versioning:} Developing a data science pipeline typically requires several rounds of iterative development and refinement and may involve more than one developer. Keeping track of versions of the pipeline (and the associated results) in a version control manner is critical for aiding users in their development process (e.g., roll back to a past working version of the pipeline or test an experimental idea in a separate branch). However, we argue that unlike in version control systems where the user decides which versions of their code are persisted, for data science platforms it is beneficial if by default all past versions are retained (\textbf{requirement (R1)}). That is there should be no difference between the state kept for supporting undo as well as state kept for versioning. Furthermore, for reproducibility, unless the user's code contains non-deterministic operations, repeated executions of a pipeline should return the same result. Since we want to support interactive modalities like spreadsheets, modifications made through such modalities (e.g., manual edits of cell values or inserting and deleting rows in a spreadsheet) have to be translated into steps in the pipeline (\textbf{requirement (R2)}).
\item \textbf{(I) Supporting Iterative Refinement of Pipelines:} Data pipelines are typically constructed in an iterative fashion by adding additional steps and revisiting prior steps in the pipeline. For example, if an analysis returns unexpected results, the user may backtrack and modify steps in the pipeline which clean the data that is used in the analysis. A data science platform should support users in this process by (i) providing an overview of the structure of the pipeline to help the user to navigate between different parts of the pipeline (\textbf{requirement (O)}), (ii) by providing coarse-grained provenance to help the user identify which steps affected the data used by a pipeline step (\textbf{requirement (P)}), and (iii) ensure that outputs of pipeline steps are always up to date when upstream steps are modified. Note that the last requirement is the ``no stale outputs'' requirement (\textbf{requirement (N2)}) we have already discussed in \Cref{sec:noteb-inter-pipel}.
\item \textbf{(D) Documenting Data and Operations:} One advantage of notebook style interfaces is that they allow code and data to be documented using a markup language. For instance, a user may use this feature to document some insights into the semantics of their data or to explain why they selected a particular cleaning or learning technique. However, documentation in notebooks is associated with steps in the pipeline (cells in the notebook) rather than with pieces of data. While this type of documentation should be support (\textbf{requirement (D1)}), data documentation serves a wide range of purposes such as documenting data semantics (e.g., the year for a column storing dates as a month and day of the month), encoding information of how data was collected and processed, and recording information about issues with the data (e.g., a heart rate measurement is outside of the physically possible range). In \cite{kumari:2021:cidr:datasense} we argued that documentation pertaining to data is mission critical should be associated with the data (\textbf{requirement (D2)}) and should persist through operations (\textbf{requirement (D3)}). For instance, if the user annotated some values in a dataset with a note explaining that these values are suspicious, then aggregated summaries derived from the data should also be associated with these notes.
\item \textbf{(E) Dealing with Errors and Uncertainty:} Uncertainty and errors are prevalent in many application domains due to sensor, outliers~\cite{HA04}, and data entry errors, heuristics applied during data curation, integration~\cite{AS10, FK11b}, and cleaning~\cite{YM15, BS10a}, and misinterpretation of data semantics. Data science platforms should help users in identifying errors, should provide access to semi-automated (heuristic) methods for cleaning and curating data such as the data repair techniques~\cite{B19, ilyas-15-tcrd}. Furthermore, the platform should help the user to determine how choices made during cleaning (whether by a human or an algorithm) affect the results of analyzing the data.
\end{itemize}

In this section we have outlined a set of requirements, both to support specific modalities of interacting with data as well as general requirements that any system for building data science pipelines should support.


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../2022_IEEE_DEB_Vizier"
%%% End:
