\section{Related Work}
\label{sec:relwork}
There has been a significant interest in testing query equivalence or correctness and in automated grading. Related work include the following.

\subsection*{Checking Query Equivalence}

XData uses test data generation based on mutation testing to generate test datasets to check the equivalence of the correct query and student query. Tuya et al.~\cite{mutation1} describe a number of possible mutations for SQL queries. However, they do not handle test data generation for killing these mutations. Other approaches on testing query equivalence using datasets include Qex~\cite{QEX} and SQL full predicate coverage by Riva et al.~\cite{Riva:2010}. Test data generation in these systems is aimed at testing SQL queries in database applications and they consider only a limited subset of SQL query constructs. 

Techniques based on tableau~\cite{tableaux} and its extensions~\cite{tableaux1, tableaux2} can be used to check for query equivalence for a restricted class of conjunctive queries. Cosette~\cite{cosette} and U-semiring~\cite{semantic} can also be used to check for SQL semantic equivalence using a restricted set of axioms. 
SPES~\cite{spes} uses a symbolic approach for checking query equivalence on SQL queries under bag semantics for select-project-join(SPJ) queries as well as aggregate and union queries. 

\subsection*{Grading SQL Queries}
The Gradiance system~\cite{gradience} provides multiple choice type questions where the instructor has to provide some correct as well as incorrect answers and explanations of incorrectness. In such assignment settings, since the students are only able to select from limited options, subtle mistakes that students could make may not always be covered by the incorrect answers. Gradescope\cite{gradescope}, uses a fixed dataset to evaluate the correctness of student SQL queries. As discussed earlier, using fixed datasets may miss errors and would not be able to provide any meaningful feedback to incorrect student queries. RATest~\cite{rattest} provides feedback for incorrect queries by deriving small datasets that produce different results in a student query as compared to a correct query. I-Rex~\cite{irex} allows users to trace the SQL query evaluation for each constituent block in the query execution. 


\subsection*{Automated Grading for Programming Assignments}
Grading programming assignments has some similar challenges as grading database queries. CPSGrader~\cite{cpsgrader} can grade programming assignments for cyber-physical systems using constraints synthesis and uses reference solutions to provide feedback. AutoGrader~\cite{autograder} can grade introductory level python programs and provide student feedback using program synthesis and high-level error modeling specifications. However, AutoGrader can only model specific predictable errors. 

SARFGEN~\cite{sarfgen} provides feedback to student queries by aligning student programs to similar correct reference programs and finds the minimum number of edits to the student program to match the chosen reference program.  This approach is similar to our approach of edit-based grading. However, SQL queries have database constraints that are not part of the query but need to be accounted for during edits and equivalence checking. Hence we have a more complex semantic canonicalization step that can take into account constraints such as primary keys and foreign keys.



