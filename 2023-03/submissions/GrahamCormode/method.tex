\section{Method}
\label{sec:method}

In this section, we propose solutions to
% the challenges identified in Section~\ref{subsec:comp_methods} to
reconcile security (\SecAgg) and communication efficiency.
Our approach is to modify compression techniques to share some hyperparameters globally across all clients so that aggregation can be done by uniformly combining each client's response, while still ensuring that there is scope to achieve accurate compressed representations.
\modif{As detailed below, each of the proposed methods offers the same level of security as standard \SecAgg without compression.}

\subsection{Secure Aggregation and Compression}
\label{subsec:secagg_comp}
We propose to compress the uplink model updates through a compression operator $q$, whose parameters are round-dependent but the same for all clients participating in the same round.
Then, we will add a random mask $m_i$ to each quantized client update $q(g_i)$ in the compressed domain, thus effectively reducing uplink bandwidth while ensuring that $h_i = q(g_i) + m_i$ is statistically indistinguishable from any other representable value in the finite group (see Section~\ref{subsec:secagg}).
In this setting, \SecAgg allows the server to recover the aggregate of the client model updates in the compressed domain: $\sum_i q(g_i)$.
% \begin{equation*}
%     \sum_i q(g_i) = \sum_i h_i- \sum_i m_i
% \end{equation*}
If the decompression operator $d$ is linear, the server is able to recover the aggregate in the non-compressed domain, up to quantization error, as illustrated in Figure~\ref{fig:secagg_summary}:
\begin{equation*}\textstyle
    d\left(\sum_i h_i - \sum_i m_i\right) = d\left(\sum_i q(g_i)\right) =\sum_i d(q(g_i)) \approx \sum_i g_i.
\end{equation*}
% \ashkan{according to the top equation in page 4, $\sum h_i$ minus $\sum m_i$ is exactly $\sum g_i$ and not $\sum q(g_i)$. I understand here we are talking about compression. But a reviewer might complain here that this is not consistent. Should we add $q(.)$ around the first part too?}
The server periodically updates the quantization and decompression operator parameters, either from the aggregated model update, which is deemed public, or by emulating a client update on some similarly distributed public data. Once these parameters are updated, the server broadcasts them to the clients for the next round. \modif{This adds overhead to the downlink communication payload, however, this is negligible compared to the downlink model size to transmit}. For instance, for scalar quantization, $q$ is entirely characterized by one \texttt{fp32} scale and one \texttt{int32} zero-point per layer, the latter of which is unnecessary in the case of a symmetric quantization scheme. Finally, this approach is compatible with both synchronous FL methods such as FedAvg~\cite{mcmahan2016communicationefficient} and asynchronous methods such as FedBuff~\cite{nguyen2021federated} as long as \SecAgg maintains the mapping between the successive versions of quantization parameters and the corresponding client updates.

\subsection{Application}
Next, we show how we adapt scalar quantization and random pruning with no changes required to \SecAgg. We illustrate our point with TEE-based \SecAgg while these adapted uplink compression mechanisms are agnostic of the \SecAgg mechanism. Finally, we show how to obtain extreme uplink compression by proposing a variant of \SecAgg, which we call \SecInd. This variant supports product quantization and is provably secure.

\subsubsection{Scalar Quantization and Secure Aggregation}
\label{subsubsec:sq_sa}

As detailed in Section~\ref{subsec:sq}, a model update matrix $g_i$ compressed with scalar quantization is given by an integer representation in the range $[0, 2^{b}-1]$ and by the quantization parameters \emph{scale} ($s$) and \emph{zero-point} ($z$). A sufficient condition for the decompression operator to be linear is to broadcast common quantization parameters per layer for each client. Denote $q(g_i)$ as the integer representation of quantized client model update $g_i$ corresponding to a particular layer for client $1\leq i \leq N$.
%Let us assume that the server's aggregation method is the sum.
Set the scale of the decompression operator to $s$ and its zero-point to $z/N$.
Then, the server is able to decompress as follows (where the decompression operator is defined in Section~\ref{subsec:sq}):
\begin{equation*}\textstyle
    % (s\sum_i  q(g_i)) - \frac{sz}{n} = \sum_i s(q(g_i) - z) \simeq \sum_i g_i.
    d\left(\sum_i q(g_i)\right) = s\sum_i  q(g_i) -  \frac{z}{N}  = \sum_i \left( s(q(g_i)) - z \right) \approx \sum_i g_i
\end{equation*}
Recall that all operations are performed in a finite group. Therefore, to avoid overflows at aggregation time, we quantize with a bit-width $b$ but take \SecAgg bit-width $p > b$, thus creating a margin for potential overflows (see Section~\ref{subsec:ablations}).
%\ilya{Do we really? (Make SecAgg bitwidth larger than b.) Masked updates are defined only modulo $2^b$, adding them in larger group makes little sense.}.
%
This approach is related to the fixed-point aggregation described in \cite{bonavitz2019federated,huba2021papaya}, but we calibrate the quantization parameters and perform the calibration per layer and periodically, unlike the related approaches.
% whereas the scale parameter used in fixed-point conversion does not change during the training and is common to the whole network.\ilya{I am confused - does the scale change periodically or it doesn't?}

\modif{\para{Privacy, Security and Bandwidth.}} Scales and zero points are determined from public data on the server. Downlink overhead is negligible: the server broadcasts the per-layer quantization parameters. The upload bandwidth is $p$ bits per weight, where $p$ is the \SecAgg finite group size (Section~\ref{subsec:secagg}). \modif{Since the masks $m_i$ are chosen in the integer range $[0, 2^p-1]$, any masked integer representation taken modulo $2^p$ is statistically indistinguishable from any other vector.}
% The maximum ``price of security'' is thus the overflow buffer of $p-b = \lceil\log_2 N\rceil$ bits per weight.

\subsubsection{Pruning and Secure Aggregation}

To enable linear decompression with random pruning, all clients will share a common pruning mask for each round.
This can be communicated compactly before each round as a seed for a pseudo-random function.
%the server broadcasts one common pruning mask seed before each round.
This pruning mask seed is different from the \SecAgg mask seed introduced in Section~\ref{subsec:secagg} and has a distinct role.
Each client uses the pruning seed to reconstruct a pruning mask, prunes their model update $g_i$, and only needs to encrypt and transmit the unpruned parameters.
The trade-off here is that some parameters are completely unobserved in a given round, as opposed to traditional pruning.
%we obtain no signal about some parameters in a given round, in contrast to traditional pruning.
\SecAgg operates as usual and the server receives the sum of the tensor of unpruned parameters computed by participating clients in the round, which it can expand using the mask seed.
We denote the pruning operator as $\phi$ applied to the original model update $g_i$, and the decompression operator as $d$ applied to a compressed tensor $\phi(g_i)$. Decompression is an expansion operation equivalent to multiplication with a sparse permutation matrix $P_i$ whose entries are dependent on the $i$'th client's mask seed.
Crucially, when all clients share the same mask seed within each round, we have $P_i = P$ for all $i$ and linearity of decompression is maintained:
\begin{equation*} \textstyle
    d \left(\sum_i \phi(g_i) \right) = P \left( \sum_i \phi(g_i) \right) = \sum_i P_i\phi(g_i) = \sum_i d(\phi(g_i)) \approx \sum_i g_i.
\end{equation*}
%
\modif{\para{Privacy, Security and Bandwidth.}} Since the mask is random, no information leaks from the pruning mask. The downlink overhead (the server broadcasts one integer mask seed) is negligible. The upload bandwidth is simply the size of the sparse client model updates. \modif{Finally, there is no loss in security since each client uses standard \SecAgg mechanism on the non-pruned entries.}

\subsubsection{Product Quantization and Secure Indexing}

\begin{algorithm}[t]
\caption{Secure Indexing (\SecInd)}
\label{alg:sec_indexing}
\begin{algorithmic}[1]

% \Procedure{On client}{C}      \Comment{Client-side logic}
%     \State Receive common codebook $C$
%     \State Compute assignment matrix $A^i$  \Comment{$i$ is the client index}
%     \State Encrypt assigment matrix \Comment{For instance with additive masking}
% \EndProcedure

\Procedure{SecureIndexing}{C}      \Comment{This happens inside the TEE}
    \State Receive common codebook $C$ from server \Comment{$C$ is periodically updated by the server}
    \State Initialize histograms $H_{m,n}$ to $0$ \Comment{Each histogram for block $(m, n)$ has size $k$}
    \For{each client $i$}
    \State Receive and decrypt assignment matrix $A^i$ %\Comment{For instance with additive masking}
        \For{each block index $(m, n)$}
            \State $r \leftarrow A^i_{m, n}$ \Comment{Recover assignment of client $i$ for block $(m, m)$}
            \State $H_{m, n}[r] \leftarrow H_{m, n}[r] + 1$ \Comment{Update global count for codeword index $r$}
        \EndFor
    \EndFor
    \State Send back histograms $H_{m, n}$ to the server
\EndProcedure
\end{algorithmic}
\end{algorithm}



% \ashkan{question on notatin: sometimes we have () around A, sometimes we do not, e.g. in the equation below. Sometimes the superscript $i$ is outside () and sometimes inside. Do these have different meaning?}
% We first express product quantization in terms of a linear decompression operator.
%We describe the novel Secure Indexing protocol


We next describe the Secure Indexing (\SecInd) primitive, and discuss how to instantiate it.
Recall that with PQ, each layer has its own codebook $C$ as explained in Section~\ref{sec:method}.
Let us fix one particular layer compressed with codebook $C$, containing $k$ codewords.
We assume that $C$ is common to all clients participating in the round.
Consider the assignment matrix of a given layer $(A^i)_{m,n}$ for client~$i$.
From these, we seek to build the \emph{assignment histograms} $H_{m,n} \in \mathbb R^k$ that satisfy
%\begin{equation*}
$H_{m,n}[r] = \sum_i \mathbf 1\left(A^i_{m,n} = r\right),$
%\end{equation*}
where the indicator function $\mathbf 1$ satisfies $\mathbf 1\left(A^i_{m,n} = r\right) = 1$ if $A^i_{m,n} = r$ and $0$ otherwise.
A \emph{Secure Indexing} primitive will produce~$H_{m,n}$ while ensuring that no other information about client assignments or partial aggregations is revealed.
The server receives assignment histograms from \SecInd and is able to recover the aggregated update for each block indexed by $(m, n)$ as
%\begin{equation*}
$
        \sum_r  H_{m,n}[r] \cdot C[r].
$
%\end{equation*}
%
We describe how \SecInd can be implemented with a TEE in Algorithm~\ref{alg:sec_indexing}.
Each client encrypts the assignment matrix, for instance with additive masking as described in Section~\ref{subsec:secagg}, and sends it to the TEE via the server.
Hence, the server does not have access to the plaintexts client-specific assignments.
TEE decrypts each assignment matrix and for each block indexed by $(m, n)$ produces the assignment histogram.
%\ilya{sentence fragment. I don't know where it is heading.}
% where
% \begin{equation*}
%     \mathbf 1\left(A^i_{m,n} = r\right) =
%     \begin{cases}
%         1 & \text{if } A^i_{m,n} = r  \\
%         0  & \text{otherwise}.
%     \end{cases}
% \end{equation*}
%\ilya{Delete:}\st{Since the enclave decrypts the individual assignment matrices, the proposed method is orthogonal to the encryption process.}
Compared to \SecAgg, where the TEE receives an encrypted seed per client (a few bytes per client) and sends back the sum of the masks $m_i$ (same size as the considered model), \SecInd receives the (masked) assignment matrices and sends back histograms for each round.
%\SecInd implementation feasibility is briefly discussed in Appendix~\ref{appendix:secind}.
\SecInd can be implemented in other models, offering different trust paradigms,
such as the multi-party computation setting (using two or more servers to operate on shares of the input).
Encoding inputs as shares of one-hot vectors would lose the advantages of compression.
Instead, each client can send evaluations of \emph{distributed point functions} to encode each assignment~\cite{boyle16}.
These are represented compactly, but may require longer codewords to overcome the overheads.

%An advantage of working with multiple servers is that we can compute the final output (\ie the update) without exposing any intermediate representation in terms of the histogram of codewords.
%That is, we could obtain shares of the histogram in the MPC model, and then use these in conjunction with the (public) codebook to build shares of the update, before finally combining these to reveal the update.
%This could also be combined with the introduction of DP noise to ensure privacy as well as security.
% expands it to $(\widetilde A^i)_{m,n,r}$ by replacing each assignment index by a one-hot vector of size $k$:
% \[  \widetilde A^i_{m,n,r} =
%     \begin{cases}
%         1 & \text{if } A^i_{m,n} = r  \\
%         0  & \text{otherwise}.
%     \end{cases}
% \]
% Let us assume further that the codebook $C$ is common to all clients participating in the round.
% % $C$ is computed by the server
% % %on a mock client
% % using public data or from the aggregated (public) model update.
% Then, the server is able to recover the aggregated update for each block indexed by $(m, n)$:
% \[ \ashkan{?=}\sum_r \left(\sum_{i} \widetilde A^i_{m,n,r} \right)\cdot C_r
% \]
% However, in practice we do not wish to instantiate the sparse matrix $(\widetilde A^i)_{m,n,r}$ due to its size.
% Instead, we propose Secure Indexing\karthik{where exactly do we propose this?}, a variant of SecAgg that receives the (non-expanded) assignment matrices $(A)^i_{m,n}$ and sends the aggregate back to the server in a compact form.



\modif{\para{Privacy, Security and Bandwidth.}}
Codebooks are computed from public data while individual assignments are never revealed to the server.
The downlink overhead of sending the codebooks is negligible as demonstrated in Section~\ref{sec:experiments}.
The upload bandwidth in the TEE implementation is the assignment size, represented in $k$ bits (the number of codewords).
For instance, with a block size $d=8$ and $k=32$ codewords, assignment storage costs are 5 bits per 8 weights, which converts to 0.625 bits per weight.
The tradeoff compared to non-secure PQ is the restriction to a global codebook for all clients (instead of one tailored to each client), and the need to instantiate \SecInd instead of \SecAgg. \modif{Since the assignments are encrypted before being sent to the TEE, there is no loss in security. Here, any encryption mechanism (not necessarily relying on additive masking) would work.}
