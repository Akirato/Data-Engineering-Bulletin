%%%%%%%% mlsys 2022 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
\PassOptionsToPackage{round}{natbib}
% before loading neurips_2022


% ready for submission
\usepackage{neurips_2022}

\usepackage{enumitem}
% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2022}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2022}


% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{bm}
\usepackage{amsmath,amssymb}
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{soul}
\usepackage{xspace}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{ulem}

\DeclareMathOperator{\round}{round}
\DeclareMathOperator{\clamp}{clamp}

\newcommand{\R}{\mathbf R}
\newcommand{\Z}{\mathbf Z}
\newcommand{\cin}{C_{\text{in}}}
\newcommand{\cout}{C_{\text{out}}}
\newcommand{\wb}{\mathbf W}
\newcommand{\xb}{\mathbf x}
\newcommand{\yb}{\mathbf y}
\newcommand{\calA}{\mathcal{A}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\calZ}{\mathcaL{Z}}
\newcommand{\SecInd}{{\sc SecInd}\xspace}
\newcommand{\SecAgg}{{\sc SecAgg}\xspace}
\newcommand{\FedAvg}{{\sc FedAvg}\xspace}
\newcommand{\SGD}{{\sc SGD}\xspace}

\def\ie{\textit{i.e.,}\@\xspace}
\def\eg{\textit{e.g.,}\@\xspace}

\newcommand{\pierre}[1]{{\color{purple}Pierre: #1}}
\newcommand{\sayan}[1]{{\color{red}Sayan: #1}}
\newcommand{\karthik}[1]{{\color{blue}Karthik: #1}}
\newcommand{\graham}[1]{{\color{green}Graham: #1}}
\newcommand{\ilya}[1]{{\color{red}Ilya: #1}}
\newcommand{\ashkan}[1]{{\color{blue}Ashkan: #1}}
\newcommand{\dzmitry}[1]{{\color{purple}Dzmitry: #1}}
\newcommand{\modif}[1]{{\color{black}#1}}

\title{Reconciling Security and Communication Efficiency in Federated Learning}
% Communication Efficient and Secure Federated Learning
% Unifying Secure Federated Learning and Communication Efficiency
% The Missing Bit for Practical Efficient Communication in Federated Learning
% Enabling Federated Learning with Communication Efficiency


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{
Karthik Prasad\thanks{Equal contribution. Correspondence to \texttt{pstock@fb.com}.} $^{~\dagger}$~~ Sayan Ghosh\footnotemark[1]$^{~~\dagger}$~~ Graham Cormode$^\dagger$~~ \\ \textbf{Ilya Mironov$^\dagger$~~ Ashkan Yousefpour$^\dagger$~~ Pierre Stock$^\dagger$} \\ $^\dagger$Meta AI
%   David S.~Hippocampus\thanks{Use footnote for providing further information
%     about author (webpage, alternative address)---\emph{not} for acknowledging
%     funding agencies.} \\
%   Department of Computer Science\\
%   Cranberry-Lemon University\\
%   Pittsburgh, PA 15213 \\
%   \texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}

\maketitle 

\begin{abstract}

Cross-device Federated Learning is an increasingly popular machine learning setting to train a model by leveraging 
a large population of client devices with high privacy and security guarantees. 
However, 
communication efficiency remains a major bottleneck when scaling federated learning to production environments, particularly due to bandwidth constraints during uplink communication.
In this paper, we formalize and address the problem of compressing client-to-server model updates
under the Secure Aggregation primitive, a core component of Federated Learning pipelines that allows the server to aggregate the client updates without accessing them individually. 
In particular, we adapt standard scalar quantization and pruning methods
to Secure Aggregation and propose Secure Indexing, a variant of Secure Aggregation that supports quantization for extreme compression.
We establish state-of-the-art results on LEAF benchmarks in a secure Federated Learning setup with up to 40$\times$ compression in uplink communication 
with no meaningful loss in utility compared to uncompressed baselines.
% compared to a.
% \karthik{\st{with less than one bit per weight on the LEAF benchmark and no significant loss in utility.}} 
% \karthik{I have edited the abstract a bit, please check history to review my changes}
\end{abstract}

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \mlsysEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\input{introduction}

\input{related}

\input{background}

\input{method}

\input{experiments}

\input{discussion}

\section{Conclusion}
In this paper, we reconcile efficiency and security for uplink communication in Federated Learning. We propose to adapt existing compression mechanisms such as scalar quantization and pruning to the secure aggregation protocol by imposing a linearity constraint on the decompression operator. Our experiments demonstrate that we can adapt both quantization and pruning mechanisms to obtain a high degree of uplink compression with minimal degradation in performance and higher security guarantees. For achieving the highest rates of compression, we introduce \SecInd, a variant of \SecAgg well-suited for TEE-based implementation that supports product quantization while maintaining a high security bar. 
We plan to extend our work to other federated learning scenarios, such as asynchronous FL, and further investigate the interaction of compression and privacy.
% and global and local differential privacy, 
% and investigate strategies to select optimal compression parameters 
% (quantization scales, centroids and pruning masks) 
% for better performance in these scenarios.

% % This 
% % - Generic and less data-dependent codebook
% % - Model compression 
% % - Compression and DP
% % - Error correction

% % \section*{Acknowledgements}


\bibliography{bibliography}
\bibliographystyle{plainnat}
% \pagebreak
\graham{\sout{Checklist}}
%\input{checklist}}


% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % SUPPLEMENTAL CONTENT AS APPENDIX AFTER REFERENCES
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\appendix
\input{appendix}
% %


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018. It was modified from a version from Dan Roy in
% 2017, which was based on a version from Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
