\newcommand{\para}[1]{\noindent \textbf{#1}}

\section{Related Work}
\label{sec:related}

Communication is identified as a primary efficiency bottleneck in FL, especially in the cross-device FL setting~\cite{kairouz2019advances}.
This has led to significant interest in reducing FL's communication requirements. In what follows, we refer to a local model update in distributed training as a \emph{gradient}, including updates from multiple local training steps.

\para{Efficient Distributed Optimization.} There is a large body of literature on reducing the communication cost for distributed training. \cite{seide2014bit} proposes quantizing gradients to one bit while carrying the quantization error forward across mini-batches with error feedback. Similarly, \cite{wen2017terngrad} proposes layer-wise ternary gradients and \cite{bernstein2018signsgd} suggests using only the sign of the gradients. Gradient sparsity is another related area that is extensively studied \cite{wangni2017gradient,aji2017sparse,lin2017deep,renggli2018sparcml,parcollet2022zerofl}.
For instance, \cite{chen2017adacomp} and \cite{han2020adaptive} explore adapting the degree of sparsity to the distribution of local client data. Another method, QSGD, tunes the quantization level to trade possibly higher variance gradients for reduced communication bandwidth~\cite{alistarh2016qsgd}. Researchers also studied structured and sketched model updates \cite{konen2016federated}.
For example, \cite{wang2018atomo} proposes expressing gradients as a linear combination of basis vectors common to all workers and \cite{wang2022fedlite} propose to cluster the gradients and to implement error correction on the client side. Besides gradient compression, other methods such as~\cite{vepakomma2018split,hu2019dynamic} reduce the communication cost by partitioning the model such that each client learns a portion of it, while \cite{he2020group} proposes training small models and periodically distilling them to a larger central model. However, as detailed in Section~\ref{sec:background} and below, most of the proposed methods are not readily compatible with \SecAgg and cannot be used in secure FL.

\para{Bi-directional Compression.} In addition to uplink gradient compression, a line of work also focuses on downlink model compression. In a non-distributed setup, \cite{zhou2016dorefanet, courbariaux2015binaryconnect} demonstrates that it is possible to meaningfully train with low bit-width models and gradients. In FL, \cite{jiang2019model} proposes adapting the model size to the device to reduce both communication and computation overhead. Since the local models are perturbed due to compression, researchers propose adapting the optimization algorithm for better convergence \cite{liu2019double,sattler2019robust,tang2019doublesqueeze,zheng2019communicationefficient,amiri2020federated,philippenko2021preserved}.
Finally, pre-conditioning models during FL training can allow for quantized on-device inference, as demonstrated for non-distributed training by \cite{gupta2015deep, krishnamoorthi2018quantizing}. As stated in Section~\ref{sec:intro}, we do not focus on downlink model compression since uplink bandwidth is the main communication bottleneck and since \SecAgg only involves uplink communication.

\para{Aggregation in the Compressed Domain.} In the distributed setting, \cite{yu2018gradiveq} propose to leverage both gradient compression and parallel aggregation by performing the \emph{ring all-reduce} operation in the compressed domain and decompressing the aggregate. To do so, the authors exploit temporal correlations of the gradients to design a linear compression operator.
% Hence, the server receives the individual compressed gradients and is able to recover the sum of the non-compressed gradients:
% \[\sum_i q(g_i) = q\left(\sum g_i\right).\]
Another method, PowerSGD~\cite{vogels2019powersgd}, leverages a fast low-rank gradient compressor. However, both aforementioned methods are not evaluated in the FL setup and do not mention \SecAgg.
Indeed, the proposed methods focus on decentralized communication between the workers by leveraging the all-reduce operation.
Moreover, PowerSGD uses (stateful) error feedback on all distributed nodes, which is not readily adaptable to cross-device FL when clients generally participate in a few (not necessarily consecutive) rounds.
% not amenable to FL with SecAgg since the all-reduce operations requires communication between all distributed nodes and not solely through a central server.
Finally, \cite{rothchild2020fetchsgd} proposes FetchSGD, a compression method using sketching, which is compatible with \SecAgg.

% \ilya{Should we say that our methods are also variants of sketching?}
% \ilya{delete?}\st{In this paper, we focus on adapting standard compression methods to SecAgg (namely, scalar quantization and pruning) and propose Secure Indexing, a variant of SecAgg that is provably secure, to obtain extremely small model updates with product quantization.}




