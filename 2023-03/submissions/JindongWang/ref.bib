@book{Abiteboul1995Foundationsdatabases,
  title = {Foundations of Databases},
  author = {Abiteboul, S. and Hull, Richard and Vianu, Victor},
  year = {1995},
  publisher = {{Addison-Wesley}},
  address = {{Reading, Mass}},
  isbn = {978-0-201-53771-0},
  langid = {english},
  lccn = {QA76.9.D3 A26 1995},
  keywords = {Database management},
  annotation = {00000}
}

@inproceedings{Alivanistos2022QueryEmbedding,
  title = {Query {{Embedding}} on {{Hyper-Relational Knowledge Graphs}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Alivanistos, Dimitrios and Berrendorf, Max and Cochez, Michael and Galkin, Mikhail},
  year = {2022},
  month = mar,
  abstract = {Multi-hop logical reasoning is an established problem in the field of representation learning on knowledge graphs (KGs). It subsumes both one-hop link prediction as well as other more complex types of logical queries. Existing algorithms operate only on classical, triple-based graphs, whereas modern KGs often employ a hyper-relational modeling paradigm. In this paradigm, typed edges may have several key-value pairs known as qualifiers that provide fine-grained context for facts. In queries, this context modifies the meaning of relations, and usually reduces the answer set. Hyper-relational queries are often observed in real-world KG applications, and existing approaches for approximate query answering cannot make use of qualifier pairs. In this work, we bridge this gap and extend the multi-hop reasoning problem to hyper-relational KGs allowing to tackle this new type of complex queries. Building upon recent advancements in Graph Neural Networks and query embedding techniques, we study how to embed and answer hyper-relational conjunctive queries. Besides that, we propose a method to answer such queries and demonstrate in our experiments that qualifiers improve query answering on a diverse set of query patterns.},
  langid = {english}
}

@inproceedings{Amayuelas2022NeuralMethods,
  ids = {amayuelasNeuralMethodsLogical2022},
  title = {Neural {{Methods}} for {{Logical Reasoning}} over {{Knowledge Graphs}}},
  booktitle = {The {{Tenth International Conference}} on {{Learning Representations}}, {{ICLR}} 2022, {{Virtual Event}}, {{April}} 25-29, 2022},
  author = {Amayuelas, Alfonso and Zhang, Shuai and Rao, Susie Xi and Zhang, Ce},
  year = {2022},
  publisher = {{OpenReview.net}}
}

@inproceedings{Arakelyan2021ComplexQuery,
  title = {Complex {{Query Answering}} with {{Neural Link Predictors}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Arakelyan, Erik and Daza, Daniel and Minervini, Pasquale and Cochez, Michael},
  year = {2021},
  abstract = {Neural link predictors are immensely useful for identifying missing edges in large scale Knowledge Graphs. However, it is still not clear how to use these models for answering more complex queries that arise in a number of domains, such as queries using logical conjunctions (\$\textbackslash land\$), disjunctions (\$\textbackslash lor\$) and existential quantifiers (\$\textbackslash exists\$), while accounting for missing edges. In this work, we propose a framework for efficiently answering complex queries on incomplete Knowledge Graphs. We translate each query into an end-to-end differentiable objective, where the truth value of each atom is computed by a pre-trained neural link predictor. We then analyse two solutions to the optimisation problem, including gradient-based and combinatorial search. In our experiments, the proposed approach produces more accurate results than state-of-the-art methods --- black-box neural models trained on millions of generated queries --- without the need of training on a large and diverse set of complex queries. Using orders of magnitude less training data, we obtain relative improvements ranging from 8\% up to 40\% in Hits@3 across different knowledge graphs containing factual information. Finally, we demonstrate that it is possible to explain the outcome of our model in terms of the intermediate solutions identified for each of the complex query atoms. All our source code and datasets are available online, at https://github.com/uclnlp/cqd.},
  langid = {english}
}

@article{bommasani2021opportunities,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@inproceedings{Auer2007DBpediaNucleus,
  title = {{{DBpedia}}: {{A Nucleus}} for a {{Web}} of {{Open Data}}},
  shorttitle = {{{DBpedia}}},
  booktitle = {The {{Semantic Web}}},
  author = {Auer, S{\"o}ren and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
  editor = {Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and {Cudr{\'e}-Mauroux}, Philippe},
  year = {2007},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {722--735},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-76298-0_52},
  abstract = {DBpedia is a community effort to extract structured information from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data.},
  isbn = {978-3-540-76298-0},
  langid = {english},
  keywords = {Open Dataset,Relational Database Table,Sophisticated Query,SPARQL Endpoint,Triple Pattern}
}

@inproceedings{Bai2022Query2ParticlesKnowledge,
  ids = {baiQuery2ParticlesKnowledgeGraph2022a},
  title = {{{Query2Particles}}: {{Knowledge Graph Reasoning}} with {{Particle Embeddings}}},
  shorttitle = {{{Query2Particles}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{NAACL}} 2022},
  author = {Bai, Jiaxin and Wang, Zihao and Zhang, Hongming and Song, Yangqiu},
  year = {2022},
  month = jul,
  pages = {2703--2714},
  publisher = {{Association for Computational Linguistics}},
  address = {{Seattle, United States}},
  doi = {10.18653/v1/2022.findings-naacl.207},
  abstract = {Answering complex logical queries on incomplete knowledge graphs (KGs) with missing edges is a fundamental and important task for knowledge graph reasoning. The query embedding method is proposed to answer these queries by jointly encoding queries and entities to the same embedding space. Then the answer entities are selected according to the similarities between the entity embeddings and the query embedding. As the answers to a complex query are obtained from a combination of logical operations over sub-queries, the embeddings of the answer entities may not always follow a uni-modal distribution in the embedding space. Thus, it is challenging to simultaneously retrieve a set of diverse answers from the embedding space using a single and concentrated query representation such as a vector or a hyper-rectangle. To better cope with queries with diversified answers, we propose Query2Particles (Q2P), a complex KG query answering method. Q2P encodes each query into multiple vectors, named particle embeddings. By doing so, the candidate answers can be retrieved from different areas over the embedding space using the maximal similarities between the entity embeddings and any of the particle embeddings. Meanwhile, the corresponding neural logic operations are defined to support its reasoning over arbitrary first-order logic queries. The experiments show that Query2Particles achieves state-of-the-art performance on the complex query answering tasks on FB15k, FB15K-237, and NELL knowledge graphs.}
}

@inproceedings{Bollacker2008Freebasecollaboratively,
  title = {Freebase: A Collaboratively Created Graph Database for Structuring Human Knowledge},
  shorttitle = {Freebase},
  booktitle = {Proceedings of the {{ACM SIGMOD International Conference}} on {{Management}} of {{Data}}, {{SIGMOD}} 2008, {{Vancouver}}, {{BC}}, {{Canada}}, {{June}} 10-12, 2008},
  author = {Bollacker, Kurt D. and Evans, Colin and Paritosh, Praveen K. and Sturge, Tim and Taylor, Jamie},
  editor = {Wang, Jason Tsong-Li},
  year = {2008},
  pages = {1247--1250},
  publisher = {{ACM}},
  doi = {10.1145/1376616.1376746}
}

@inproceedings{Bordes2013TranslatingEmbeddings,
  title = {Translating {{Embeddings}} for {{Modeling Multi-relational Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bordes, Antoine and Usunier, Nicolas and {Garcia-Duran}, Alberto and Weston, Jason and Yakhnenko, Oksana},
  year = {2013},
  volume = {26},
  publisher = {{Curran Associates, Inc.}},
  annotation = {04160}
}

@article{Bosselut2021DynamicNeuroSymbolic,
  title = {Dynamic {{Neuro-Symbolic Knowledge Graph Construction}} for {{Zero-shot Commonsense Question Answering}}},
  author = {Bosselut, Antoine and Le Bras, Ronan and Choi, Yejin},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {6},
  pages = {4923--4931},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v35i6.16625},
  abstract = {Understanding narratives requires reasoning about implicit world knowledge related to the causes, effects, and states of situations described in text. At the core of this challenge is how to access contextually relevant knowledge on demand and reason over it.},
  langid = {english}
}

@inproceedings{Carlson2010ArchitectureNeverEnding,
  title = {Toward an {{Architecture}} for {{Never-Ending Language Learning}}},
  booktitle = {Proceedings of the {{Twenty-Fourth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2010, {{Atlanta}}, {{Georgia}}, {{USA}}, {{July}} 11-15, 2010},
  author = {Carlson, Andrew and Betteridge, Justin and Kisiel, Bryan and Settles, Burr and Jr, Estevam R. Hruschka and Mitchell, Tom M.},
  editor = {Fox, Maria and Poole, David},
  year = {2010},
  publisher = {{AAAI Press}}
}

@inproceedings{Chen2022FuzzyLogic,
  title = {Fuzzy {{Logic Based Logical Query Answering}} on {{Knowledge Graphs}}},
  booktitle = {Thirty-{{Sixth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2022, {{Thirty-Fourth Conference}} on {{Innovative Applications}} of {{Artificial Intelligence}}, {{IAAI}} 2022, {{The Twelveth Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}, {{EAAI}} 2022 {{Virtual Event}}, {{February}} 22 - {{March}} 1, 2022},
  author = {Chen, Xuelu and Hu, Ziniu and Sun, Yizhou},
  year = {2022},
  pages = {3939--3948},
  publisher = {{AAAI Press}}
}

@inproceedings{Choudhary2021ProbabilisticEntity,
  ids = {choudharyProbabilisticEntityRepresentation2021a},
  title = {Probabilistic {{Entity Representation Model}} for {{Reasoning}} over {{Knowledge Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Choudhary, Nurendra and Rao, Nikhil and Katariya, Sumeet and Subbian, Karthik and Reddy, Chandan},
  year = {2021},
  volume = {34},
  pages = {23440--23451},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Logical reasoning over Knowledge Graphs (KGs) is a fundamental technique that can provide an efficient querying mechanism over large and incomplete databases. Current approaches employ spatial geometries such as boxes to learn query representations that encompass the answer entities and model the logical operations of projection and intersection. However, their geometry is restrictive and leads to non-smooth strict boundaries, which further results in ambiguous answer entities. Furthermore, previous works propose transformation tricks to handle unions which results in non-closure and, thus, cannot be chained in a stream. In this paper, we propose a Probabilistic Entity Representation Model (PERM) to encode entities as a Multivariate Gaussian density with mean and covariance parameters to capture its semantic position and smooth decision boundary, respectively. Additionally, we also define the closed logical operations of projection, intersection, and union that can be aggregated using an end-to-end objective function. On the logical query reasoning problem, we demonstrate that the proposed PERM significantly outperforms the state-of-the-art methods on various public benchmark KG datasets on standard evaluation metrics. We also evaluate PERM's competence on a COVID-19 drug-repurposing case study and show that our proposed work is able to recommend drugs with substantially better F1 than current methods. Finally, we demonstrate the working of our PERM's query answering process through a low-dimensional visualization of the Gaussian representations.}
}

@inproceedings{Choudhary2021SelfSupervisedHyperboloidc,
  title = {Self-{{Supervised Hyperboloid Representations}} from {{Logical Queries}} over {{Knowledge Graphs}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Choudhary, Nurendra and Rao, Nikhil and Katariya, Sumeet and Subbian, Karthik and Reddy, Chandan K.},
  year = {2021},
  month = jun,
  series = {{{WWW}} '21},
  pages = {1373--1384},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3442381.3449974},
  abstract = {Knowledge Graphs (KGs) are ubiquitous structures for information storage in several real-world applications such as web search, e-commerce, social networks, and biology. Querying KGs remains a foundational and challenging problem due to their size and complexity. Promising approaches to tackle this problem include embedding the KG units (e.g., entities and relations) in a Euclidean space such that the query embedding contains the information relevant to its results. These approaches, however, fail to capture the hierarchical nature and semantic information of the entities present in the graph. Additionally, most of these approaches only utilize multi-hop queries (that can be modeled by simple translation operations) to learn embeddings and ignore more complex operations such as intersection, and union of simpler queries. To tackle such complex operations, in this paper, we formulate KG representation learning as a self-supervised logical query reasoning problem that utilizes translation, intersection and union queries over KGs. We propose Hyperboloid Embeddings (HypE), a novel self-supervised dynamic reasoning framework, that utilizes positive first-order existential queries on a KG to learn representations of its entities and relations as hyperboloids in a Poincar\'e ball. HypE models the positive first-order queries as geometrical translation, intersection, and union. For the problem of KG reasoning in real-world datasets, the proposed HypE model significantly outperforms the state-of-the art results. We also apply HypE to an anomaly detection task on a popular e-commerce website product taxonomy as well as hierarchically organized web articles and demonstrate significant performance improvements compared to existing baseline methods. Finally, we also visualize the learned HypE embeddings in a Poincar\'e ball to clearly interpret and comprehend the representation space.},
  isbn = {978-1-4503-8312-7},
  keywords = {hyperbolic space,knowledge graphs,reasoning queries,Representation learning}
}

@article{Cohen2020TensorLogProbabilistic,
  title = {{{TensorLog}}: {{A Probabilistic Database Implemented Using Deep-Learning Infrastructure}}},
  shorttitle = {{{TensorLog}}},
  author = {Cohen, William and Yang, Fan and Mazaitis, Kathryn Rivard},
  year = {2020},
  month = feb,
  journal = {Journal of Artificial Intelligence Research},
  volume = {67},
  pages = {285--325},
  issn = {1076-9757},
  doi = {10.1613/jair.1.11944},
  abstract = {We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.},
  copyright = {Copyright (c)},
  langid = {english}
}

@misc{Daza2020MessagePassing,
  title = {Message {{Passing Query Embedding}}},
  author = {Daza, Daniel and Cochez, Michael},
  year = {2020},
  month = jun,
  number = {arXiv:2002.02406},
  eprint = {2002.02406},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2002.02406},
  abstract = {Recent works on representation learning for Knowledge Graphs have moved beyond the problem of link prediction, to answering queries of an arbitrary structure. Existing methods are based on ad-hoc mechanisms that require training with a diverse set of query structures. We propose a more general architecture that employs a graph neural network to encode a graph representation of the query, where nodes correspond to entities and variables. The generality of our method allows it to encode a more diverse set of query types in comparison to previous work. Our method shows competitive performance against previous models for complex queries, and in contrast with these models, it can answer complex queries when trained for link prediction only. We show that the model learns entity embeddings that capture the notion of entity type without explicit supervision.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{Galkin2020MessagePassing,
  title = {Message {{Passing}} for {{Hyper-Relational Knowledge Graphs}}},
  booktitle = {Proceedings of the 2020 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Galkin, Mikhail and Trivedi, Priyansh and Maheshwari, Gaurav and Usbeck, Ricardo and Lehmann, Jens},
  year = {2020},
  month = nov,
  pages = {7346--7359},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.596},
  abstract = {Hyper-relational knowledge graphs (KGs) (e.g., Wikidata) enable associating additional key-value pairs along with the main triple to disambiguate, or restrict the validity of a fact. In this work, we propose a message passing based graph encoder - StarE capable of modeling such hyper-relational KGs. Unlike existing approaches, StarE can encode an arbitrary number of additional information (qualifiers) along with the main triple while keeping the semantic roles of qualifiers and triples intact. We also demonstrate that existing benchmarks for evaluating link prediction (LP) performance on hyper-relational KGs suffer from fundamental flaws and thus develop a new Wikidata-based dataset - WD50K. Our experiments demonstrate that StarE based LP model outperforms existing approaches across multiple benchmarks. We also confirm that leveraging qualifiers is vital for link prediction with gains up to 25 MRR points compared to triple-based representations.}
}

@inproceedings{Galkin2022InductiveLogical,
  title = {Inductive {{Logical Query Answering}} in {{Knowledge Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33: {{Annual Conference}} on {{Neural Information Processing Systems}} 2022 {{NeurIPS}} 2022, {{November}} 27- {{December}} 9, 2022, {{New Orleans}}},
  author = {Galkin, Mikhail and Zhu, Zhaocheng and Ren, Hongyu and Tang, Jian},
  year = {2022},
  pages = {25},
  abstract = {Formulating and answering logical queries is a standard communication interface for knowledge graphs (KGs). Alleviating the notorious incompleteness of realworld KGs, neural methods achieved impressive results in link prediction and complex query answering tasks by learning representations of entities, relations, and queries. Still, most existing query answering methods rely on transductive entity embeddings and cannot generalize to KGs containing new entities without retraining the entity embeddings. In this work, we study the inductive query answering task where inference is performed on a graph containing new entities with queries over both seen and unseen entities. To this end, we devise two mechanisms leveraging inductive node and relational structure representations powered by graph neural networks (GNNs). Experimentally, we show that inductive models are able to perform logical reasoning at inference time over unseen nodes generalizing to graphs up to 500\% larger than training ones. Exploring the efficiency\textendash effectiveness trade-off, we find the inductive relational structure representation method generally achieves higher performance, while the inductive node representation method is able to answer complex queries in the inference-only regime without any training on queries and scales to graphs of millions of nodes. Code is available at https://github.com/DeepGraphLearning/InductiveQE.},
  langid = {english}
}

@book{Hajek1998MetamathematicsFuzzy,
  title = {Metamathematics of {{Fuzzy Logic}}},
  author = {H{\'a}jek, Petr},
  editor = {W{\'o}jcicki, Ryszard and H{\'a}jek, Petr and Makinson, David and Mundici, Daniele and Segerberg, Krister and Urquhart, Alasdair and Malinowski, Jacek},
  year = {1998},
  series = {Trends in {{Logic}}},
  volume = {4},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-5300-3},
  isbn = {978-1-4020-0370-7 978-94-011-5300-3},
  langid = {english}
}

@inproceedings{Hamilton2018EmbeddingLogical,
  title = {Embedding {{Logical Queries}} on {{Knowledge Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31: {{Annual Conference}} on {{Neural Information Processing Systems}} 2018, {{NeurIPS}} 2018, {{December}} 3-8, 2018, {{Montr\'eal}}, {{Canada}}},
  author = {Hamilton, William L. and Bajaj, Payal and Zitnik, Marinka and Jurafsky, Dan and Leskovec, Jure},
  editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and {Cesa-Bianchi}, Nicol{\`o} and Garnett, Roman},
  year = {2018},
  pages = {2030--2041}
}

@inproceedings{Hu2020OpenGraph,
  title = {Open {{Graph Benchmark}}: {{Datasets}} for {{Machine Learning}} on {{Graphs}}},
  shorttitle = {Open {{Graph Benchmark}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hu, Weihua and Fey, Matthias and Zitnik, Marinka and Dong, Yuxiao and Ren, Hongyu and Liu, Bowen and Catasta, Michele and Leskovec, Jure},
  year = {2020},
  volume = {33},
  pages = {22118--22133},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale, encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at https://ogb.stanford.edu .}
}

@inproceedings{Hu2022TypeawareEmbeddings,
  title = {Type-Aware {{Embeddings}} for {{Multi-Hop Reasoning}} over {{Knowledge Graphs}}},
  booktitle = {Proceedings of the {{Thirty-First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Hu, Zhiwei and Gutierrez Basulto, Victor and Xiang, Zhiliang and Li, Xiaoli and Li, Ru and Z. Pan, Jeff},
  year = {2022},
  month = jul,
  pages = {3078--3084},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Vienna, Austria}},
  doi = {10.24963/ijcai.2022/427},
  abstract = {Multi-hop reasoning over real-life knowledge graphs (KGs) is a highly challenging problem as traditional subgraph matching methods are not capable to deal with noise and missing information. To address this problem, it has been recently introduced a promising approach based on jointly embedding logical queries and KGs into a low-dimensional space to identify answer entities. However, existing proposals ignore critical semantic knowledge inherently available in KGs, such as type information. To leverage type information, we propose a novel TypE-aware Message Passing (TEMP) model, which enhances the entity and relation representations in queries, and simultaneously improves generalization, deductive and inductive reasoning. Remarkably, TEMP is a plug-and-play model that can be easily incorporated into existing embedding-based models to improve their performance. Extensive experiments on three real-world datasets demonstrate TEMP's effectiveness.},
  isbn = {978-1-956792-00-3},
  langid = {english}
}

@inproceedings{Huang2022LinELogical,
  ids = {huangLinELogicalQuery2022a,huangLinELogicalQuery2022b},
  title = {{{LinE}}: {{Logical Query Reasoning}} over {{Hierarchical Knowledge Graphs}}},
  shorttitle = {{{LinE}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Huang, Zijian and Chiang, Meng-Fen and Lee, Wang-Chien},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {615--625},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539338},
  abstract = {Logical reasoning over Knowledge Graphs (KGs) for first-order logic (FOL) queries performs the query inference over KGs with logical operators, including conjunction, disjunction, existential quantification and negation, to approximate true answers in embedding spaces. However, most existing work imposes strong distributional assumptions (e.g., Beta distribution) to represent entities and queries into presumed distributional shape, which limits their expressive power. Moreover, query embeddings are challenging due to the relational complexities in multi-relational KGs (e.g., symmetry, anti-symmetry and transitivity). To bridge the gap, we propose a logical query reasoning framework, Line Embedding (LinE), for FOL queries. To relax the distributional assumptions, we introduce the logic space transformation layer, which is a generic neural function that converts embeddings from probabilistic distribution space to LinE embeddings space. To tackle multi-relational and logical complexities, we formulate neural relation-specific projections and individual logical operators to truthfully ground LinE query embeddings on logical regularities and KG factoids. Lastly, to verify the LinE embedding quality, we generate a FOL query dataset from WordNet, which richly encompasses hierarchical relations. Extensive experiments show superior reasoning sensitivity of LinE on three benchmarks against strong baselines, particularly for multi-hop relational queries and negation-related queries.},
  isbn = {978-1-4503-9385-0},
  keywords = {knowledge representation learning,logical query reasoning}
}

@article{Ji2022Surveyknowledge,
  title = {A Survey on Knowledge Graphs: Representation, Acquisition, and Applications},
  shorttitle = {A Survey on Knowledge Graphs},
  author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
  year = {2022},
  month = feb,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {33},
  number = {2},
  pages = {494--514},
  publisher = {{IEEE, Institute of Electrical and Electronics Engineers}},
  issn = {2162-237X},
  doi = {10.1109/TNNLS.2021.3070843},
  langid = {english},
  pmid = {33900922}
}

@inproceedings{Jia2021ComplexTemporal,
  title = {Complex {{Temporal Question Answering}} on {{Knowledge Graphs}}},
  booktitle = {{{CIKM}} '21: {{The}} 30th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}, {{Virtual Event}}, {{Queensland}}, {{Australia}}, {{November}} 1 - 5, 2021},
  author = {Jia, Zhen and Pramanik, Soumajit and Roy, Rishiraj Saha and Weikum, Gerhard},
  editor = {Demartini, Gianluca and Zuccon, Guido and Culpepper, J. Shane and Huang, Zi and Tong, Hanghang},
  year = {2021},
  pages = {792--802},
  publisher = {{ACM}},
  doi = {10.1145/3459637.3482416}
}

@article{Katsaras1977Fuzzyvector,
  title = {Fuzzy Vector Spaces and Fuzzy Topological Vector Spaces},
  author = {Katsaras, A. K and Liu, D. B},
  year = {1977},
  month = mar,
  journal = {Journal of Mathematical Analysis and Applications},
  volume = {58},
  number = {1},
  pages = {135--146},
  issn = {0022-247X},
  doi = {10.1016/0022-247X(77)90233-5},
  langid = {english}
}

@article{Kotnis2021AnsweringComplex,
  title = {Answering {{Complex Queries}} in {{Knowledge Graphs}} with {{Bidirectional Sequence Encoders}}},
  author = {Kotnis, Bhushan and Lawrence, Carolin and Niepert, Mathias},
  year = {2021},
  month = may,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {6},
  pages = {4968--4977},
  issn = {2374-3468},
  copyright = {Copyright (c) 2021 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {Relational Learning}
}

@book{Kroenke2018Databaseprocessing,
  title = {Database Processing: Fundamentals, Design, and Implementation},
  shorttitle = {Database Processing},
  author = {Kroenke, David M. and Auer, David J. and Vandenberg, Scott L. and Yoder, Robert C.},
  year = {2018},
  edition = {15th edition, 40th anniversary edition},
  publisher = {{Pearson}},
  address = {{NY NY}},
  isbn = {978-0-13-480274-9},
  langid = {english},
  lccn = {QA76.9.D3 K7365 2018},
  keywords = {Database management}
}

@book{Libkin2004ElementsFinite,
  title = {Elements of {{Finite Model Theory}}},
  author = {Libkin, Leonid},
  year = {2004},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-07003-1},
  isbn = {978-3-642-05948-3 978-3-662-07003-1},
  langid = {english},
  annotation = {00000}
}

@inproceedings{Libkin2009OpenClosed,
  title = {Open and {{Closed World Assumptions}} in {{Data Exchange}}},
  booktitle = {Proceedings of the 22nd {{International Workshop}} on {{Description Logics}} ({{DL}} 2009), {{Oxford}}, {{UK}}, {{July}} 27-30, 2009},
  author = {Libkin, Leonid and Sirangelo, Cristina},
  editor = {Grau, Bernardo Cuenca and Horrocks, Ian and Motik, Boris and Sattler, Ulrike},
  year = {2009},
  series = {{{CEUR Workshop Proceedings}}},
  volume = {477},
  publisher = {{CEUR-WS.org}}
}

@article{Lin2019KagNetKnowledgeAware,
  title = {{{KagNet}}: {{Knowledge-Aware Graph Networks}} for {{Commonsense Reasoning}}},
  shorttitle = {{{KagNet}}},
  author = {Lin, Bill Yuchen and Chen, Xinyue and Chen, Jamin and Ren, Xiang},
  year = {2019},
  journal = {EMNLP/IJCNLP},
  doi = {10.18653/v1/D19-1282},
  abstract = {Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named KagNet, and finally scores answers with graph representations. Our model is based on graph convolutional networks and LSTMs, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using ConceptNet as the only external resource for Bert-based models, we achieved state-of-the-art performance on the CommonsenseQA, a large-scale dataset for commonsense reasoning.}
}

@inproceedings{Liu2021NeuralAnsweringLogical,
  ids = {liuNeuralAnsweringLogicalQueries2021a},
  title = {Neural-{{Answering Logical Queries}} on {{Knowledge Graphs}}},
  booktitle = {Proceedings of the 27th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Liu, Lihui and Du, Boxin and Ji, Heng and Zhai, ChengXiang and Tong, Hanghang},
  year = {2021},
  month = aug,
  series = {{{KDD}} '21},
  pages = {1087--1097},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3447548.3467375},
  abstract = {Logical queries constitute an important subset of questions posed in knowledge graph question answering systems. Yet, effectively answering logical queries on large knowledge graphs remains a highly challenging problem. Traditional subgraph matching based methods might suffer from the noise and incompleteness of the underlying knowledge graph, often with a prolonged online response time. Recently, an alternative type of method has emerged whose key idea is to embed knowledge graph entities and the query in an embedding space so that the embedding of answer entities is close to that of the query. Compared with subgraph matching based methods, it can better handle the noisy or missing information in knowledge graph, with a faster online response. Promising as it might be, several fundamental limitations still exist, including the linear transformation assumption for modeling relations and the inability to answer complex queries with multiple variable nodes. In this paper, we propose an embedding based method (NewLook) to address these limitations. Our proposed method offers three major advantages. First (Applicability), it supports four types of logical operations and can answer queries with multiple variable nodes. Second (Effectiveness), the proposed NewLook goes beyond the linear transformation assumption, and thus consistently outperforms the existing methods. Third (Efficiency), compared with subgraph matching based methods, NewLook is at least 3 times faster in answering the queries; compared with the existing embed-ding based methods, NewLook bears a comparable or even faster online response and offline training time.},
  isbn = {978-1-4503-8332-5},
  keywords = {knowledge graph embedding,knowledge graph question answering,logical query embedding}
}

@inproceedings{Liu2022JointKnowledge,
  ids = {liuJointKnowledgeGraph2022a},
  title = {Joint {{Knowledge Graph Completion}} and {{Question Answering}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Liu, Lihui and Du, Boxin and Xu, Jiejun and Xia, Yinglong and Tong, Hanghang},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {1098--1108},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539289},
  abstract = {Knowledge graph reasoning plays a pivotal role in many real-world applications, such as network alignment, computational fact-checking, recommendation, and many more. Among these applications, knowledge graph completion (KGC) and multi-hop question answering over knowledge graph (Multi-hop KGQA) are two representative reasoning tasks. In the vast majority of the existing works, the two tasks are considered separately with different models or algorithms. However, we envision that KGC and Multi-hop KGQA are closely related to each other. Therefore, the two tasks will benefit from each other if they are approached adequately. In this work, we propose a neural model named BiNet to jointly handle KGC and multi-hop KGQA, and formulate it as a multi-task learning problem. Specifically, our proposed model leverages a shared embedding space and an answer scoring module, which allows the two tasks to automatically share latent features and learn the interactions between natural language question decoder and answer scoring module. Compared to the existing methods, the proposed BiNet model addresses both multi-hop KGQA and KGC tasks simultaneously with superior performance. Experiment results show that BiNet outperforms state-of-the-art methods on a wide range of KGQA and KGC benchmark datasets.},
  isbn = {978-1-4503-9385-0},
  keywords = {knowledge graph completion,knowledge graph question answering,multi-task learning}
}

@inproceedings{Liu2022MaskReason,
  ids = {liuMaskReasonPreTraining2022a,liuMaskReasonPreTraining2022b},
  title = {Mask and {{Reason}}: {{Pre-Training Knowledge Graph Transformers}} for {{Complex Logical Queries}}},
  shorttitle = {Mask and {{Reason}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Liu, Xiao and Zhao, Shiyu and Su, Kai and Cen, Yukuo and Qiu, Jiezhong and Zhang, Mengdi and Wu, Wei and Dong, Yuxiao and Tang, Jie},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {1120--1130},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539472},
  abstract = {Knowledge graph (KG) embeddings have been a mainstream approach for reasoning over incomplete KGs. However, limited by their inherently shallow and static architectures, they can hardly deal with the rising focus on complex logical queries, which comprise logical operators, imputed edges, multiple source entities, and unknown intermediate entities. In this work, we present the Knowledge Graph Transformer (kgTransformer) with masked pre-training and fine-tuning strategies. We design a KG triple transformation method to enable Transformer to handle KGs, which is further strengthened by the Mixture-of-Experts (MoE) sparse activation. We then formulate the complex logical queries as masked prediction and introduce a two-stage masked pre-training strategy to improve transferability and generalizability.Extensive experiments on two benchmarks demonstrate that kgTransformer can consistently outperform both KG embedding-based baselines and advanced encoders on nine in-domain and out-of-domain reasoning tasks. Additionally, kgTransformer can reason with explainability via providing the full reasoning paths to interpret given answers.},
  isbn = {978-1-4503-9385-0},
  keywords = {graph neural networks,knowledge graph,pre-training}
}

@article{Luus2021LogicEmbeddings,
  title = {Logic {{Embeddings}} for {{Complex Query Answering}}},
  author = {Luus, Francois and Sen, Prithviraj and Kapanipathi, Pavan and Riegel, Ryan and Makondo, Ndivhuwo and Lebese, Thabang and Gray, Alexander},
  year = {2021},
  month = feb,
  journal = {arXiv:2103.00418 [cs]},
  eprint = {2103.00418},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Answering logical queries over incomplete knowledge bases is challenging because: 1) it calls for implicit link prediction, and 2) brute force answering of existential first-order logic queries is exponential in the number of existential variables. Recent work of query embeddings provides fast querying, but most approaches model set logic with closed regions, so lack negation. Query embeddings that do support negation use densities that suffer drawbacks: 1) only improvise logic, 2) use expensive distributions, and 3) poorly model answer uncertainty. In this paper, we propose Logic Embeddings, a new approach to embedding complex queries that uses Skolemisation to eliminate existential variables for efficient querying. It supports negation, but improves on density approaches: 1) integrates well-studied t-norm logic and directly evaluates satisfiability, 2) simplifies modeling with truth values, and 3) models uncertainty with truth bounds. Logic Embeddings are competitively fast and accurate in query answering over large, incomplete knowledge graphs, outperform on negation queries, and in particular, provide improved modeling of answer uncertainty as evidenced by a superior correlation between answer set size and embedding entropy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Databases,Computer Science - Logic in Computer Science,Computer Science - Machine Learning}
}

@inproceedings{Lv2020GraphBasedReasoning,
  title = {Graph-{{Based Reasoning}} over {{Heterogeneous External Knowledge}} for {{Commonsense Question Answering}}},
  booktitle = {The {{Thirty-Fourth AAAI Conference}} on {{Artificial Intelligence}}, {{AAAI}} 2020, {{The Thirty-Second Innovative Applications}} of {{Artificial Intelligence Conference}}, {{IAAI}} 2020, {{The Tenth AAAI Symposium}} on {{Educational Advances}} in {{Artificial Intelligence}}, {{EAAI}} 2020, {{New York}}, {{NY}}, {{USA}}, {{February}} 7-12, 2020},
  author = {Lv, Shangwen and Guo, Daya and Xu, Jingjing and Tang, Duyu and Duan, Nan and Gong, Ming and Shou, Linjun and Jiang, Daxin and Cao, Guihong and Hu, Songlin},
  year = {2020},
  pages = {8449--8456},
  publisher = {{AAAI Press}}
}

@inproceedings{Ma2022QueryNeighborAware,
  title = {Query and {{Neighbor-Aware Reasoning Based Multi-hop Question Answering}} over {{Knowledge Graph}}},
  booktitle = {Knowledge {{Science}}, {{Engineering}} and {{Management}} - 15th {{International Conference}}, {{KSEM}} 2022, {{Singapore}}, {{August}} 6-8, 2022, {{Proceedings}}, {{Part I}}},
  author = {Ma, Biao and Chen, Xiaoying and Xiong, Shengwu},
  editor = {Memmi, G{\'e}rard and Yang, Baijian and Kong, Linghe and Zhang, Tianwei and Qiu, Meikang},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13368},
  pages = {133--145},
  publisher = {{Springer}},
  doi = {10.1007/978-3-031-10983-6_11}
}

@book{Marker2002Modeltheory,
  title = {Model Theory: An Introduction},
  shorttitle = {Model Theory},
  author = {Marker, D.},
  year = {2002},
  series = {Graduate Texts in Mathematics},
  number = {217},
  publisher = {{Springer}},
  address = {{New York}},
  isbn = {978-0-387-98760-6},
  langid = {english},
  lccn = {QA9.7 .M367 2002},
  keywords = {Model theory},
  annotation = {00000}
}

@article{Miller1995WordNetlexical,
  title = {{{WordNet}}: A Lexical Database for {{English}}},
  shorttitle = {{{WordNet}}},
  author = {Miller, George A.},
  year = {1995},
  month = nov,
  journal = {Communications of the ACM},
  volume = {38},
  number = {11},
  pages = {39--41},
  issn = {0001-0782},
  doi = {10.1145/219717.219748},
  abstract = {Because meaningful sentences are composed of meaningful words, any system that hopes to process natural languages as people do must have information about words and their meanings. This information is traditionally provided through dictionaries, and machine-readable dictionaries are now widely available. But dictionary entries evolved for the convenience of human readers, not for machines. WordNet1 provides a more effective combination of traditional lexicographic information and modern computing. WordNet is an online lexical database designed for use under program control. English nouns, verbs, adjectives, and adverbs are organized into sets of synonyms, each representing a lexicalized concept. Semantic relations link the synonym sets [4].}
}

@inproceedings{PellissierTanon2016FreebaseWikidata,
  title = {From {{Freebase}} to {{Wikidata}}: {{The Great Migration}}},
  shorttitle = {From {{Freebase}} to {{Wikidata}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{World Wide Web}}},
  author = {Pellissier Tanon, Thomas and Vrande{\v c}i{\'c}, Denny and Schaffert, Sebastian and Steiner, Thomas and Pintscher, Lydia},
  year = {2016},
  month = apr,
  series = {{{WWW}} '16},
  pages = {1419--1428},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Republic and Canton of Geneva, CHE}},
  doi = {10.1145/2872427.2874809},
  abstract = {Collaborative knowledge bases that make their data freely available in a machine-readable form are central for the data strategy of many projects and organizations. The two major collaborative knowledge bases are Wikimedia's Wikidata and Google's Freebase. Due to the success of Wikidata, Google decided in 2014 to offer the content of Freebase to the Wikidata community. In this paper, we report on the ongoing transfer efforts and data mapping challenges, and provide an analysis of the effort so far. We describe the Primary Sources Tool, which aims to facilitate this and future data migrations. Throughout the migration, we have gained deep insights into both Wikidata and Freebase, and share and discuss detailed statistics on both knowledge bases.},
  isbn = {978-1-4503-4143-1},
  keywords = {crowdsourcing systems,freebase,semantic web,wikidata}
}

@inproceedings{Ren2020BetaEmbeddings,
  title = {Beta {{Embeddings}} for {{Multi-Hop Logical Reasoning}} in {{Knowledge Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 33: {{Annual Conference}} on {{Neural Information Processing Systems}} 2020, {{NeurIPS}} 2020, {{December}} 6-12, 2020, Virtual},
  author = {Ren, Hongyu and Leskovec, Jure},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  year = {2020}
}

@inproceedings{Ren2020Query2boxReasoning,
  title = {Query2box: {{Reasoning}} over {{Knowledge Graphs}} in {{Vector Space Using Box Embeddings}}},
  shorttitle = {Query2box},
  booktitle = {8th {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2020, {{Addis Ababa}}, {{Ethiopia}}, {{April}} 26-30, 2020},
  author = {Ren, Hongyu and Hu, Weihua and Leskovec, Jure},
  year = {2020},
  publisher = {{OpenReview.net}}
}

@inproceedings{Ren2021LEGOLatent,
  ids = {renLEGOLatentExecutionGuided2021a},
  title = {{{LEGO}}: {{Latent Execution-Guided Reasoning}} for {{Multi-Hop Question Answering}} on {{Knowledge Graphs}}},
  shorttitle = {{{LEGO}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Ren, Hongyu and Dai, Hanjun and Dai, Bo and Chen, Xinyun and Yasunaga, Michihiro and Sun, Haitian and Schuurmans, Dale and Leskovec, Jure and Zhou, Denny},
  year = {2021},
  month = jul,
  pages = {8959--8970},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {Answering complex natural language questions on knowledge graphs (KGQA) is a challenging task. It requires reasoning with the input natural language questions as well as a massive, incomplete heterogeneous KG. Prior methods obtain an abstract structured query graph/tree from the input question and traverse the KG for answers following the query tree. However, they inherently cannot deal with missing links in the KG. Here we present LEGO, a Latent Execution-Guided reasOning framework to handle this challenge in KGQA. LEGO works in an iterative way, which alternates between (1) a Query Synthesizer, which synthesizes a reasoning action and grows the query tree step-by-step, and (2) a Latent Space Executor that executes the reasoning action in the latent embedding space to combat against the missing information in KG. To learn the synthesizer without step-wise supervision, we design a generic latent execution guided bottom-up search procedure to find good execution traces efficiently in the vast query space. Experimental results on several KGQA benchmarks demonstrate the effectiveness of our framework compared with previous state of the art.},
  langid = {english}
}

@inproceedings{Ren2022SMOREKnowledgea,
  title = {{{SMORE}}: {{Knowledge Graph Completion}} and {{Multi-hop Reasoning}} in {{Massive Knowledge Graphs}}},
  shorttitle = {{{SMORE}}},
  booktitle = {Proceedings of the 28th {{ACM SIGKDD Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Ren, Hongyu and Dai, Hanjun and Dai, Bo and Chen, Xinyun and Zhou, Denny and Leskovec, Jure and Schuurmans, Dale},
  year = {2022},
  month = aug,
  series = {{{KDD}} '22},
  pages = {1472--1482},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3534678.3539405},
  abstract = {Knowledge graphs (KGs) capture knowledge in the form of head--relation--tail triples and are a crucial component in many AI systems. There are two important reasoning tasks on KGs: (1) single-hop knowledge graph completion, which involves predicting individual links in the KG; and (2), multi-hop reasoning, where the goal is to predict which KG entities satisfy a given logical query. Embedding-based methods solve both tasks by first computing an embedding for each entity and relation, then using them to form predictions. However, existing scalable KG embedding frameworks only support single-hop knowledge graph completion and cannot be applied to the more challenging multi-hop reasoning task. Here we present Scalable Multi-hOp REasoning (SMORE), the first general framework for both single-hop and multi-hop reasoning in KGs. Using a single machine SMORE can perform multi-hop reasoning in Freebase KG (86M entities, 338M edges), which is 1,500x larger than previously considered KGs. The key to SMORE's runtime performance is a novel bidirectional rejection sampling that achieves a square root reduction of the complexity of online training data generation. Furthermore, SMORE exploits asynchronous scheduling, overlapping CPU-based data sampling, GPU-based embedding computation, and frequent CPU--GPU IO. SMORE increases throughput (i.e., training speed) over prior multi-hop KG frameworks by 2.2x with minimal GPU memory requirements (2GB for training 400-dim embeddings on 86M-node Freebase) and achieves near linear speed-up with the number of GPUs. Moreover, on the simpler single-hop knowledge graph completion task SMORE achieves comparable or even better runtime performance to state-of-the-art frameworks on both single GPU and multi-GPU settings.},
  isbn = {978-1-4503-9385-0},
  keywords = {knowledge graph embeddings,multi-hop reasoning,scalable system}
}

@inproceedings{Ruffinelli2020YouCAN,
  title = {You {{CAN Teach}} an {{Old Dog New Tricks}}! {{On Training Knowledge Graph Embeddings}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ruffinelli, Daniel and Broscheit, Samuel and Gemulla, Rainer},
  year = {2020},
  month = mar,
  abstract = {Knowledge graph embedding (KGE) models learn algebraic representations of the entities and relations in a knowledge graph. A vast number of KGE techniques for multi-relational link prediction have been proposed in the recent literature, often with state-of-the-art performance. These approaches differ along a number of dimensions, including different model architectures, different training strategies, and different approaches to hyperparameter optimization. In this paper, we take a step back and aim to summarize and quantify empirically the impact of each of these dimensions on model performance. We report on the results of an extensive experimental study with popular model architectures and training strategies across a wide range of hyperparameter settings. We found that when trained appropriately, the relative performance differences between various model architectures often shrinks and sometimes even reverses when compared to prior results. For example, RESCAL\textasciitilde\textbackslash citep\{nickel2011three\}, one of the first KGE models, showed strong performance when trained with state-of-the-art techniques; it was competitive to or outperformed more recent architectures. We also found that good (and often superior to prior studies) model configurations can be found by exploring relatively few random samples from a large hyperparameter space. Our results suggest that many of the more advanced architectures and techniques proposed in the literature should be revisited to reassess their individual benefits. To foster further reproducible research, we provide all our implementations and experimental results as part of the open source LibKGE framework.},
  langid = {english}
}

@inproceedings{Saxena2021QuestionAnswering,
  title = {Question {{Answering Over Temporal Knowledge Graphs}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}}, {{ACL}}/{{IJCNLP}} 2021, ({{Volume}} 1: {{Long Papers}}), {{Virtual Event}}, {{August}} 1-6, 2021},
  author = {Saxena, Apoorv and Chakrabarti, Soumen and Talukdar, Partha P.},
  editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
  year = {2021},
  pages = {6663--6676},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.acl-long.520}
}

@inproceedings{Schlichtkrull2018ModelingRelational,
  title = {Modeling {{Relational Data}} with {{Graph Convolutional Networks}}},
  booktitle = {The {{Semantic Web}}},
  author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and {van~den Berg}, Rianne and Titov, Ivan and Welling, Max},
  editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Rapha{\"e}l and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
  year = {2018},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {593--607},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-93417-4_38},
  abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e.~subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
  isbn = {978-3-319-93417-4},
  langid = {english}
}

@inproceedings{Sun2022JointLKJoint,
  title = {{{JointLK}}: {{Joint Reasoning}} with {{Language Models}} and {{Knowledge Graphs}} for {{Commonsense Question Answering}}},
  shorttitle = {{{JointLK}}},
  booktitle = {Proceedings of the 2022 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{NAACL}} 2022, {{Seattle}}, {{WA}}, {{United States}}, {{July}} 10-15, 2022},
  author = {Sun, Yueqing and Shi, Qi and Qi, Le and Zhang, Yu},
  editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Ru{\'i}z, Iv{\'a}n Vladimir Meza},
  year = {2022},
  pages = {5049--5060},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2022.naacl-main.372}
}

@inproceedings{Talmor2018WebKnowledgeBase,
  title = {The {{Web}} as a {{Knowledge-Base}} for {{Answering Complex Questions}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Talmor, Alon and Berant, Jonathan},
  year = {2018},
  month = jun,
  pages = {641--651},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1059},
  abstract = {Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, ComplexWebQuestions, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.}
}

@inproceedings{Teru2020InductiveRelation,
  title = {Inductive {{Relation Prediction}} by {{Subgraph Reasoning}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Teru, Komal and Denis, Etienne and Hamilton, William L},
  year = {2020},
  month = nov,
  pages = {9448--9457},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The dominant paradigm for relation prediction in knowledge graphs involves learning and operating on latent representations (i.e., embeddings) of entities and relations. However, these embedding-based methods do not explicitly capture the compositional logical rules underlying the knowledge graph, and they are limited to the transductive setting, where the full set of entities must be known during training. Here, we propose a graph neural network based relation prediction framework, GraIL, that reasons over local subgraph structures and has a strong inductive bias to learn entity-independent relational semantics. Unlike embedding-based models, GraIL is naturally inductive and can generalize to unseen entities and graphs after training. We provide theoretical proof and strong empirical evidence that GraIL can rep-resent a useful subset of first-order logic and show that GraIL outperforms existing rule-induction baselines in the inductive setting. We also demonstrate significant gains obtained by ensembling GraIL with various knowledge graph embedding methods in the transductive setting, highlighting the complementary inductive bias of our method.},
  langid = {english}
}

@inproceedings{Toutanova2015Observedlatent,
  title = {Observed versus Latent Features for Knowledge Base and Text Inference},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Continuous Vector Space Models}} and Their {{Compositionality}}},
  author = {Toutanova, Kristina and Chen, Danqi},
  year = {2015},
  month = jul,
  pages = {57--66},
  publisher = {{Association for Computational Linguistics}},
  address = {{Beijing, China}},
  doi = {10.18653/v1/W15-4007}
}

@inproceedings{Trouillon2016ComplexEmbeddings,
  title = {Complex {{Embeddings}} for {{Simple Link Prediction}}},
  booktitle = {Proceedings of {{The}} 33rd {{International Conference}} on {{Machine Learning}}},
  author = {Trouillon, Th{\'e}o and Welbl, Johannes and Riedel, Sebastian and Gaussier, Eric and Bouchard, Guillaume},
  year = {2016},
  month = jun,
  pages = {2071--2080},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {In statistical relational learning, the link prediction problem is key to automatically understand the structure of large knowledge bases. As in previous studies, we propose to solve this problem through latent factorization. However, here we make use of complex valued embeddings. The composition of complex embeddings can handle a large variety of binary relations, among them symmetric and antisymmetric relations. Compared to state-of-the-art models such as Neural Tensor Network and Holographic Embeddings, our approach based on complex embeddings is arguably simpler, as it only uses the Hermitian dot product, the complex counterpart of the standard dot product between real vectors. Our approach is scalable to large datasets as it remains linear in both space and time, while consistently outperforming alternative approaches on standard link prediction benchmarks.},
  langid = {english}
}

@article{VandenBroeck2017QueryProcessing,
  title = {Query {{Processing}} on {{Probabilistic Data}}: {{A Survey}}},
  shorttitle = {Query {{Processing}} on {{Probabilistic Data}}},
  author = {{Van den Broeck}, Guy and Suciu, Dan},
  year = {2017},
  journal = {Foundations and Trends\textregistered{} in Databases},
  volume = {7},
  number = {3-4},
  pages = {197--341},
  issn = {1931-7883, 1931-7891},
  doi = {10.1561/1900000052},
  langid = {english}
}

@article{Vrandecic2014Wikidatafree,
  title = {Wikidata: A Free Collaborative Knowledgebase},
  shorttitle = {Wikidata},
  author = {Vrande{\v c}i{\'c}, Denny and Kr{\"o}tzsch, Markus},
  year = {2014},
  month = sep,
  journal = {Communications of the ACM},
  volume = {57},
  number = {10},
  pages = {78--85},
  issn = {0001-0782},
  doi = {10.1145/2629489},
  abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.}
}

@inproceedings{Wang2021BenchmarkingCombinatorial,
  title = {Benchmarking the {{Combinatorial Generalizability}} of {{Complex Query Answering}} on {{Knowledge Graphs}}},
  booktitle = {Proceedings of the {{Neural Information Processing Systems Track}} on {{Datasets}} and {{Benchmarks}} 1, {{NeurIPS Datasets}} and {{Benchmarks}} 2021, {{December}} 2021, Virtual},
  author = {Wang, Zihao and Yin, Hang and Song, Yangqiu},
  editor = {Vanschoren, Joaquin and Yeung, Sai-Kit},
  year = {2021}
}

@inproceedings{Xu2022NeuralSymbolicEntangleda,
  title = {Neural-{{Symbolic Entangled Framework}} for {{Complex Query Answering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xu, Zezhong and Zhang, Wen and Ye, Peng and Chen, Hui and Chen, Huajun},
  year = {2022},
  month = oct,
  abstract = {Answering complex queries over knowledge graphs (KG) is an important yet challenging task because of the KG incompleteness issue and cascading errors during reasoning. Recent query embedding (QE) approaches embed the entities and relations in a KG and the first-order logic (FOL) queries into a low dimensional space, making the query can be answered by dense similarity searching. However, previous works mainly concentrate on the target answers, ignoring intermediate entities' usefulness, which is essential for relieving the cascading error problem in logical query answering. In addition, these methods are usually designed with their own geometric or distributional embeddings to handle logical operators like union, intersection, and negation, with the sacrifice of the accuracy of the basic operator -- projection, and they could not absorb other embedding methods to their models. In this work, we propose a Neural and Symbolic Entangled framework (ENeSy) for complex query answering, which enables the neural and symbolic reasoning to enhance each other to alleviate the cascading error and KG incompleteness. The projection operator in ENeSy could be any embedding method with the capability of link prediction, and the other FOL operators are handled without parameters. With both neural and symbolic reasoning results contained, ENeSy answers queries in ensembles. We evaluate ENeSy on complex query answering benchmarks, and ENeSy achieves the state-of-the-art, especially in the setting of training model only with the link prediction task.},
  langid = {english}
}

@misc{Yang2022GammaEGamma,
  title = {{{GammaE}}: {{Gamma Embeddings}} for {{Logical Queries}} on {{Knowledge Graphs}}},
  shorttitle = {{{GammaE}}},
  author = {Yang, Dong and Qing, Peijun and Li, Yang and Lu, Haonan and Lin, Xiaodong},
  year = {2022},
  month = oct,
  number = {arXiv:2210.15578},
  eprint = {2210.15578},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.15578},
  abstract = {Embedding knowledge graphs (KGs) for multi-hop logical reasoning is a challenging problem due to massive and complicated structures in many KGs. Recently, many promising works projected entities and queries into a geometric space to efficiently find answers. However, it remains challenging to model the negation and union operator. The negation operator has no strict boundaries, which generates overlapped embeddings and leads to obtaining ambiguous answers. An additional limitation is that the union operator is non-closure, which undermines the model to handle a series of union operators. To address these problems, we propose a novel probabilistic embedding model, namely Gamma Embeddings (GammaE), for encoding entities and queries to answer different types of FOL queries on KGs. We utilize the linear property and strong boundary support of the Gamma distribution to capture more features of entities and queries, which dramatically reduces model uncertainty. Furthermore, GammaE implements the Gamma mixture method to design the closed union operator. The performance of GammaE is validated on three large logical query datasets. Experimental results show that GammaE significantly outperforms state-of-the-art models on public benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,Computer Science - Machine Learning}
}

@inproceedings{Yasunaga2021QAGNNReasoning,
  title = {{{QA-GNN}}: {{Reasoning}} with {{Language Models}} and {{Knowledge Graphs}} for {{Question Answering}}},
  shorttitle = {{{QA-GNN}}},
  booktitle = {Proceedings of the 2021 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{NAACL-HLT}} 2021, {{Online}}, {{June}} 6-11, 2021},
  author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
  editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and {Hakkani-T{\"u}r}, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
  year = {2021},
  pages = {535--546},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.naacl-main.45}
}

@inproceedings{Zaheer2017DeepSets,
  title = {Deep {{Sets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {We study the problem of designing models for machine learning tasks defined on sets. In contrast to the traditional approach of operating on fixed dimensional vectors, we consider objective functions defined on sets and are invariant to permutations. Such problems are widespread, ranging from the estimation of population statistics, to anomaly detection in piezometer data of embankment dams, to cosmology. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and outlier detection.}
}

@inproceedings{Zhang2021ConECone,
  ids = {zhangConEConeEmbeddings2021a},
  title = {{{ConE}}: {{Cone Embeddings}} for {{Multi-Hop Reasoning}} over {{Knowledge Graphs}}},
  shorttitle = {{{ConE}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhang, Zhanqiu and Wang, Jie and Chen, Jiajun and Ji, Shuiwang and Wu, Feng},
  year = {2021},
  volume = {34},
  pages = {19172--19183},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Query embedding (QE)---which aims to embed entities and first-order logical (FOL) queries in low-dimensional spaces---has shown great power in multi-hop reasoning over knowledge graphs. Recently, embedding entities and queries with geometric shapes becomes a promising direction, as geometric shapes can naturally represent answer sets of queries and logical relationships among them. However, existing geometry-based models have difficulty in modeling queries with negation, which significantly limits their applicability. To address this challenge, we propose a novel query embedding model, namely \textbackslash textbf\{Con\}e \textbackslash textbf\{E\}mbeddings (ConE), which is the first geometry-based QE model that can handle all the FOL operations, including conjunction, disjunction, and negation. Specifically, ConE represents entities and queries as Cartesian products of two-dimensional cones, where the intersection and union of cones naturally model the conjunction and disjunction operations. By further noticing that the closure of complement of cones remains cones, we design geometric complement operators in the embedding space for the negation operations. Experiments demonstrate that ConE significantly outperforms existing state-of-the-art methods on benchmark datasets.}
}

@article{Zhang2021Drugrepurposing,
  title = {Drug Repurposing for {{COVID-19}} via Knowledge Graph Completion},
  author = {Zhang, Rui and Hristovski, Dimitar and Schutte, Dalton and Kastrin, Andrej and Fiszman, Marcelo and Kilicoglu, Halil},
  year = {2021},
  month = mar,
  journal = {Journal of Biomedical Informatics},
  volume = {115},
  pages = {103696},
  issn = {1532-0480},
  doi = {10.1016/j.jbi.2021.103696},
  abstract = {OBJECTIVE: To discover candidate drugs to repurpose for COVID-19 using literature-derived knowledge and knowledge graph completion methods. METHODS: We propose a novel, integrative, and neural network-based literature-based discovery (LBD) approach to identify drug candidates from PubMed and other COVID-19-focused research literature. Our approach relies on semantic triples extracted using SemRep (via SemMedDB). We identified an informative and accurate subset of semantic triples using filtering rules and an accuracy classifier developed on a BERT variant. We used this subset to construct a knowledge graph, and applied five state-of-the-art, neural knowledge graph completion algorithms (i.e., TransE, RotatE, DistMult, ComplEx, and STELP) to predict drug repurposing candidates. The models were trained and assessed using a time slicing approach and the predicted drugs were compared with a list of drugs reported in the literature and evaluated in clinical trials. These models were complemented by a discovery pattern-based approach. RESULTS: Accuracy classifier based on PubMedBERT achieved the best performance (F1 = 0.854) in identifying accurate semantic predications. Among five knowledge graph completion models, TransE outperformed others (MR~=~0.923, Hits@1~=~0.417). Some known drugs linked to COVID-19 in the literature were identified, as well as others that have not yet been studied. Discovery patterns enabled identification of additional candidate drugs and generation of plausible hypotheses regarding the links between the candidate drugs and COVID-19. Among them, five highly ranked and novel drugs (i.e., paclitaxel, SB 203580, alpha 2-antiplasmin, metoclopramide, and oxymatrine) and the mechanistic explanations for their potential use are further discussed. CONCLUSION: We showed that a LBD approach can be feasible not only for discovering drug candidates for COVID-19, but also for generating mechanistic explanations. Our approach can be generalized to other diseases as well as to other clinical questions. Source code and data are available at https://github.com/kilicogluh/lbd-covid.},
  langid = {english},
  pmcid = {PMC7869625},
  pmid = {33571675},
  keywords = {Algorithms,Antiviral Agents,COVID-19,COVID-19 Drug Treatment,Drug Repositioning,Drug repurposing,Humans,Knowledge Discovery,Knowledge graph completion,Literature-based discovery,Neural Networks; Computer,SARS-CoV-2,Text mining}
}

@inproceedings{Zhang2021MultiHopReasoning,
  title = {Multi-{{Hop Reasoning}} for {{Question Answering}} with {{Knowledge Graph}}},
  booktitle = {19th {{IEEE}}/{{ACIS International Conference}} on {{Computer}} and {{Information Science}}, {{ICIS}} 2021, {{Shanghai}}, {{China}}, {{June}} 23-25, 2021},
  author = {Zhang, Jiayuan and Cai, Yifei and Zhang, Qian and Cao, Zehao and Cheng, Zhenrong and Li, Dongmei and Meng, Xianghao},
  year = {2021},
  pages = {121--125},
  publisher = {{IEEE}},
  doi = {10.1109/ICIS51600.2021.9516865}
}

@inproceedings{Zhang2022FactTreeReasoning,
  title = {Fact-{{Tree Reasoning}} for {{N-ary Question Answering}} over {{Knowledge Graphs}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022, {{Dublin}}, {{Ireland}}, {{May}} 22-27, 2022},
  author = {Zhang, Yao and Li, Peiyao and Liang, Hongru and Jatowt, Adam and Yang, Zhenglu},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = {2022},
  pages = {788--802},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2022.findings-acl.66}
}

@article{Zhang2022KnowledgeGraph,
  title = {Knowledge {{Graph Reasoning}} with {{Logics}} and {{Embeddings}}: {{Survey}} and {{Perspective}}},
  shorttitle = {Knowledge {{Graph Reasoning}} with {{Logics}} and {{Embeddings}}},
  author = {Zhang, Wen and Chen, Jiaoyan and Li, Juan and Xu, Zezhong and Pan, Jeff Z. and Chen, Huajun},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.07412 [cs]},
  eprint = {2202.07412},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Knowledge graph (KG) reasoning is becoming increasingly popular in both academia and industry. Conventional KG reasoning based on symbolic logic is deterministic, with reasoning results being explainable, while modern embedding-based reasoning can deal with uncertainty and predict plausible knowledge, often with high efficiency via vector computation. A promising direction is to integrate both logic-based and embedding-based methods, with the vision to have advantages of both. It has attracted wide research attention with more and more works published in recent years. In this paper, we comprehensively survey these works, focusing on how logics and embeddings are integrated. We first briefly introduce preliminaries, then systematically categorize and discuss works of logic and embedding-aware KG reasoning from different perspectives, and finally conclude and discuss the challenges and further directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {00000}
}

@article{Zhao2022Improvingquestion,
  title = {Improving Question Answering over Incomplete Knowledge Graphs with Relation Prediction},
  author = {Zhao, Fen and Li, Yinguo and Hou, Jie and Bai, Ling},
  year = {2022},
  journal = {Neural Computing and Applications},
  issn = {1433-3058},
  doi = {10.1007/s00521-021-06736-7},
  abstract = {Large-scale knowledge graphs (KGs) play a critical role in question answering over KGs (KGs-QA). Despite of large scale, KGs suffer from incompleteness, which has fueled a lot of research on relation prediction. Since existing researches of relation prediction process each triple independently, the hidden relations which are inherently present can not be captured. Complementarily, to simultaneously capture both entity features and relation features in a given entity's neighborhood, an entity importance estimation network of attention-based graph embedding is proposed, which consists of the attention-based graph embedding module and the entity importance estimation module. Firstly, the new embedding of an entity from its n-hop neighbor is learned by an attention-based graph embedding module. Then, the learned new embedding is integrated into the entity importance estimation module to find entities of high importance in n-hop neighbors of the central entity. Finally, multi-hop relations are encapsulated and an auxiliary edge of n-hop neighbors is introduced, which realizes the relation prediction task. To the best our knowledge, we are the first to realize KGs-QA while realizing relation prediction, which alleviates the phenomenon of missing relations and the low-precision problem of KGs-QA. On the SQ datasets, the proposed method obtains a high F1 score (49.3\%) in 10\% missing relation, compared to QASE and MCCNNs with F1 scores of 44.2\% and 46.3\%, respectively.},
  langid = {english},
  annotation = {00000}
}

@inproceedings{Zhao2022SimulateHuman,
  title = {Simulate {{Human Thinking}}: {{Cognitive Knowledge Graph Reasoning}} for {{Complex Question Answering}}},
  shorttitle = {Simulate {{Human Thinking}}},
  booktitle = {Advances in {{Knowledge Discovery}} and {{Data Mining}} - 26th {{Pacific-Asia Conference}}, {{PAKDD}} 2022, {{Chengdu}}, {{China}}, {{May}} 16-19, 2022, {{Proceedings}}, {{Part I}}},
  author = {Zhao, Hong and Fu, Yao and Jiang, Weihao and Pu, Shiliang and Cai, Xiaoyu},
  editor = {Gama, Jo{\~a}o and Li, Tianrui and Yu, Yang and Chen, Enhong and Zheng, Yu and Teng, Fei},
  year = {2022},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {13280},
  pages = {522--534},
  publisher = {{Springer}},
  doi = {10.1007/978-3-031-05933-9_41}
}

@inproceedings{Zhu2021NeuralBellmanForda,
  title = {Neural {{Bellman-Ford Networks}}: {{A General Graph Neural Network Framework}} for {{Link Prediction}}},
  shorttitle = {Neural {{Bellman-Ford Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhu, Zhaocheng and Zhang, Zuobai and Xhonneux, Louis-Pascal and Tang, Jian},
  year = {2021},
  volume = {34},
  pages = {29476--29490},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Link prediction is a very fundamental task on graphs. Inspired by traditional path-based methods, in this paper we propose a general and flexible representation learning framework based on paths for link prediction. Specifically, we define the representation of a pair of nodes as the generalized sum of all path representations, with each path representation as the generalized product of the edge representations in the path. Motivated by the Bellman-Ford algorithm for solving the shortest path problem, we show that the proposed path formulation can be efficiently solved by the generalized Bellman-Ford algorithm. To further improve the capacity of the path formulation, we propose the Neural Bellman-Ford Network (NBFNet), a general graph neural network framework that solves the path formulation with learned operators in the generalized Bellman-Ford algorithm. The NBFNet parameterizes the generalized Bellman-Ford algorithm with 3 neural components, namely Indicator, Message and Aggregate functions, which corresponds to the boundary condition, multiplication operator, and summation operator respectively. The NBFNet covers many traditional path-based methods, and can be applied to both homogeneous graphs and multi-relational graphs (e.g., knowledge graphs) in both transductive and inductive settings. Experiments on both homogeneous graphs and knowledge graphs show that the proposed NBFNet outperforms existing methods by a large margin in both transductive and inductive settings, achieving new state-of-the-art results.}
}

@misc{Zhu2022NeuralSymbolicModelsa,
  title = {Neural-{{Symbolic Models}} for {{Logical Queries}} on {{Knowledge Graphs}}},
  author = {Zhu, Zhaocheng and Galkin, Mikhail and Zhang, Zuobai and Tang, Jian},
  year = {2022},
  month = may,
  number = {arXiv:2205.10128},
  eprint = {2205.10128},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Answering complex first-order logic (FOL) queries on knowledge graphs is a fundamental task for multi-hop reasoning. Traditional symbolic methods traverse a complete knowledge graph to extract the answers, which provides good interpretation for each step. Recent neural methods learn geometric embeddings for complex queries. These methods can generalize to incomplete knowledge graphs, but their reasoning process is hard to interpret. In this paper, we propose Graph Neural Network Query Executor (GNN-QE), a neural-symbolic model that enjoys the advantages of both worlds. GNN-QE decomposes a complex FOL query into relation projections and logical operations over fuzzy sets, which provides interpretability for intermediate variables. To reason about the missing links, GNN-QE adapts a graph neural network from knowledge graph completion to execute the relation projections, and models the logical operations with product fuzzy logic. Extensive experiments on 3 datasets show that GNN-QE significantly improves over previous state-of-the-art models in answering FOL queries. Meanwhile, GNN-QE can predict the number of answers without explicit supervision, and provide visualizations for intermediate variables.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}

@incollection{emerson1990temporal,
  title={Temporal and modal logic},
  author={Emerson, E Allen},
  booktitle={Formal Models and Semantics},
  pages={995--1072},
  year={1990},
  publisher={Elsevier}
}

@inproceedings{DBLP:conf/iclr/BelloPL0B17,
  author    = {Irwan Bello and
               Hieu Pham and
               Quoc V. Le and
               Mohammad Norouzi and
               Samy Bengio},
  title     = {Neural Combinatorial Optimization with Reinforcement Learning},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Workshop Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Bk9mxlSFx},
  timestamp = {Thu, 04 Apr 2019 13:20:08 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/BelloPL0B17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{petroni2019language,
  title={Language Models as Knowledge Bases?},
  author={Petroni, Fabio and Rockt{\"a}schel, Tim and Riedel, Sebastian and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={2463--2473},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{xiong2017deeppath,
  title={DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning},
  author={Xiong, Wenhan and Hoang, Thien and Wang, William Yang},
  booktitle={Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
  pages={564--573},
  year={2017}
}

@article{lu2022semantic,
  title = {Semantic-Discriminative Mixup for Generalizable Sensor-based Cross-domain Activity Recognition},
  author = {Lu, Wang and Wang, Jindong and Chen, Yiqiang and Pan, Sinno and Hu, Chunyu and Qin, Xin},
  journal = {IMWUT},
  year = {2022},
  abbr = {IMWUT},
  bibtex_show = {true},
  corr = {true},
  selected = {true}
}

@article{he2021locality,
  title={Locality-Aware Channel-Wise Dropout for Occluded Face Recognition},
  author={He, Mingjie and Zhang, Jie and Shan, Shiguang and Liu, Xiao and Wu, Zhongqin and Chen, Xilin},
  journal={IEEE Transactions on Image Processing},
  volume={31},
  pages={788--798},
  year={2021},
  publisher={IEEE}
}

@article{lu2021cross,
  title = {Cross-domain activity recognition via substructural optimal transport},
  author = {Lu, Wang and Chen, Yiqiang and Wang, Jindong and Qin, Xin},
  journal = {Neurocomputing},
  volume = {454},
  pages = {65--75},
  year = {2021},
  publisher = {Elsevier},
  abbr = {NeuCom},
  bibtex_show = {true},
  arxiv = {https://arxiv.org/abs/2102.03353},
  html = {https://www.sciencedirect.com/science/article/abs/pii/S0925231221007025},
  zhihu = {https://zhuanlan.zhihu.com/p/356904023},
  code = {https://github.com/jindongwang/transferlearning/tree/master/code/traditional/sot},
  pdf = {neurocomputing21-sot.pdf}
}

@inproceedings{li2021medical,
  title={Medical Image Segmentation using Squeeze-and-Expansion Transformers},
  author={Li, Shaohua and Sui, Xiuchao and Luo, Xiangde and Xu, Xinxing and Liu, Yong and Goh, Rick Siow Mong},
  booktitle={IJCAI},
  year={2021}
}

@inproceedings{DBLP:conf/kdd/MaYLXS21,
  author    = {Fenglong Ma and
               Muchao Ye and
               Junyu Luo and
               Cao Xiao and
               Jimeng Sun},
  editor    = {Feida Zhu and
               Beng Chin Ooi and
               Chunyan Miao},
  title     = {Advances in Mining Heterogeneous Healthcare Data},
  booktitle = {{KDD} '21: The 27th {ACM} {SIGKDD} Conference on Knowledge Discovery
               and Data Mining, Virtual Event, Singapore, August 14-18, 2021},
  pages     = {4050--4051},
  publisher = {{ACM}},
  year      = {2021},
  url       = {https://doi.org/10.1145/3447548.3470789},
  doi       = {10.1145/3447548.3470789},
  timestamp = {Mon, 16 Aug 2021 16:18:50 +0200},
  biburl    = {https://dblp.org/rec/conf/kdd/MaYLXS21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{inkster2018china,
	title={China’s cyber power},
	author={Inkster, Nigel},
	year={2018},
	publisher={Routledge}
}

@article{yang2019federated,
	title={Federated machine learning: Concept and applications},
	author={Yang, Qiang and Liu, Yang and Chen, Tianjian and Tong, Yongxin},
	journal={ACM Transactions on Intelligent Systems and Technology (TIST)},
	volume={10},
	number={2},
	pages={1--19},
	year={2019},
	publisher={ACM New York, NY, USA}
}

@article{voigt2017eu,
  title={The eu general data protection regulation (gdpr)},
  author={Voigt, Paul and Von dem Bussche, Axel},
  journal={A Practical Guide, 1st Ed., Cham: Springer International Publishing},
  volume={10},
  pages={3152676},
  year={2017},
  publisher={Springer}
}

@inproceedings{mcmahan2017communication,
	title={Communication-efficient learning of deep networks from decentralized data},
	author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
	booktitle={Artificial Intelligence and Statistics},
	pages={1273--1282},
	year={2017},
	organization={PMLR}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff and others},
  journal={arXiv preprint arXiv:1503.02531},
  volume={2},
  number={7},
  year={2015}
}

@article{romero2014fitnets,
  title={Fitnets: Hints for thin deep nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1412.6550},
  year={2014}
}

@article{wang2019deep,
  title = {Deep learning for sensor-based activity recognition: A survey},
  author = {Wang, Jindong and Chen, Yiqiang and Hao, Shuji and Peng, Xiaohui and Hu, Lisha},
  journal = {Pattern Recognition Letters},
  volume = {119},
  pages = {3--11},
  year = {2019},
  publisher = {Elsevier},
  html = {https://www.sciencedirect.com/science/article/pii/S016786551830045X},
  pdf = {a10_prl18.pdf},
  bibtex_show = {true},
  abbr = {PRL}
}

@ARTICLE{9129779,
  author={Muhammad, Khan and Khan, Salman and Ser, Javier Del and Albuquerque, Victor Hugo C. de},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Deep Learning for Multigrade Brain Tumor Classification in Smart Healthcare Systems: A Prospective Survey}, 
  year={2021},
  volume={32},
  number={2},
  pages={507-522},
  doi={10.1109/TNNLS.2020.2995800}}
  
@inproceedings{chen2017pdassist,
	title={PdAssist: Objective and quantified symptom assessment of Parkinson's disease via smartphone},
	author={Chen, Yiqiang and Yang, Xiaodong and Chen, Biao and Miao, Chunyan and Yu, Hanchao},
	booktitle={2017 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)},
	pages={939--945},
	year={2017},
	organization={IEEE}
}

@article{li2019convergence,
	title={On the convergence of fedavg on non-iid data},
	author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
	journal={arXiv preprint arXiv:1907.02189},
	year={2019}
}

@article{li2018federated,
  title={Federated optimization in heterogeneous networks},
  author={Li, Tian and Sahu, Anit Kumar and Zaheer, Manzil and Sanjabi, Maziar and Talwalkar, Ameet and Smith, Virginia},
  journal={Proceedings of Machine Learning and Systems},
  volume={2},
  pages={429--450},
  year={2020}
}

@article{arivazhagan2019federated,
	title={Federated learning with personalization layers},
	author={Arivazhagan, Manoj Ghuhan and Aggarwal, Vinay and Singh, Aaditya Kumar and Choudhary, Sunav},
	journal={arXiv preprint arXiv:1912.00818},
	year={2019}
}

@article{yu2020salvaging,
	title={Salvaging federated learning by local adaptation},
	author={Yu, Tao and Bagdasaryan, Eugene and Shmatikov, Vitaly},
	journal={arXiv preprint arXiv:2002.04758},
	year={2020}
}

@inproceedings{li2021fedbn,
  title={FedBN: Federated Learning on Non-IID Features via Local Batch Normalization},
  author={Li, Xiaoxiao and JIANG, Meirui and Zhang, Xiaofei and Kamp, Michael and Dou, Qi},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{chen2020fedhealth,
	title={Fedhealth: A federated transfer learning framework for wearable healthcare},
	author={Chen, Yiqiang and Qin, Xin and Wang, Jindong and Yu, Chaohui and Gao, Wen},
	journal={IEEE Intelligent Systems},
	volume={35},
	number={4},
	pages={83--93},
	year={2020},
	publisher={IEEE}
}

@article{chen2021federated,
  title={Federated Learning with Adaptive Batchnorm for Personalized Healthcare},
  author={Chen, Yiqiang and Lu, Wang and Wang, Jindong and Qin, Xin and Qin, Tao},
  journal={arXiv preprint arXiv:2112.00734},
  year={2021}
}

@article{zaccone2022speeding,
  title={Speeding up Heterogeneous Federated Learning with Sequentially Trained Superclients},
  author={Zaccone, Riccardo and Rizzardi, Andrea and Caldarola, Debora and Ciccone, Marco and Caputo, Barbara},
  journal={arXiv},
  year={2022}
}

@article{kopparapu2020fedfmc,
  title={Fedfmc: Sequential efficient federated learning on non-iid data},
  author={Kopparapu, Kavya and Lin, Eric},
  journal={arXiv preprint arXiv:2006.10937},
  year={2020}
}

@inproceedings{zeng2022heterogeneous,
  author    = {Shenglai Zeng and
               Zonghang Li and
               Hongfang Yu and
               Yihong He and
               Zenglin Xu and
               Dusit Niyato and
               Han Yu},
  title = {Heterogeneous Federated Learning via Grouped Sequential-to-Parallel Training},
  booktitle = {Database Systems for Advanced Applications - 27th International Conference, {DASFAA} 2022, Virtual Event, April 11-14, 2022, Proceedings, Part{II}},
  series    = {Lecture Notes in Computer Science},
  volume    = {13246},
  pages     = {455--471},
  publisher = {Springer},
  year      = {2022}
}

@inproceedings{usmanova2021distillation,
  title={A distillation-based approach integrating continual learning and federated learning for pervasive services},
  author={Usmanova, Anastasiia and Portet, Fran{\c{c}}ois and Lalanda, Philippe and Vega, German},
  booktitle={3rd Workshop on Continual and Multimodal Learning for Internet of Things--Co-located with IJCAI 2021},
  year={2021}
}

@inproceedings{fang2013unbiased,
  title={Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias},
  author={Fang, Chen and Xu, Ye and Rockmore, Daniel N},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1657--1664},
  year={2013}
}

@inproceedings{reiss2012introducing,
	title={Introducing a new benchmarked dataset for activity monitoring},
	author={Reiss, Attila and Stricker, Didier},
	booktitle={2012 16th International Symposium on Wearable Computers},
	pages={108--109},
	year={2012},
	organization={IEEE}
}

@inproceedings{krizhevsky2012imagenet,
	title={Imagenet classification with deep convolutional neural networks},
	author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	booktitle={NeurIPS},
	volume={25},
	pages={1097--1105},
	year={2012}
}

@article{medmnistv2,
    title={MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification},
    author={Yang, Jiancheng and Shi, Rui and Wei, Donglai and Liu, Zequan and Zhao, Lin and Ke, Bilian and Pfister, Hanspeter and Ni, Bingbing},
    journal={arXiv preprint arXiv:2008.\#TODO},
    year={2021}
}

@inproceedings{medmnistv1,
    title={MedMNIST Classification Decathlon: A Lightweight AutoML Benchmark for Medical Image Analysis},
    author={Yang, Jiancheng and Shi, Rui and Ni, Bingbing},
    booktitle={ISBI},
    pages={191--195},
    year={2021}
}

@article{bilic2019liver,
  title={The liver tumor segmentation benchmark (lits)},
  author={Bilic, Patrick and Christ, Patrick and Li, Hongwei Bran and Vorontsov, Eugene and Ben-Cohen, Avi and Kaissis, Georgios and Szeskin, Adi and Jacobs, Colin and Mamani, Gabriel Efrain Humpire and Chartrand, Gabriel and others},
  journal={Medical Image Analysis},
  pages={102680},
  year={2022},
  publisher={Elsevier}
}

@article{xu2019efficient,
  title={Efficient multiple organ localization in CT image using 3D region proposal network},
  author={Xu, Xuanang and Zhou, Fugen and Liu, Bo and Fu, Dongshan and Bai, Xiangzhi},
  journal={IEEE transactions on medical imaging},
  volume={38},
  number={8},
  pages={1885--1898},
  year={2019},
  publisher={IEEE}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{yurochkin2019bayesian,
	title={Bayesian nonparametric federated learning of neural networks},
	author={Yurochkin, Mikhail and Agarwal, Mayank and Ghosh, Soumya and Greenewald, Kristjan and Hoang, Nghia and Khazaeni, Yasaman},
	booktitle={ICML},
	pages={7252--7261},
	year={2019}
}

@inproceedings{lu2022local,
  title = {Local and global alignments for generalizable sensor-based human activity recognition},
  author = {Lu, Wang and Wang, Jindong and Chen, Yiqiang},
  booktitle = {IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2022},
  bibtex_show = {true},
  corr = {true},
  abbr = {ICASSP}
}

@article{roy2019braintorrent,
  title={Braintorrent: A peer-to-peer environment for decentralized federated learning},
  author={Roy, Abhijit Guha and Siddiqui, Shayan and P{\"o}lsterl, Sebastian and Navab, Nassir and Wachinger, Christian},
  journal={arXiv},
  year={2019}
}

@article{rieke2020future,
  title={The future of digital health with federated learning},
  author={Rieke, Nicola and Hancox, Jonny and Li, Wenqi and Milletari, Fausto and Roth, Holger R and Albarqouni, Shadi and Bakas, Spyridon and Galtier, Mathieu N and Landman, Bennett A and Maier-Hein, Klaus and others},
  journal={NPJ digital medicine},
  volume={3},
  number={1},
  pages={1--7},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{li2021fedh2l,
  title={FedH2L: Federated learning with model and statistical heterogeneity},
  author={Li, Yiying and Zhou, Wei and Wang, Huaimin and Mi, Haibo and Hospedales, Timothy M},
  journal={arXiv},
  year={2021}
}

@inproceedings{afonin2021towards,
  title={Towards Model Agnostic Federated Learning Using Knowledge Distillation},
  author={Afonin, Andrei and Karimireddy, Sai Praneeth},
  booktitle={International Conference on Learning Representations},
  year={2022}
}


@article{lu2022personalized,
  title = {Personalized Federated Learning with Adaptive Batchnorm for Healthcare},
  author = {Lu, Wang and Wang, Jindong and Chen, Yiqiang and Qin, Xin and Xu, Renjun and Dimitriadis, Dimitrios and Qin, Tao},
  journal = {IEEE Transactions on Big Data},
  year = {2022},
  bibtex_show = {true},
  corr = {true},
  abbr = {TBD},
  arxiv = {https://arxiv.org/abs/2112.00734},
  pdf = {tbd22-fedap.pdf},
  html = {https://ieeexplore.ieee.org/document/9780172}
}

@article{lu2022domain,
  title={Domain-invariant Feature Exploration for Domain Generalization},
  author={Lu, Wang and Wang, Jindong and Li, Haoliang and Chen, Yiqiang and Xie, Xing},
  journal={arXiv preprint arXiv:2207.12020},
  year={2022}
}

@article{paluru2021anam,
  title={Anam-Net: Anamorphic depth embedding-based lightweight CNN for segmentation of anomalies in COVID-19 chest CT images},
  author={Paluru, Naveen and Dayal, Aveen and Jenssen, H{\aa}vard Bj{\o}rke and Sakinis, Tomas and Cenkeramaddi, Linga Reddy and Prakash, Jaya and Yalavarthy, Phaneendra K},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={32},
  number={3},
  pages={932--946},
  year={2021},
  publisher={IEEE}
}

@article{yu2022healthnet,
  title={HealthNet: A Health Progression Network via Heterogeneous Medical Information Fusion},
  author={Yu, Fuqiang and Cui, Lizhen and Chen, Huanhuan and Cao, Yiming and Liu, Ning and Huang, Weiming and Xu, Yonghui and Lu, Hua},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@article{ding2021deepkeygen,
  title={DeepKeyGen: a deep learning-based stream cipher generator for medical image encryption and decryption},
  author={Ding, Yi and Tan, Fuyuan and Qin, Zhen and Cao, Mingsheng and Choo, Kim-Kwang Raymond and Qin, Zhiguang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2021},
  publisher={IEEE}
}

@article{tan2022towards,
  title={Towards personalized federated learning},
  author={Tan, Alysa Ziying and Yu, Han and Cui, Lizhen and Yang, Qiang},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2022},
  publisher={IEEE}
}

@inproceedings{aguiar2022learning,
  title={Learning of Cluster-based Feature Importance for Electronic Health Record Time-series},
  author={Aguiar, Henrique and Santos, Mauro and Watkinson, Peter and Zhu, Tingting},
  booktitle={International Conference on Machine Learning},
  pages={161--179},
  year={2022},
  organization={PMLR}
}

@inproceedings{liu2022contribution,
  title={Contribution-aware federated learning for smart healthcare},
  author={Liu, Zelei and Chen, Yuanyuan and Zhao, Yansong and Yu, Han and Liu, Yang and Bao, Renyi and Jiang, Jinpeng and Nie, Zaiqing and Xu, Qian and Yang, Qiang},
  booktitle={Proceedings of the 34th Annual Conference on Innovative Applications of Artificial Intelligence (IAAI-22)},
  year={2022}
}

@inproceedings{li2017deeper,
  title={Deeper, broader and artier domain generalization},
  author={Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5542--5550},
  year={2017}
}

@article{sait2020curated,
  title={Curated dataset for COVID-19 posterior-anterior chest radiography images (X-Rays)},
  author={Sait, Unais and Lal, KG and Prajapati, S and Bhaumik, Rahul and Kumar, Tarun and Sanjana, S and Bhalla, Kriti},
  journal={Mendeley Data},
  volume={1},
  year={2020}
}


@online{covid-19-d,
  author="Oxford University Press",
  title="{Oxford English Dictionary}",
  url="https://www.oed.com/view/Entry/88575495",
  note="(2020, April)",
}

@online{covid-19,
  author="WIKIPEDIA",
  title="{COVID-19}",
  url="https://en.wikipedia.org/wiki/COVID-19#cite_ref-WSJ-20210226_7-0",
  note="(2021, Feburary)",
}

@article{salehi2020coronavirus,
  title={Coronavirus disease 2019 (COVID-19): a systematic review of imaging findings in 919 patients},
  author={Salehi, Sana and Abedi, Aidin and Balakrishnan, Sudheer and Gholamrezanezhad, Ali and others},
  journal={Ajr Am J Roentgenol},
  volume={215},
  number={1},
  pages={87--93},
  year={2020}
}

@article{weintraub2008parkinson,
  title={Parkinson's disease--Part 1: Pathophysiology, symptoms, burden, diagnosis, and assessment},
  author={Weintraub, Daniel and Comella, Cynthia L and Horn, Stacy},
  journal={Am J Manag Care},
  volume={14},
  number={2 Suppl},
  pages={S40--S48},
  year={2008}
}

@article{jankovic2008parkinson,
  title={Parkinson’s disease: clinical features and diagnosis},
  author={Jankovic, Joseph},
  journal={Journal of neurology, neurosurgery \& psychiatry},
  volume={79},
  number={4},
  pages={368--376},
  year={2008},
  publisher={BMJ Publishing Group Ltd}
}

@online{pd-wiki,
  author="WIKIPEDIA",
  title="{Parkinson's disease}",
  url="https://en.wikipedia.org/wiki/Parkinson\%27s_disease",
  note="(2021, April)",
}

@article{yang2020current,
  title={Current and projected future economic burden of Parkinson’s disease in the US},
  author={Yang, Wenya and Hamilton, Jamie L and Kopil, Catherine and Beck, James C and Tanner, Caroline M and Albin, Roger L and Ray Dorsey, E and Dahodwala, Nabila and Cintina, Inna and Hogan, Paul and others},
  journal={npj Parkinson's Disease},
  volume={6},
  number={1},
  pages={1--9},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{movement2003unified,
  title={The unified Parkinson's disease rating scale (UPDRS): status and recommendations},
  author={Movement Disorder Society Task Force on Rating Scales for Parkinson's Disease},
  journal={Movement Disorders},
  volume={18},
  number={7},
  pages={738--750},
  year={2003},
  publisher={Wiley Online Library}
}

@article{vogenberg2010personalized,
  title={Personalized medicine: part 1: evolution and development into theranostics},
  author={Vogenberg, F Randy and Barash, Carol Isaacson and Pursel, Michael},
  journal={Pharmacy and Therapeutics},
  volume={35},
  number={10},
  pages={560},
  year={2010},
  publisher={MediMedia, USA}
}

@inproceedings{chenbridging,
  title={On Bridging Generic and Personalized Federated Learning for Image Classification},
  author={Chen, Hong-You and Chao, Wei-Lun},
  year={2022},
  booktitle={International Conference on Learning Representations}
}

@article{warnat2021swarm,
  title={Swarm learning for decentralized and confidential clinical machine learning},
  author={Warnat-Herresthal, Stefanie and Schultze, Hartmut and Shastry, Krishnaprasad Lingadahalli and Manamohan, Sathyanarayanan and Mukherjee, Saikat and Garg, Vishesh and Sarveswara, Ravi and H{\"a}ndler, Kristian and Pickkers, Peter and Aziz, N Ahmad and others},
  journal={Nature},
  volume={594},
  number={7862},
  pages={265--270},
  year={2021},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{meng2021knowledge,
  title={Knowledge distillation in medical data mining: a survey},
  author={Meng, Hefeng and Lin, Zhiqiang and Yang, Fan and Xu, Yonghui and Cui, Lizhen},
  booktitle={5th International Conference on Crowd Science and Engineering},
  pages={175--182},
  year={2021}
}

@inproceedings{zhang2021survey,
  title={A Survey on Knowledge Enhanced EHR Data Mining},
  author={Zhang, Jiancheng and Yang, Xiao and Meng, Hefeng and Lin, Zhiqiang and Xu, Yonghui and Cui, Lizhen},
  booktitle={5th International Conference on Crowd Science and Engineering},
  pages={124--131},
  year={2021}
}

@article{li2020review,
  title={A review of applications in federated learning},
  author={Li, Li and Fan, Yuxi and Tse, Mike and Lin, Kuo-Yi},
  journal={Computers \& Industrial Engineering},
  volume={149},
  pages={106854},
  year={2020},
  publisher={Elsevier}
}

@article{issa2023blockchain,
  title={Blockchain-based federated learning for securing internet of things: A comprehensive survey},
  author={Issa, Wael and Moustafa, Nour and Turnbull, Benjamin and Sohrabi, Nasrin and Tari, Zahir},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--43},
  year={2023},
  publisher={ACM New York, NY}
}

@article{rodriguez2023survey,
  title={Survey on federated learning threats: Concepts, taxonomy on attacks and defences, experimental study and challenges},
  author={Rodr{\'\i}guez-Barroso, Nuria and Jim{\'e}nez-L{\'o}pez, Daniel and Luz{\'o}n, M Victoria and Herrera, Francisco and Mart{\'\i}nez-C{\'a}mara, Eugenio},
  journal={Information Fusion},
  volume={90},
  pages={148--173},
  year={2023},
  publisher={Elsevier}
}

@article{banabilah2022federated,
  title={Federated learning review: Fundamentals, enabling technologies, and future applications},
  author={Banabilah, Syreen and Aloqaily, Moayad and Alsayed, Eitaa and Malik, Nida and Jararweh, Yaser},
  journal={Information processing \& management},
  volume={59},
  number={6},
  pages={103061},
  year={2022},
  publisher={Elsevier}
}

@inproceedings{gao2022feddc,
  title={Feddc: Federated learning with non-iid data via local drift decoupling and correction},
  author={Gao, Liang and Fu, Huazhu and Li, Li and Chen, Yingwen and Xu, Ming and Xu, Cheng-Zhong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10112--10121},
  year={2022}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{guo2022promptfl,
  title={PromptFL: Let Federated Participants Cooperatively Learn Prompts Instead of Models--Federated Learning in Age of Foundation Model},
  author={Guo, Tao and Guo, Song and Wang, Junxiao and Xu, Wenchao},
  journal={arXiv preprint arXiv:2208.11625},
  year={2022}
}

@inproceedings{Honglinyuan2022,
  author    = {Honglin Yuan and
               Warren Richard Morningstar and
               Lin Ning and
               Karan Singhal},
  title     = {What Do We Mean by Generalization in Federated Learning?},
  booktitle = {The Tenth International Conference on Learning Representations, {ICLR}
               2022, Virtual Event, April 25-29, 2022},
  publisher = {OpenReview.net},
  year      = {2022},
  url       = {https://openreview.net/forum?id=VimqQq-i\_Q},
  timestamp = {Sat, 20 Aug 2022 01:15:42 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/0002MNS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{tenney2019bert,
  title={BERT Rediscovers the Classical NLP Pipeline},
  author={Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={4593--4601},
  year={2019}
}

@article{han2022survey,
  title={A survey on vision transformer},
  author={Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and others},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={1},
  pages={87--110},
  year={2022},
  publisher={IEEE}
}

@article{sarker2021machine,
  title={Machine learning: Algorithms, real-world applications and research directions},
  author={Sarker, Iqbal H},
  journal={SN computer science},
  volume={2},
  number={3},
  pages={160},
  year={2021},
  publisher={Springer}
}

@article{chen2022metafed,
  title={Metafed: Federated learning among federations with cyclic knowledge distillation for personalized healthcare},
  author={Chen, Yiqiang and Lu, Wang and Qin, Xin and Wang, Jindong and Xie, Xing},
  journal={arXiv preprint arXiv:2206.08516},
  year={2022}
}

@article{liu2022sphereface,
  title={Sphereface revived: Unifying hyperspherical face recognition},
  author={Liu, Weiyang and Wen, Yandong and Raj, Bhiksha and Singh, Rita and Weller, Adrian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={2},
  pages={2458--2474},
  year={2022},
  publisher={IEEE}
}

@article{van2023chatgpt,
  title={ChatGPT: five priorities for research},
  author={van Dis, Eva AM and Bollen, Johan and Zuidema, Willem and van Rooij, Robert and Bockting, Claudi L},
  journal={Nature},
  volume={614},
  number={7947},
  pages={224--226},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{liu2022distributed,
  title={From distributed machine learning to federated learning: A survey},
  author={Liu, Ji and Huang, Jizhou and Zhou, Yang and Li, Xuhong and Ji, Shilei and Xiong, Haoyi and Dou, Dejing},
  journal={Knowledge and Information Systems},
  volume={64},
  number={4},
  pages={885--917},
  year={2022},
  publisher={Springer}
}

@article{sattler2019robust,
  title={Robust and communication-efficient federated learning from non-iid data},
  author={Sattler, Felix and Wiedemann, Simon and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={IEEE transactions on neural networks and learning systems},
  volume={31},
  number={9},
  pages={3400--3413},
  year={2019},
  publisher={IEEE}
}

@article{wang2022generalizing,
  title={Generalizing to unseen domains: A survey on domain generalization},
  author={Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Qin, Tao and Lu, Wang and Chen, Yiqiang and Zeng, Wenjun and Yu, Philip},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2022},
  publisher={IEEE}
}

@inproceedings{gupta2022fl,
  title={FL Games: A Federated Learning Framework for Distribution Shifts},
  author={Gupta, Sharut and Ahuja, Kartik and Havaei, Mohammad and Chatterjee, Niladri and Bengio, Yoshua},
  year=2022,
  booktitle={Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction with NeurIPS 2022)}
}

@article{tenison2022gradient,
  title={Gradient masked averaging for federated learning},
  author={Tenison, Irene and Sreeramadas, Sai Aravind and Mugunthan, Vaikkunth and Oyallon, Edouard and Belilovsky, Eugene and Rish, Irina},
  journal={arXiv preprint arXiv:2201.11986},
  year={2022}
}

@inproceedings{qu2022generalized,
  title={Generalized federated learning via sharpness aware minimization},
  author={Qu, Zhe and Li, Xingyu and Duan, Rui and Liu, Yao and Tang, Bo and Lu, Zhuo},
  booktitle={International Conference on Machine Learning},
  pages={18250--18280},
  year={2022},
  organization={PMLR}
}

@inproceedings{caldarola2022improving,
  title={Improving generalization in federated learning by seeking flat minima},
  author={Caldarola, Debora and Caputo, Barbara and Ciccone, Marco},
  booktitle={Computer Vision--ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23--27, 2022, Proceedings, Part XXIII},
  pages={654--672},
  year={2022},
  organization={Springer}
}

@inproceedings{foretsharpness,
  title={Sharpness-aware Minimization for Efficiently Improving Generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{arjovsky2020invariant,
  title={Invariant Risk Minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={stat},
  volume={1050},
  pages={27},
  year={2020}
}

@article{gardner1998artificial,
  title={Artificial neural networks (the multilayer perceptron)—a review of applications in the atmospheric sciences},
  author={Gardner, Matt W and Dorling, SR},
  journal={Atmospheric environment},
  volume={32},
  number={14-15},
  pages={2627--2636},
  year={1998},
  publisher={Elsevier}
}

@article{alom2018history,
  title={The history began from alexnet: A comprehensive survey on deep learning approaches},
  author={Alom, Md Zahangir and Taha, Tarek M and Yakopcic, Christopher and Westberg, Stefan and Sidike, Paheding and Nasrin, Mst Shamima and Van Esesn, Brian C and Awwal, Abdul A S and Asari, Vijayan K},
  journal={arXiv preprint arXiv:1803.01164},
  year={2018}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{yuan2021tokens,
  title={Tokens-to-token vit: Training vision transformers from scratch on imagenet},
  author={Yuan, Li and Chen, Yunpeng and Wang, Tao and Yu, Weihao and Shi, Yujun and Jiang, Zi-Hang and Tay, Francis EH and Feng, Jiashi and Yan, Shuicheng},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={558--567},
  year={2021}
}

@inproceedings{sun2019fine,
  title={How to fine-tune bert for text classification?},
  author={Sun, Chi and Qiu, Xipeng and Xu, Yige and Huang, Xuanjing},
  booktitle={Chinese Computational Linguistics: 18th China National Conference, CCL 2019, Kunming, China, October 18--20, 2019, Proceedings 18},
  pages={194--206},
  year={2019},
  organization={Springer}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@inproceedings{lee2023image,
  title={Image-free Domain Generalization via CLIP for 3D Hand Pose Estimation},
  author={Lee, Seongyeong and Park, Hansoo and Kim, Dong Uk and Kim, Jihyeon and Boboev, Muhammadjon and Baek, Seungryul},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={2934--2944},
  year={2023}
}



@article{wenxinhou22,
  author    = {Wenxin Hou and
               Han Zhu and
               Yidong Wang and
               Jindong Wang and
               Tao Qin and
               Renjun Xu and
               Takahiro Shinozaki},
  title     = {Exploiting Adapters for Cross-Lingual Low-Resource Speech Recognition},
  journal   = {{IEEE} {ACM} Trans. Audio Speech Lang. Process.},
  volume    = {30},
  pages     = {317--329},
  year      = {2022},
  url       = {https://doi.org/10.1109/TASLP.2021.3138674},
  doi       = {10.1109/TASLP.2021.3138674},
  timestamp = {Tue, 08 Feb 2022 10:43:28 +0100},
  biburl    = {https://dblp.org/rec/journals/taslp/HouZWWQXS22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zhang2018generalized,
  title={Generalized cross entropy loss for training deep neural networks with noisy labels},
  author={Zhang, Zhilu and Sabuncu, Mert},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{venkateswara2017deep,
  title={Deep hashing network for unsupervised domain adaptation},
  author={Venkateswara, Hemanth and Eusebio, Jose and Chakraborty, Shayok and Panchanathan, Sethuraman},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5018--5027},
  year={2017}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{dosovitskiyimage,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
year={2021}
}

@inproceedings{huang2020self,
  title={Self-challenging improves cross-domain generalization},
  author={Huang, Zeyi and Wang, Haohan and Xing, Eric P and Huang, Dong},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part II 16},
  pages={124--140},
  year={2020},
  organization={Springer}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{agostinelli2023musiclm,
  title={MusicLM: Generating Music From Text},
  author={Agostinelli, Andrea and Denk, Timo I and Borsos, Zal{\'a}n and Engel, Jesse and Verzetti, Mauro and Caillon, Antoine and Huang, Qingqing and Jansen, Aren and Roberts, Adam and Tagliasacchi, Marco and others},
  journal={arXiv preprint arXiv:2301.11325},
  year={2023}
}

