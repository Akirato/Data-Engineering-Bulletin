\documentclass[11pt]{article}
%
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
%\usepackage{deauthor}
%\usepackage{times,graphicx}
%
%% user packages
%\usepackage{natbib}
%    \renewcommand{\bibsection}{\subsubsection*{References}}
%
%\usepackage{todonotes}
%\usepackage{pifont}
%\newcommand{\cmark}{\ding{51}}
%\newcommand{\xmark}{\ding{55}}
%\usepackage{multirow}
%\usepackage{booktabs}
%\usepackage{caption,subcaption}
%\usepackage{graphicx}
%
%\usepackage{mathtools}
%\usepackage{tikz}
%\usepackage{hyperref}       % hyperlinks
%\usepackage{url}            % simple URL typesetting       % professional-quality tables
%%\usepackage{amsmath, amsfonts, amssymb, amsthm}       % blackboard math symbols
%\usepackage{amsmath, amsfonts, amssymb}
%\usepackage{cleveref}
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{microtype}      % microtypography
%\usepackage{xcolor}         % colors
%\usepackage{bbm}
%\usepackage{algorithm, algorithmic}
%% %\usepackage{algpseudocode, algorithmicx}
%% %\renewcommand{\algorithmicrequire}{\textbf{Input:}}
%\usepackage{placeins}
%\usepackage{tabularx}
%\usepackage{makecell}
 
 
 %\newtheorem{remark}{Remark}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % THEOREMS
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
% %\theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}



\title{Accelerated Federated Optimization with Quantization}

\author{Yeojoon Youn$^{\dagger}$
\hspace{2em} Bhuvesh Kumar$^{\dagger}$
\hspace{2em} Jacob Abernethy$^{\dagger}$ \\
$^{\dagger}$ Georgia Institute of Technology \hspace{1em}
\texttt{\small\{yjyoun92,bhuvesh,prof\}@gatech.edu}
}


\begin{document}

\maketitle

\input{submissions/YeojoonYoun/abstract}

\input{submissions/YeojoonYoun/introduction}

\input{submissions/YeojoonYoun/related_works}

\input{submissions/YeojoonYoun/problem_setup}

\input{submissions/YeojoonYoun/FedAQ}

\input{submissions/YeojoonYoun/convergence_analysis}

\input{submissions/YeojoonYoun/experiment}

\input{submissions/YeojoonYoun/conclusion}

%\small
%\bibliographystyle{abbrv} %abbrv
%\bibliography{submissions/YeojoonYoun/ref}

\begin{thebibliography}{10}

\bibitem{alistarh2017qsgd}
D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock {\em Advances in Neural Information Processing Systems},
  30:1709--1720, 2017.

\bibitem{bansal2019potential}
N.~Bansal and A.~Gupta.
\newblock Potential-function proofs for gradient methods.
\newblock {\em Theory of Computing}, 15(1):1--32, 2019.

\bibitem{basu2019qsparse}
D.~Basu, D.~Data, C.~Karakus, and S.~Diggavi.
\newblock Qsparse-local-sgd: Distributed sgd with quantization, sparsification,
  and local computations.
\newblock {\em arXiv preprint arXiv:1906.02367}, 2019.

\bibitem{bernstein2018signsgd}
J.~Bernstein, Y.-X. Wang, K.~Azizzadenesheli, and A.~Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In {\em International Conference on Machine Learning}, pages
  560--569. PMLR, 2018.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{ghadimi2012optimal}
S.~Ghadimi and G.~Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock {\em SIAM Journal on Optimization}, 22(4):1469--1492, 2012.

\bibitem{haddadpour2019local}
F.~Haddadpour, M.~M. Kamani, M.~Mahdavi, and V.~Cadambe.
\newblock Local sgd with periodic averaging: Tighter analysis and adaptive
  synchronization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11082--11094, 2019.

\bibitem{haddadpour2019trading}
F.~Haddadpour, M.~M. Kamani, M.~Mahdavi, and V.~Cadambe.
\newblock Trading redundancy for communication: Speeding up distributed sgd for
  non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  2545--2554. PMLR, 2019.

\bibitem{haddadpour2021federated}
F.~Haddadpour, M.~M. Kamani, A.~Mokhtari, and M.~Mahdavi.
\newblock Federated learning with compression: Unified analysis and sharp
  guarantees.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2350--2358. PMLR, 2021.

\bibitem{haddadpour2019convergence}
F.~Haddadpour and M.~Mahdavi.
\newblock On the convergence of local descent methods in federated learning.
\newblock {\em arXiv preprint arXiv:1910.14425}, 2019.

\bibitem{horvath2019natural}
S.~Horvath, C.-Y. Ho, L.~Horvath, A.~N. Sahu, M.~Canini, and P.~Richt{\'a}rik.
\newblock Natural compression for distributed deep learning.
\newblock {\em arXiv preprint arXiv:1905.10988}, 2019.

\bibitem{kairouz2019advances}
P.~Kairouz, H.~B. McMahan, B.~Avent, A.~Bellet, M.~Bennis, A.~N. Bhagoji,
  K.~Bonawitz, Z.~Charles, G.~Cormode, R.~Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock {\em arXiv preprint arXiv:1912.04977}, 2019.

\bibitem{karimireddy2020mime}
S.~P. Karimireddy, M.~Jaggi, S.~Kale, M.~Mohri, S.~J. Reddi, S.~U. Stich, and
  A.~T. Suresh.
\newblock Mime: Mimicking centralized stochastic algorithms in federated
  learning.
\newblock {\em arXiv preprint arXiv:2008.03606}, 2020.

\bibitem{karimireddy2020scaffold}
S.~P. Karimireddy, S.~Kale, M.~Mohri, S.~Reddi, S.~Stich, and A.~T. Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In {\em International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020.

\bibitem{khaled2020tighter}
A.~Khaled, K.~Mishchenko, and P.~Richt{\'a}rik.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 4519--4529. PMLR, 2020.

\bibitem{konevcny2016federated}
J.~Kone{\v{c}}n{\`y}, H.~B. McMahan, F.~X. Yu, P.~Richt{\'a}rik, A.~T. Suresh,
  and D.~Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}, 2016.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Manuscript, 2009.

\bibitem{lecun1998mnist}
Y.~LeCun.
\newblock The mnist database of handwritten digits.
\newblock {\em http://yann. lecun. com/exdb/mnist/}, 1998.

\bibitem{li2020federated}
T.~Li, A.~K. Sahu, A.~Talwalkar, and V.~Smith.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock {\em IEEE Signal Processing Magazine}, 37(3):50--60, 2020.

\bibitem{li2018federated}
T.~Li, A.~K. Sahu, M.~Zaheer, M.~Sanjabi, A.~Talwalkar, and V.~Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock {\em arXiv preprint arXiv:1812.06127}, 2018.

\bibitem{li2019convergence}
X.~Li, K.~Huang, W.~Yang, S.~Wang, and Z.~Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock {\em arXiv preprint arXiv:1907.02189}, 2019.

\bibitem{li2022distributed}
X.~Li, B.~Karimi, and P.~Li.
\newblock On distributed adaptive optimization with gradient compression.
\newblock {\em arXiv preprint arXiv:2205.05632}, 2022.

\bibitem{li2020acceleration}
Z.~Li, D.~Kovalev, X.~Qian, and P.~Richt{\'a}rik.
\newblock Acceleration for compressed gradient descent in distributed and
  federated optimization.
\newblock {\em arXiv preprint arXiv:2002.11364}, 2020.

\bibitem{li2021canita}
Z.~Li and P.~Richt{\'a}rik.
\newblock Canita: Faster rates for distributed convex optimization with
  communication compression.
\newblock {\em arXiv preprint arXiv:2107.09461}, 2021.

\bibitem{lin2018don}
T.~Lin, S.~U. Stich, K.~K. Patel, and M.~Jaggi.
\newblock Don't use large mini-batches, use local sgd.
\newblock {\em arXiv preprint arXiv:1808.07217}, 2018.

\bibitem{mcmahan2017communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem{reisizadeh2020fedpaq}
A.~Reisizadeh, A.~Mokhtari, H.~Hassani, A.~Jadbabaie, and R.~Pedarsani.
\newblock Fedpaq: A communication-efficient federated learning method with
  periodic averaging and quantization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2021--2031. PMLR, 2020.

\bibitem{rothchild2020fetchsgd}
D.~Rothchild, A.~Panda, E.~Ullah, N.~Ivkin, I.~Stoica, V.~Braverman,
  J.~Gonzalez, and R.~Arora.
\newblock Fetchsgd: Communication-efficient federated learning with sketching.
\newblock In {\em International Conference on Machine Learning}, pages
  8253--8265. PMLR, 2020.

\bibitem{singh2021squarm}
N.~Singh, D.~Data, J.~George, and S.~Diggavi.
\newblock Squarm-sgd: Communication-efficient momentum sgd for decentralized
  optimization.
\newblock {\em IEEE Journal on Selected Areas in Information Theory}, 2021.

\bibitem{stich2018local}
S.~U. Stich.
\newblock Local sgd converges fast and communicates little.
\newblock {\em arXiv preprint arXiv:1805.09767}, 2018.

\bibitem{stich2019error}
S.~U. Stich and S.~P. Karimireddy.
\newblock The error-feedback framework: Better rates for sgd with delayed
  gradients and compressed communication.
\newblock {\em arXiv preprint arXiv:1909.05350}, 2019.

\bibitem{suresh2017distributed}
A.~T. Suresh, X.~Y. Felix, S.~Kumar, and H.~B. McMahan.
\newblock Distributed mean estimation with limited communication.
\newblock In {\em International Conference on Machine Learning}, pages
  3329--3337. PMLR, 2017.

\bibitem{vogels2019powersgd}
T.~Vogels, S.~P. Karinireddy, and M.~Jaggi.
\newblock Powersgd: Practical low-rank gradient compression for distributed
  optimization.
\newblock {\em Advances In Neural Information Processing Systems 32 (Nips
  2019)}, 32(CONF), 2019.

\bibitem{wang2018atomo}
H.~Wang, S.~Sievert, Z.~Charles, S.~Liu, S.~Wright, and D.~Papailiopoulos.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock {\em arXiv preprint arXiv:1806.04090}, 2018.

\bibitem{wang2021field}
J.~Wang, Z.~Charles, Z.~Xu, G.~Joshi, H.~B. McMahan, M.~Al-Shedivat, G.~Andrew,
  S.~Avestimehr, K.~Daly, D.~Data, et~al.
\newblock A field guide to federated optimization.
\newblock {\em arXiv preprint arXiv:2107.06917}, 2021.

\bibitem{wang2018cooperative}
J.~Wang and G.~Joshi.
\newblock Cooperative sgd: A unified framework for the design and analysis of
  communication-efficient sgd algorithms.
\newblock {\em arXiv preprint arXiv:1808.07576}, 2018.

\bibitem{wang2021local}
J.~Wang, Z.~Xu, Z.~Garrett, Z.~Charles, L.~Liu, and G.~Joshi.
\newblock Local adaptivity in federated learning: Convergence and consistency.
\newblock {\em arXiv preprint arXiv:2106.02305}, 2021.

\bibitem{wang2022communication}
Y.~Wang, L.~Lin, and J.~Chen.
\newblock Communication-efficient adaptive federated learning.
\newblock {\em arXiv preprint arXiv:2205.02719}, 2022.

\bibitem{wangni2017gradient}
J.~Wangni, J.~Wang, J.~Liu, and T.~Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock {\em arXiv preprint arXiv:1710.09854}, 2017.

\bibitem{woodworth2020local}
B.~Woodworth, K.~K. Patel, S.~Stich, Z.~Dai, B.~Bullins, B.~Mcmahan, O.~Shamir,
  and N.~Srebro.
\newblock Is local sgd better than minibatch sgd?
\newblock In {\em International Conference on Machine Learning}, pages
  10334--10343. PMLR, 2020.

\bibitem{yu2019linear}
H.~Yu, R.~Jin, and S.~Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  7184--7193. PMLR, 2019.

\bibitem{yu2019parallel}
H.~Yu, S.~Yang, and S.~Zhu.
\newblock Parallel restarted sgd with faster convergence and less
  communication: Demystifying why model averaging works for deep learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 5693--5700, 2019.

\bibitem{yuan2020federated}
H.~Yuan and T.~Ma.
\newblock Federated accelerated stochastic gradient descent.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\end{thebibliography}


\end{document}

