\section{Problem Setup} \label{problem_setup}

In this paper, we build our algorithm based on federated learning with captain-worker topology where $M$ local devices contain their own local data, and a server aggregates local parameter updates without sharing any data during synchronization rounds. Since we focus on \emph{homogeneous} local data distribution settings for the convergence analysis of our algorithm, we define the distributed stochastic optimization problem as below.
\begin{align*}
    \min_{w \in \mathbb{R}^d} F(w) := \mathbb{E}_{z\sim\mathcal{D}}[f(w;z)]
\end{align*}
In our convergence analysis, we assume $F$ is \emph{strongly-convex}. Each client can access $F$ at $w$ via oracle $\nabla f(w;z)$ because all clients have the same loss function $f$. Also, every local device has the same local data distribution $\mathcal{D}$. Moreover, we use the \emph{full participation} of nodes for local updates and synchronizations. 

\subsection{Assumptions}

Let us clarify assumptions on the unbiased quantizer $Q$, the global objective function $F$, and the unbiased gradient estimator $\nabla f$.

\begin{assumption} \label{assumption1}
The variance of the unbiased quantizer $Q$ is bounded by the squared of $l_2$-norm of its argument, i.e., $\mathbb{E}[Q(x)|x]=x, \textrm{ } \mathbb{E}[\|Q(x) - x\|^2 |x] \leq q \|x\|^2$.
\end{assumption}
For example, a well-known randomized quantizer which satisfies Assumption \ref{assumption1} is low-precision quantizer in \citet{alistarh2017qsgd}.

\textbf{Example 1.} (Low-precision quantizer) Given $x \in \mathbb{R}^d$, the quantizer $Q:\mathbb{R}^d \rightarrow \mathbb{R}^d$ is defined by
\begin{align*}
    Q_i(x) = \textrm{sign}(x_i)\cdot\|x\|\cdot \xi_i(x, s),   \textrm{ } \textrm{ } i \in [d]
\end{align*}
$\xi_i$ is defined as below.
$$\xi_i(x, s)=
\begin{cases}
\frac{l+1}{s},~\textrm{with probability } \frac{|x_i|}{\|x\|}s - l\\
\frac{l}{s},~\textrm{ } \textrm{ } \textrm{ o/w}
\end{cases}$$
$s$ is the number of quantization levels. $l \in [0, s)$ is an integer which satisfies $\frac{|x_i|}{\|x\|} \in [\frac{l}{s}, \frac{l+1}{s})$.

\begin{assumption} \label{assumption2}
F is $\mu$-strongly convex, i.e., $F(w_1) \geq F(w_2) + \langle \nabla F(w_2), w_1-w_2 \rangle + \frac{1}{2}\mu \|w_1-w_2\|^2$ for any $w_1, w_2 \in \mathbb{R}^d$.
\end{assumption}

\begin{assumption} \label{assumption3}
F is L-smooth, i.e., $F(w_1) \leq F(w_2) + \langle \nabla F(w_2), w_1-w_2 \rangle + \frac{1}{2}L \|w_1-w_2\|^2$ for any $w_1, w_2 \in \mathbb{R}^d$.
\end{assumption}

\begin{assumption} \label{assumption4}
$\nabla f(w; \xi)$ is unbiased and variance bounded, i.e., $\mathbb{E}_\xi[\nabla f(w; \xi)] = \nabla F(w)$, $\mathbb{E}_\xi[\|\nabla f(w; \xi) - \nabla F(w)\|^2] \leq \sigma^2$ for any $w \in \mathbb{R}^d$.
\end{assumption}


\subsection{Notation}

We use $\tau, K$ to respectively denote the number of local updates and total communication rounds, which means the total number of iterations $T$ at each node satisfies $T = K\tau$. Since we consider a strongly-convex case, we can find the optimal point $w^*$ and denote the optimal function value as $F^* := F(w^*)$. The local parameter $w_{k, t}^m$ indicates the parameter of the $m$-th local model after $k$th synchronization followed by $t$ local SGD updates. There are other types of parameters such as $w_{k, t}^{\textrm{ag}, m} \textrm{ and } w_{k, t}^{\textrm{md}, m}$, and we obtain two types of parameters $w_k$ and $w_k^{\textrm{ag}}$ in the server side after $k$th synchronization. More details on these parameters will be discussed in the next section.