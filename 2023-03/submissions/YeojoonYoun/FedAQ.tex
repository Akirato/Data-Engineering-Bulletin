\section{FedAQ Algorithm}

We propose a novel communication efficient algorithm that combines an accelerated variant of federated averaging and an efficient quantization scheme. Our FedAQ algorithm has two main parts: (1) multiple accelerated local updates and (2) communication with quantization. Both components contribute to achieving better communication efficiency than other previous federated algorithms. The entire process is summarized in Algorithm \ref{algorithm1}. %(See Appx.~\ref{app:fedaq_alg}).

\subsection{Multiple Accelerated Local Updates}

The FedAvg algorithm, proposed by \cite{mcmahan2017communication}, is widely used for federated learning to improve communication efficiency by reducing communication rounds with multiple local SGD updates. \cite{yuan2020federated} provide FedAC that replaces the stochastic gradient updates of FedAvg by accelerated version of SGD by \cite{ghadimi2012optimal} resulting in a linear speedup in $M$ with fewer communication rounds than FedAvg.

Thus, we apply the FedAC scheme to multiple updates of each local model. Since previous quantization-based federated optimization algorithms are FedAvg variants with no acceleration, the accelerated method enables our algorithm to gain better communication efficiency than others.

As you can see in Algorithm \ref{algorithm1}, we need two more local parameters $w_{k, t}^{\textrm{ag}, m}$ and $w_{k, t}^{\textrm{md}, m}$ for acceleration in addition to the main local parameter $w_{k, t}^m$. $w_{k,t}^{\textrm{ag},m}$ aggregates the past iterates, and the gradients are queried at the auxiliary parameter $w_{k,t}^{\textrm{md},m}$. While typical FL algorithms without acceleration only have a learning rate $\eta$ as their hyperparameter, the general acceleration scheme makes our algorithm flexible due to four hyperparameters $\alpha, \beta, \eta, \gamma$. $\alpha, \beta$ are hyperparameters related to coupling coefficients, and $\eta, \gamma$ stand for learning rates respectively for $w_{k, t}^{\textrm{ag}, m}, w_{k, t}^m$. The flexibility of hyperparameters enables the fast convergence speed of FedAQ, but naively chosen hyperparameters also cause unstable training of FedAQ. We discuss the exact choice of hyperparameters in \cref{convergence_analysis}. Unlike FedAC, that requires each client to communicate the exact iterates to the server with high precision, we discuss in the following subsection how FedAQ incorporates quantization techniques to reduce communication cost.


%Thus, we carefully determine two parameter condition sets that theoretically ensure the convergence guarantees. The first one is
% \begin{align} \label{parameter_FedAQ}
%     \eta, \gamma \in \Big( 0, \frac{1}{L} \Big], \gamma = \max \Big( \sqrt{\frac{\eta}{\mu\tau}}, \eta \Big), \alpha = \frac{1}{\gamma\mu}, \beta = \alpha + 1
% \end{align}
% We add one more condition $\gamma \in (0, \frac{1}{L}]$ to the FedAC-I condition \cite{yuan2020federated} and create our parameter condition set (\ref{parameter_FedAQ}). The second one is
% \begin{align} \label{parameter2_FedAQ}
%     \eta, \gamma \in \Big( 0, \frac{1}{L} \Big], \gamma = \max \Big( \sqrt{\frac{\eta}{\mu\tau}}, \eta \Big), \alpha=\frac{3}{2\gamma\mu} - \frac{1}{2}, \beta = \frac{2\alpha^2-1}{\alpha-1}, \gamma\mu \leq \frac{3}{4}
% \end{align}
% We add two more conditions $\gamma \in (0, \frac{1}{L}]$ and $\gamma\mu \leq \frac{3}{4}$ to the FedAC-II condition to build our parameter condition set (\ref{parameter2_FedAQ}). Even though quantization adds complexity to the algorithm, these weak assumptions are the only additional requirements for showing the convergence of FedAQ. More details on how the above conditions help the convergence proof of FedAQ are shown in Appx.~\ref{app:proof_lemma} and Appx.~\ref{app:proof_lemma2}.

\subsection{Communication with Quantization}

In cross-device federated learning, a large amount of communicated messages from a number of devices and the limited communication bandwidth can lead to severe communication bottlenecks. Therefore, in this scenario, an efficient quantization scheme can significantly reduce the size of communicated messages and make communication between local devices and a server faster. We apply the same unbiased quantizer used in \cite{haddadpour2021federated} that satisfies Assumption \ref{assumption1}.

In contrast with other quantization-based federated optimization algorithms \cite{reisizadeh2020fedpaq, haddadpour2021federated}, the algorithmic novelty of FedAQ is based on applying quantization to two model parameter updates, which is required in order to simultaneously reduce the frequency of communication and the volume of communicated bits. To the best of our knowledge, this is the first quantization-based method that achieves the accelerated rate with the dramatic reduction in communication cost.
% federated optimization  where quantization is used for two model updates during a synchronization step with theoretical guarantees of how acceleration reduces the number of communication rounds to achieve a linear speedup for quantization-based methods.
To be specific on the communication process, after each client $m$ obtains $w_{k, \tau}^m, w_{k, \tau}^{\textrm{ag}, m}$ through $\tau$ accelerated local iterations, each client quantizes the difference between $w_{k, \tau}^m, w_{k, \tau}^{\textrm{ag}, m}$ and the most recent server models $w_k, w_k^{\textrm{ag}}$. Then, a server aggregates $Q(w_{k, \tau}^m - w_k), Q(w_{k, \tau}^{\textrm{ag}, m} - w_k^{\textrm{ag}})$ from all clients. After dequantizing those messages, the server obtains the following new models $w_{k+1}, w_{k+1}^{\textrm{ag}}$ and broadcasts them back to each client.
% \begin{align*}
%     w_{k+1} &= w_k + \frac{1}{M}\sum_{m=1}^M Q(w_{k, \tau}^m - w_k)\\
%     w_{k+1}^{\textrm{ag}} &= w_k^{\textrm{ag}} + \frac{1}{M} \sum_{m=1}^M Q(w_{k, \tau}^{\textrm{ag}, m} - w_k^{\textrm{ag}})
% \end{align*}

\begin{algorithm} %[H]
\caption{Federated Accelerated SGD with Quantization (FedAQ)}\label{algorithm1}
\begin{algorithmic}[1]
\State{\bfseries Input:} $\alpha, \beta, \eta, \gamma$, initial vector $w_0 = w_{0, 0}^{\textrm{ag}, m} = w_{0, 0}^m$ for all devices $m \in [M]$
\For{$k = 0, \cdots, K-1$}
\For{each client $m$ in parallel}
\State $w_{k, 0}^m \leftarrow w_k, w_{k, 0}^{\textrm{ag}, m} \leftarrow w_k^{\textrm{ag}}$
\For{$t = 0, \cdots, \tau-1$}
\State $w_{k, t}^{\textrm{md}, m} \leftarrow \beta^{-1}w_{k, t}^m + (1-\beta^{-1})w_{k, t}^{\textrm{ag}, m}$
\State $g_{k, t}^m \leftarrow \nabla f(w_{k, t}^{\textrm{md}, m}, \xi_{k, t}^m)$
\State $w_{k, t+1}^{\textrm{ag}, m} \leftarrow w_{k, t}^{\textrm{md}, m} - \eta g_{k, t}^m $
\State $w_{k, t+1}^m \leftarrow (1-\alpha^{-1})w_{k, t}^m + \alpha^{-1} w_{k, t}^{\textrm{md}, m} - \gamma g_{k, t}^m$
\EndFor
\State send $Q(w_{k, \tau}^m - w_k), Q(w_{k, \tau}^{\textrm{ag}, m} - w_k^{\textrm{ag}})$
\EndFor
\State server finds $w_{k+1} \leftarrow w_k + \frac{1}{M}\sum\limits_{m=1}^M Q(w_{k, \tau}^m - w_k), \textrm{ } w_{k+1}^{\textrm{ag}} \leftarrow w_k^{\textrm{ag}} + \frac{1}{M} \sum\limits_{m=1}^M Q(w_{k, \tau}^{\textrm{ag}, m} - w_k^{\textrm{ag}})$
\EndFor

\end{algorithmic}
\end{algorithm} 