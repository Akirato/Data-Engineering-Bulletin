\section{Related Works}

The first guarantee for FedAvg, showing that it converges at the same rate as mini-batch SGD in strongly convex scenarios, was shown by \cite{stich2018local} in the IID setting. The further convergence analysis of FedAvg for non-convex functions was laid out in a number of published works \cite{wang2018cooperative, haddadpour2019trading, yu2019parallel}. Followup work has managed to remove unnecessary assumptions, such as uniformly bounded gradients, to achieve better convergence rates \cite{wang2018cooperative, stich2019error, haddadpour2019local, khaled2020tighter, woodworth2020local}. Moreover, \cite{li2018federated, haddadpour2019convergence, li2019convergence, khaled2020tighter, karimireddy2020scaffold} define scenarios that depart from the IID framework, analyzing the convergence of FedAvg and its variants in settings with heterogeneous data distributions.

Reducing the transmitted bits between a server and clients through compression techniques is pivotal to saving communication costs in federated learning. This motivates researchers to develop various compression techniques such as sparsification and quantization without significantly sacrificing accuracy \cite{konevcny2016federated, alistarh2017qsgd, suresh2017distributed, wangni2017gradient, bernstein2018signsgd, wang2018atomo, vogels2019powersgd, horvath2019natural, basu2019qsparse, rothchild2020fetchsgd}. \cite{reisizadeh2020fedpaq} show near-optimal theoretical guarantees of the first federated optimization algorithm that incorporates federated averaging, partial node participation, and quantization in homogeneous local data distribution settings. \cite{haddadpour2021federated} further provide improved convergence rates for both homogeneous and heterogeneous settings.

We can achieve better communication efficiency by applying acceleration methods into client updates. \cite{yuan2020federated} have proposed the first provable acceleration of FedAvg that achieves a linear speedup with the fewest communication rounds. Several other works aim to achieve communication efficiency by using momentum or adaptive optimizers \cite{yu2019linear, karimireddy2020mime, wang2021local}. It is important to note that our work is not the first to combine acceleration and quantization.  \cite{li2020acceleration, li2021canita}, for example, propose compressed and accelerated distributed optimization methods that are neither stochastic nor FedAvg variants. \cite{singh2021squarm} propose communication efficient momentum SGD for decentralized optimization. \cite{li2022distributed, wang2022communication} show that distributed and federated versions of adaptive optimizers along with gradient compression can lead to similar convergence rates as their non-compressed counterparts. But these works do not achieve the core result of the present paper, which is the reduced communication complexity via a faster convergence rate and a linear speedup with the small number of communication rounds. To the best of our knowledge, FedAQ is the first accelerated version of federated averaging for master-worker topology that successfully integrates a quantization scheme and provides rigorous convergence guarantees. 