\documentclass[11pt]{article}

\usepackage{deauthor,times,graphicx}
%\usepackage{url}

\begin{document}

Federated learning has attracted much attention recently, as machine learning has been widely applied to many real applications while security concern of data and model has also arisen. In distributed systems, when the server should have interactions with clients, users might prefer the server being not able to access each client's personal raw data. When different data owners want to share insight into their data, they may also share only models instead of raw data. Such practical usage scenarios triggered many interesting new designs of federated learning algorithms and systems, which can be summarized as horizontal federated learning, vertical federated learning, and federated transfer learning. These variants of federated learning have show flexibility and at the same time introduced new theoretical and experimental challenges for deployments of machine learning systems. Particularly, the communication cost in existing federated learning can be a bottleneck for realtime deployment. In this issue, we included several papers covering different perspectives of federated learning which makes it a very interesting one pointing out several potential new directions of the field.

The first paper
{\it Federated Computing: Query, Learning, and Beyond} and
the second paper
{\it Federated Learning without Full Labels: A Survey}
reviewed existing federated computing strategies and particularly on tasks without full labels.
They shed some light on the connection of modern machine learning with traditional database technologies and the future trends of the field.
The following papers
{\it FedCLIP: Fast Generalization and Personalization for CLIP in Federated Learning},
{\it Reconciling Security and Communication Efficiency in Federated Learning}, and
{\it Accelerated Federated Optimization with Quantization}
studied potential new approaches to improve the effectiveness and efficiency of federated learning algorithms.
Particularly, Prasad et al., improved the federated learning with up to 40 compression in uplink
communication with no meaningful loss in utility, and Youn et al., theoretically and experimentally proved that quantization can be used to significantly reduce the communication complexity.
Then,
{\it Federated Truth Discovery for Mobile Crowdsensing with Privacy-Preserving Trustworthiness Assessment},
{\it Federated Ensemble Learning: Increasing the Capacity of Label Private Recommendation Systems}, and
{\it Enhance Mono-modal Sentiment Classification with Federated Cross-modal Transfer}
discussed the applications to mobile crowdsensing, recommender systems, and sentiment analysis.
These applications further demonstrate novel real use of federated learning as well as the need of algorithm adaptation when deploying to different applications.
Last but not least, the paper
{\it NVIDIA FLARE: Federated Learning from Simulation to Real-World}
from NVIDIA has introduced their systematic development of an open-source software development kit that can benefit the whole field for research study and real application development.


I would like to thank all the authors for their contributions, making this issue a significant and interesting discussion about present and future research directions related to federated learning.


\end{document}
