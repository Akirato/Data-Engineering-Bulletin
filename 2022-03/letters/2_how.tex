%!TEX root = paper.tex
\section{Reasoning about the How: The Symptoms of Errors}
\label{sec:how}

In its most general form, information quality is defined as ``fitness for use''~\cite{Strong96}, i.e., its definition focuses on the use case, the application, the \emph{context} of the data at hand. Most, if not all, error detection methods make heavy use of this context. They search for and focus on the \emph{symptoms} of poor data quality and ask the question \emph{how} a particular data element is an error.
We exemplify this insight with selected examples of traditional error detection problems and their solutions.
\begin{description}
  \item[Outliers] Already by definition, outliers can be recognized only in the context of other data elements -- only these ``inliers'' make some other value an outlier. Typical outlier detection methods create a model to represent normal/typical values and mark as outliers all those values that do not fit the model~\cite{Aggarwal2013}. Whether an outlier is, in fact, an error is application-dependent and user-defined.
  
  \emph{How} is an outlier a data error? It is very different from all \emph{other} data elements, suggesting it is not the intended value for this data item.
  
  \item[Constraint violations] Constraints, such as key-constraints, dependencies, or denial constraints can be used to express the validity of a data instance. These rules are specified by experts or discovered with data profiling methods~\cite{Abedjan2018}. Rarely do they refer to individual data elements; rather they forbid the existence of some elements in the presence of others, such as a key-constraint denying any other record with the same key value. Discovering and cleaning such violations is an active research area~\cite{IlyasC15}.
  
  \emph{How} does a data element violate a constraint? It exists in the presence of some \emph{other} data element. While this is not an error on its own, the collection of these values cannot be part of the intended correct data instance. %, together with which it violates the constraint.
  
  \item[Duplicates] Within a dataset, duplicate records are multiple different representations of the same real-world entity~\cite{Naumann10}. To clean a dataset, such erroneous duplicates must be detected and then merged or eliminated. Identifying a duplicated record is, by definition, possible only by regarding other records, i.e., only in the context of the entire relation. Typical approaches intelligently create duplicate candidate pairs and then determine their similarity to decide whether they are indeed duplicates~\cite{Papadakis21}.
  
  \emph{How} is a duplicate an error? It represents the same real-world object as some \emph{other} data element, with possibly other types of errors causing a different representation.
  
  \item[Missing values] Missing values are easy to detect when they appear as null values in databases or empty strings in files. In more complex cases, ``disguised missing values'' can be recognized only by regarding their context, which usually comprises the other values in the column~\cite{FAHES_18}. 
  The typical means to ``clean'' missing values is to impute their value, again based on their context, usually the non-missing values in a column.
  
  \emph{How} is a missing value an error? It is explicitly represented to indicate an error. However, the difficulty in the case of missing value relates to the interpretation of ``null'' as {\em we don't know the value, but we should} as opposed to schema issues, for example, a relational employees table with some employees who do not have middle names.
\end{description}

Data cleaning, as a means to alleviate the symptoms of poor data quality, is an established and important research and development field, relying heavily on the context of data and its use in applications. Next, we move backwards along the data processing pipeline to explore not these symptoms, but their causes.