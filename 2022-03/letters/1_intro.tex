%!TEX root = paper.tex
\section{Introduction: Data Errors and their Root Causes}
\label{sec:intro}

With the recent move towards data-centric AI, data quality is now playing an even bigger role in producing sound and reliable insights, predictions and analytics. While the data management community has been working on the problem of data cleaning for decades, the problem remains very much present. Most efforts have focused on error detection~\cite{DBLP:books/acm/IlyasC19}, attempting to leverage symptoms and the manifestations of these errors in data sets to locate and possibly repair them. 
Indeed, the last few years witnessed significant advances in automating error detection and repairing~\cite{holoclean,holodetect,raha,yeye-unidetect} by probabilistically modelling dirty data sets, and reasoning about error detection and repair as structured prediction problems~\cite{puds,uai_heidari}. In this opinion piece, we present our views on how to further advance the field of data cleaning, and go beyond treating the symptoms of the problem and understand what it takes to treat the causes and the sources of these anomalies and errors.

Tracking errors to their sources is not a new quest of the research community~\cite{DBLP:conf/sigmod/ChalamallaIOP14, DBLP:conf/sigmod/WangM017, DBLP:journals/ftdb/GlavicMR21}. So why are we revisiting it now? %The main observation that motivates this discussion is 
The way current research currently reasons about the root causes of data errors is still, in our opinion, limited. \emph{Tracking errors to sources} has often been framed as  ``computational'' provenance that represents \emph{what} was involved in computing a final data product and possibly \emph{how} this product was computed. The main goal of these provenance-based error tracking systems is projecting errors detected in downstream applications all the way to upstream data sources, where they should be fixed~\cite{DBLP:conf/sigmod/ChalamallaIOP14}.  While the principle is sound, multiple issues often complicate this approach. First, as data processing pipelines become more complex, with cascades of complex machine learning models, capturing this rich provenance information becomes harder, as most if not all of the input is involved in producing the output. Recent progress, however, has been made in tracking responsibility of training data, for example, in the predictions of complex models~\cite{DBLP:journals/corr/abs-2002-08484,pmlr-koh-Liang,DBLP:journals/corr/abs-2202-00622}. Second, even with advances in modelling the responsibility of input sources in the observations in output analytics reports, fixing the sources does not mean that errors have been fixed at their true ``roots'' -- their point of creation; these raw data sources are often the results of other processes not modelled at all in the computational data pipelines, such as grading tasks of humans, sensor readings, and extraction scripts from logs and documents.  

Hence, we argue that effective management of data errors and the next-generation data cleaning systems require a more profound understanding of the root causes data errors. These systems should: (1)~distinguish between \emph{why} errors occur and the processes that generated them in the first place, and \emph{how}  these errors manifest themselves as bad data symptoms (e.g., violations of integrity constraints and appearing as outlying values), and (2)~explicitly model these data generation processes to allow for new process repair actions that go beyond fixing raw data sources. 

% NEED TO INTEGRATE THE FOLLOWING SOMEHOW 
%
%In the absence of any context, a data element\footnote{For now, we formulate our definitions in the most general way by referring to ``data elements'', which can manifest as individual values, rows, columns, objects, graphs, tables or entire datasets. Later, we supply details on this ``what'' question.} cannot constitute an error -- it just ``is''. However, data elements do not exist outside some context: they were somehow produced and shall somehow be consumed. Only this context allows us to identify a data element as erroneous. We can ask \emph{how} the element is an error: how do we notice it as erroneous, what symptoms does it hold? These symptoms exist only in the context of the consumption of the data element. For instance, data might violate some external constraint or assumption. It might be an outlier, compared to other data elements. A relation might be missing some tuples or have some duplicated tuples.

%Given such identified errors, we can ask the obvious next question: \emph{Why} do we observe the error, how did it appear in the data? Possible answers to these questions are the provenance of the data, and the mode in which the data was originally created. For instance, human data entry can create such errors, or faulty sensors produce erroneous data.


