\documentclass[11pt,dvipdfm]{article}
%\documentclass[11pt]{article} The above line must be used for your camera-ready submission, which requires a latex -> DVI -> PDF compilation pipeline.  As a workaround while you are writing your paper, you could comment it out and use this line instead, which is compatible with pdflatex.%
\usepackage{deauthor,times,graphicx,hyperref} 

\begin{document}
\title{AI-Enhanced Adaptive Assistive Technologies: Methods for AI Design Justice}
\author{Nora McDonald*, Aaron Massey**, and Foad Hamidi**\\ 
University of Cincinnati* and University of Maryland, Baltimore County**\\ 
mcdonnan@ucmail.uc.edu, akmassey@umbc.edu, foadhamidi@umbc.edu}


\maketitle
\begin{abstract}
The design of artificial intelligent (AI) enhanced adaptive assistive technologies (AATs) presents exciting promise for those with motor or audio/vision impairment. However, these technologies also introduce tremendous privacy risks, particularly for those with compounding identity vulnerabilities. In this paper, we reflect on why and how AATs need to be designed in collaboration with intersectional AAT users to ensure that the benefits of AI do not sacrifice privacy for the most vulnerable. We discuss methods and tools we have developed to meet these challenges, lessons we have learned from studies with them, and future opportunities.
\end{abstract}

\section{Introduction}
AI systems can compound discriminations of those with disabilities. Based on individual’s social media profiles, smartphones settings, or performance data, one can infer whether individuals are blind \cite{40} or have symptoms of Parkinson’s disease \cite{45}. These data could be used by third-parties in ways that may limit opportunities or lead to other harms for these individuals. For instance, what if a system could determine that you had visual impairments by monitoring and analyzing your typing data? What if those same data could be accessed by an employer, or an insurance company, or a bad actor? Yet those in need of AATs are increasingly reliant on systems that leverage existing data and capabilities (e.g., text, pointing data, natural language processing algorithms) used by popular products like Grammarly. And, arguably, the more adaptative and helpful the AI, the greater the possibility for harm. Providing usable, private, and accessible technology is, of course, critical, but the way in which these populations are the target of discrimination by insurance companies and advertisers and the way those discriminations are compounded by identity must be central to design.

For older adults with disabilities, the emphasis on technology innovation has traditionally been on inclusivity \cite{15,41,42}
but only insofar as we account for differences in ability. HCI is increasingly embracing the way in which technologies must take into account the realities, and the limitations on agency, of diverse users \cite{37,39,54}. Yet scholars have noted that, despite this increasing attention on critical and social justice design, efforts to embrace privacy by design often fail to consider critical alternatives or values, contexts, and structural inequalities \cite{57}. Recent work has pointed out the need to consider the complex and overlapping challenges faced by disabled users \cite{17}. However, more needs to be done to take into account the way that culture, context, power, and identity interact. We place too much onus on the user to make do with the tools they have, rather than soliciting their input from the beginning, and doing so in a way that allows for them to imagine possible futures.  Instead, often designers imagine (or don’t) the challenges face by marginalized users; or disabled users are called upon to just consider their disabilities, as if that were the only feature of their experience. The moment requires a broader focus on other harms that could affect these populations.

One of the biggest challenges for AI scholars is that ethical AI cannot be solved \emph{for} merely with technical approaches \cite{23,56,61} because problems emerge from dominant social and political systems that require social and political awareness, an understanding of power \cite{61}, and data/surveillance capitalism \cite{58,59} or colonialism \cite{12}---terms which broadly refer to the sociotechnical mechanisms of capitalism that sanction the treatment of user data as commodities to mine, extract, trade, exploit, and sell. The critical need to involve more diverse scholars and marginalized users also coincides with methodological approaches like intersectionality that grapple with power and identity. Intersectionality theorizes that identities, which emerge in intersecting power dynamics (e.g., racism, capitalism, gender identity discrimination, age, ableism, etc.) produce a ``matrix'' of oppressions \cite{7}. While these oppressions are unique to the context, they can also share in common (across cultures and contexts) an oppressive relationship between identity and mechanisms of power \cite{8}. 

Against this backdrop, there is growing urgency to consider social justice when designing AI through studies with diverse users and also interventions in the classroom that introduce ethics curriculum to future designers of these systems. However, there are some challenges. For one, AI ethics education still suffers from a lack of attention and coverage in academia \cite{48}. Second, businesses that use and produce AI do not necessarily provide AI ethics training or seem to deeply consider the discriminatory possibilities of the technologies they produce or adopt. Over the last few years, we have seen a number of high profile cases in which technology companies AI are being used for discriminatory practices (e.g., \cite{27})---the examples of discriminatory AI abound in everything from healthcare (e.g., \cite{4,51}) to gig work (e.g., \cite{33}) to social services (e.g., \cite{18}) to education (e.g., \cite{29,32}). Third, the AI systems used by AATs are built on existing data and capabilities (e.g., text, pointing data, natural language processing algorithms) that increase the potential for harm. 

We take up the argument made by other critical scholars that merely recognizing that those AI systems are fostered by institutions and individuals who occupy a privileged position of power is not enough, and that design that includes marginalized individuals must have the goal of challenging structural inequality \cite{9,10}. But we also consider that students (and designers) need to be more diverse and educated about these harms and that new methods are needed to do so. In this position paper, we present two studies in which we used a participatory toolkit to understand the privacy perspectives of intersectional older AAT users and intersectional AI technology students. While our first study with intersectional AAT users \cite{24} (only briefly reported on in this paper) attempts to address this first commitment (i.e., including marginalized users), this paper focuses primarily on the second study and second commitment (i.e., designing new methods). 

For our second study, we explore a new method of incorporating intersectional inquiry into an AAT user elicitation toolkit we adapt for use with intersectional AI students. To do this, we build on our first study \cite{24} of this tool (and earlier inquiry \cite{25}) with intersectional AAT users to explore how international technology students think about AAT technologies for these populations. We wanted to explore the possibilities (and limitations) of an approach that enabled vulnerable international technology students to consider the perspectives of intersectional AAT users with some overlapping concerns and explore the extent to which they might have a more nuanced view—going beyond the empathy design work that is implicitly and overtly critiqued by design justice.

We consider that being older and having a disability vs being a non-native English speaker on a visa in an American institution with a government that might rescind that status to be separate types of intersectional identities that share in common relationships to heightened surveillance capitalism power and risk, through AI technology. Other issues that overlap for both AAT users and non-native language speakers (the students) are the way in which AI-generated language tools could both normalize speech and writing and reduce credibility when their users are very dependent on them \cite{26}. Both older AAT users and international technology students interact with AATs or AAT-like technology that collect and adapt to personal data and engender a particular dependence because of different challenges (e.g., vision and mobility vs. language and cultural pressures). That is, both AAT users and non-native language speakers are dependent on AI-generated language tools to normalize speech and writing, and these technologies collect and adapt to personal data with a kind of reliance that falls outside what is normative because of these challenges. 

Our position paper primarily focuses on whether these intersectional international AI students’ experiences with a similar technological matrix of oppression (the consequence of intersectional identities that leave them dependent on language software) might make them attuned in unique ways to the vulnerabilities in the design of technologies for older adults with Essential Tremors (ET) (our intersectional older adult AAT users). While the importance of developing empathy is considered critical in accessibility education and design research [49] we also wanted to go beyond empathy tools to support design of more private AATS. One of our aims was therefore to support development of design justice methods that could incorporate users and designers in optimal ways.

In the sections that follow, we describe the theoretical frames (AI-enabled capitalism, intersectionality, and design justice) informing our case study. We then describe our case study, reflecting our exploration of this analytical sensibility. We close by reflecting on lessons learned and detail future opportunities. 

\section{Related Literature}
\subsection{AI-enabled capitalism}
Our digital world is built on an economy of “data futures,” in which data is harvested and extracted by social media and search companies who sell it based on its ability to predict behavior. This world has, of course, been normalized, for it’s the principle on which all social media and search operate. Data capitalism \cite{12,34,52}, surveillance capitalism \cite{59}, and data colonialism \cite{11,12} are all related terms used to characterize this system in which data are extracted, harvested, and bartered for service. All these terms seem to suggest some loss of ourselves, not just as sentient consumer subjects but as citizens, and raise the questions of whether we can remain independent of the “capitalization of life” \cite{11}. Data colonialism may, in fact, represent the extreme in terms of domination and exploitation, where individuals are forced into a new social order, and where to be social or ``citizen'' or ``consumer'' is to engage in production for the data economy. Because addressing these nuances in theory is beyond the scope of this paper, we use the term “surveillance capitalism” to encompass the system in which the domination of internet spaces (enabled by AI) relinquishes privacy and exacerbates discrimination and other harms \cite{60}. 

Under surveillance capitalism, technology companies collect user data to customize their algorithms and they also sell user data. These data are used to profile groups of individuals based on socio-economics, race/ethnicity, and other identity vulnerabilities. According to a US Senate Report, a data broker creates and sells consumer groups based on, for example, financial vulnerability, ethnicity, and age \cite{16}. Our identities, life experiences, financial hardship, or other life events or circumstances are baked into our digital profiles affecting access to financial and other services. While there may be some lingering perception that surveillance data harvested about consumers for data capitalization is largely anonymous and innocuous \cite{44}, a growing body of literature argues that it has the potential to do substantial harm to individuals \cite{2,35}. For example, the use of surveillance data in algorithms to administer social welfare, healthcare, and other services can have devastating effects \cite{18,19}. 

In this system, third parties can determine not only that you have a visual or mobility impairments but also other marginalizations, just by monitoring and analyzing your typing data and triangulating it with other data. The discriminatory possibilities, should this data get in the hands of an employer, or government, or other bad actors, are alarming. At the same time, it’s not clear the extent to which vulnerable users grasp these risks—or feel capable of mitigating them, even when they do, particularly when it comes to surveillance infrastructures \cite{22} embedded in AAT systems. We need to rethink how we do research and design for privacy, taking into account, in particular, the marginalized positions of data citizens and their entanglement in an extractive system. It is this lens that seems to create more urgency around the design of systems that take into account structural inequality when considering the impacts of design.   

Designing for privacy has historically been focused on individual agency to control boundaries \cite{1,43,50} with much research dedicated to how users may not care about their privacy \cite{3,30} or feel powerless to do anything about it \cite{28,36}. By contrast, considerations for surveillance capitalism have been more focused on policy and service providers because it is assumed that users can’t affect these economic systems. Arguably, privacy affordances meant to appeal to users agency are intricately linked with economic models of surveillance capitalism \cite{38}. The AI tools that use data for predictive services sell speculative data to third parties. Arguably, they cannot be cleaved, leaving the responsibility of privacy by design to encompass both concepts of privacy—individual identity management and surveillance \cite{44}. 

\subsection{Intersectionality}
We frame our exploration of design justice methods and tools through the lens of intersectionality. Intersectional theory has its origins in Black feminist thinking and is concerned with accounting for simultaneous identities that together may magnify individuals’ susceptibility to systems of discrimination. Intersectional theory allows us to consider the user whose context demands an alternate narrative, distinguished from the “typical” user or personas invoked in design processes \cite{46}. The shared and distinct privacy concerns and risks stemming from overlapping aspects of disabled identities are not well understood by software designers and policy makers whose perceptions may be shaped by “normative” perceptions of vulnerability. Thus, intersectionality offers a non-normative framework through which to consider identity, structures of social inequality, and justice. It expands on feminist approaches to ethics by extending the marginalized view to account for simultaneous identities (and contexts) which exacerbate susceptibility to structural inequality. 

Intersectionality gained popularity with Kimberlé Crenshaw’s essays in 1989/90 \cite{13,14}, but there are many scholars and activists who have contributed to this analytical framework and theory (e.g., noted here \cite{47}). Crenshaw rendered the compounding and altering nature of identity inequality and her emphasis on the reproduction of unequal structural outcomes when identities and interlocking oppressions are not fully taken into account. A core tenant of intersectionality is the critical importance of thinking about power in relationship to multiple, interconnected social coordinates. What Patricia Collins refers to as the matrix of oppression \cite{7} can manifest as surveillance capitalism, which reifies inequality through advertising models and algorithmic surveillance that discriminates and disproportionately affects and harms certain marginalized groups. 

Collins and Bilge articulate how intersectionality grapples with the dynamic complexities of race, class, gender, and systems of normative and discriminatory power in the context of social and political conditions \cite{8}. The way in which coalitions are entangled in certain social inequalities, power, culture, etc. allows Collins and Bilge to explore how intersectionality’s critical framework can be applied to a range of circumstances and identities—from Black feminism to football players in the World Cup. That is, intersectionality is not about finding equations or demographics that operate with analytical precision; it’s about taking a messy, critical lens to interlocking oppressions operating in an environment that is loaded with complexities. These oppressions sometimes work in lock-step but sometimes in less intelligible ways. We build on this concept that even those who have different identities and contexts (and thus, different matrixes of oppressions) might be able to understand that dynamic, particularly if they have that experience. In our case study, we discuss how we put that idea to work, where we fell short, and how we are moving forward. 

\subsection{AI-enhanced AATs and design justice}
Numerous approaches have been developed for assistive and accessibility technology design development that recognize the importance of including people with disabilities at every stage of the design process. Tools like empathy activities (a mainstay of disability design is going blindfolded or using a wheelchair) are well-intended but problematic. First, they cause researchers and designers to respond to their own deprivation, subverting empathy by distracting from the experience of those that designers intend to help. Second, because of empathy exercises might have the paradoxical advantage of seeming so real, we may be more inclined to want to turn the experience off—to distance ourselves from the experience of a disability that we have the privilege to walk away from by, for example, getting up from the wheelchair. Thus, scholars like Bennet and Rosner \cite{5} and Edwards et al. \cite{17} argue that while empathy activities may be important, there is a difference between “being like” and trying to help users vs “being with” and trying to support and empower users through collaborative design practice. \emph{Help} (derived from a poignant experience of empathy) solves the problem with little context. \emph{Support} (derived from compassion and appreciation of someone’s whole experience) puts the problem in broader context of solutions. You have to understand the price people are willing to pay to solve the problem. Supporting or “being with” involves thinking through with individuals the larger context of their struggle, the efficacy of a particular solution, the effort it takes to supply that solution, the importance of the problem it’s meant to solve, and people’s willingness to make tradeoffs. Even if empathy exercises confront ergonomic constraints, they are not attuned to the overlapping oppressions of identity and disability in relationship to structural inequality. For instance, what services one disabled user has access to may not be the same for another, and that may very well be a result of structural inequalities.

Other approaches include User-Sensitive Inclusive Design \cite{42}, Design for User Empowerment \cite{31}, and Ability-based Design \cite{55}. Shinohara et al. developed an approach, Design for Social Accessibility, that recognizes the importance of supporting student awareness of socially usable aspects of a design in addition to its functionality \cite{49}. This approach calls for inclusion of perspectives from users with and without disabilities in the design process, and the use of methods that support consideration of social factors in accessible design \cite{49}.  

While these more inclusive approaches have called for presence of those with disabilities in the research process, a prevailing theme in the design justice movement is that technologies need to be built with the collaboration of users—not just with them in mind. We contend also that awareness of structural inequalities that take into account the whole person needs to be a central analytical focus. We thus see the call by scholars like Shoshana Costanza-Chock \cite{10} to include users as collaborators and consider their complex identity and oppressions as a necessary step in AAT development, but also want to explore the possibility of educating designers in tandem. We agree with their view that intersectionality is a useful framework from which to consider the ways that multiple identities interact with power that includes history of discrimination, oppression, and activism and want to explore its use as a design tool. It’s no easy task, as it requires embracing the complexities of identity as an ongoing struggle and having an ongoing dialogue with users and designers.

\subsection{Our elicitation approach}
These areas of emerging focus highlight the importance of several critical questions. How can we develop a process in the study of AI-enhanced AATs that identifies oppressions, rather than superficially connects with similar or limited experience? How can we "be with" as opposed to "be like”? How can we support and not just help? That is, how can we integrate the diverse and multi-faceted struggles of disabled users? At the same time, how can we surface the mechanisms of oppression using intersectionality’s analytic sensibilities—and how can we engender those sensibilities? 

To address these questions, we designed an elicitation tool that explores identity management and surveillance harms with disabled users (study 1). We then adapted this tool to be used with AI technology students, whose experience of structural discrimination, while certainly not the same as those experienced by diverse adult users of AATs, provide a basis for mapping out how the use of data in structural oppression could find common ground for exploring possible harms of surveillance AI enabled technology used by AATs (study 2). 

We also had another agenda. The challenges with approaches that involve users is that these methods may be hard to realize in an industry setting where investigation with users may be done only by researchers who then deliver insights to designers. We wanted to further explore this problem of how to design private/secure AATs and to do so by bringing disabled people into the process, while also thinking hard about a way to make this process something that designers could be more involved with, particularly in settings where the research role is separate.

\section{Background and Case Study}
\subsection{Toolkit and methods}
We originally developed an elicitation tool (detailed here [24]) to be used with older adult AAT users with essential tremors (ET) (study 1). We then used it with information systems (IS) students studying AI with accompanying intersectional methods (study 2). 

The tool was comprised of a set of cards that represent \textbf{Data Types} (i.e., Typing Data, Pointing data, Credit Card Data, Contact Data, Health Data, Search Queries, and Cookies); \textbf{Third-Parties} (i.e., Family, Friends, Doctors and Medical Professionals, Employers, Insurance Companies, Government, Social Media, Advertisers, Nobody, and Everybody); \textbf{Usage Scenarios} (which were verbally described and accompanied by a video demonstration); \textbf{Privacy Standards} strips (i.e., Health Insurance Portability and Accountability Act (HIPAA), the General Data Protection Regulation (GDPR), Privacy Policy, Terms of Service, Data Use Agreement, Custom Rules (for the participant to make their own standards), and No Rules); an \textbf{Expectations Chart} for users to place third-party cards and indicate which parties they thought would collect their data and which would not); and a \textbf{Wheel of Emotions} (which was adapted in our second study with students as a verbal exercise to elicit intersectional reflections about disability and powerful institutions that linked to students own experience of discrimination in the context of surveillance capitalism). 

For the \textbf{Usage Scenarios}, we first used the popular cloud-based writing assistant, Grammarly [20]. Second, we used the Pointing Interaction Notifications and AdapTAtions (PINATA) system [25] that helps users who experience difficulty when using pointing devices. It consists of a dynamic bubble cursor [21] that simulates the functionality of dynamically changing size in response to users’ pointing performance and the location of the cursor. For PINATA to assist, it must monitor a user’s pointing behavior over time. When errors are detected (for instance, a link is missed while it is being clicked) it increases the size of the cursor. For the Usage Scenarios, the verbal descriptions of Grammarly was that “Grammarly is an adaptive Spell Check application that you can use on your home computer. It works by monitoring your typing”; and for PINATA was “PINATA is an application that adapts to your changing pointing abilities. It works by collecting your pointing data.” Video demos were also shown of Grammarly (from their website grammarly.com) and PINATA (produced by researchers).  

In our second study with students, we included prompts in our interview to encourage them to extrapolate from their experiences with surveillance technology and think about other, intersectional identities, specifically, older adults with mobility or vision impairments and structural discrimination they might face. We sought to learn whether students who are guided to reflect on the chilling surveillance and tracking effects of its use, may be more or less empathetic to the risks that they experience. 

In the following sections, we briefly report on our first study with older AAT users and then focus on our second study with IS graduate students.

\subsection{Study with older adults using AATs (study 1)}
In the first phase of our research, we studied the elicitation tool with 8 older adults who experience pointing difficulties because of ET \cite{24}. We explored what privacy threats users anticipated specifically when using the two systems (Grammarly and PINATA). 

While the participants were willing to have their pointing data collected and used to improve system functionality of AATS for themselves and others, they had strong preferences about who should access their data. Specifically, older AAT users were wary of their data being shared with insurance companies and employers but were more accepting of it being used to improve the AAT for others or for research purposes. Most communicated concern about government (e.g., National Security Agency (NSA) or the Internal Revenue Service (IRS)) having access to their data. Participants were most comfortable sharing data with assistive technology companies and medical professionals. Participants also described how they wanted to have control over who would access their data and under what conditions. This study showed that the tool was effective at eliciting detailed privacy information from non-technical participants and also helped them reflect and elaborate on their choices.

\subsection{Intersectional elicitation: study with students (study 2)}
For the second study, we asked 7 IS graduate students (who were taking a course in algorithm design at the time of the study) with unique vulnerable intersectional identities of their own to explore these same technologies through the lens of both themselves and older adults using AATs. While these students were not disabled and their intersectional vulnerabilities related to immigration and being part of minorities at a time when their visa was at threat, we wanted to understand what impact their experience of risk with at least one of the technologies (i.e., Grammarly) would have on their ability to think about the privacy vulnerabilities of older AAT users. These students also share in common with older AAT users heightened risk of surveillance through AI technology. That is, both interact with/depend on AATs technology that collect and adapt to personal data and because of different challenges (e.g., vision and mobility vs language and cultural pressures).

Ultimately, we learned that while students were well aware of, and concerned about, the risks for government surveillance of their writing and clicking behaviors (which they almost exclusively attributed their status as students with visas), they didn’t imagine these were concerns for AAT users. On the contrary, students imagined these tools would collect data that led to improvements for AAT users, whom they assumed everyone just agreed needed the help. They felt, for instance, that if the systems had access to more data because of their use of AI, that would help advertisers target people with disabilities with products that were more customized and thus would help the AI work better. Similarly, they also felt that having more data would be useful to doctors or governments who wanted to monitor the condition of users with disabilities, and act on those data.

Ultimately, students didn’t connect the mechanisms that result in government tracking of themselves (which they worried very much about) with profiling of older adult AAT users; they just thought that any collection would simply go to doctors, government, family members, and teams of developers that wanted to help.

Notably, students did express knowledge of certain power structures that were oppressive to them, what Collins refers to as disciplinary domains of power (what rules apply, to whom and when) \cite{7}. Yet the government rules they spoke of, and which they felt influenced them, were more sinister than the ones that existed for older AAT users and the power imbalance they identified only applied to their own interaction with AI and not AAT users. Students also expressed knowledge of structural domains of power (in this case, how immigration institutions operate and use AI infrastructures) \cite{7}, but did not imagine similar structures for older AAT users (e.g., insurances companies and other advertisers profiling to offer different services). That is, students are thinking about the mechanics of surveillance and the way that they relate to power but don’t connect those insights with older AAT users. Even those who considered the same surveillance on AAT users assume that these privacy breaches are merely to improve the system for those with disabilities. For instance, they assume that personalization and assistance is an appropriate privacy tradeoff for someone with disabilities. If anything, their empathy and desire to help led them to consider that the problem was not of powerful institutions but, for instance, was the inability of those meaning well to help without fuller access.

\section{Lessons Learned}
In some sense, our research reiterates core design justice principles: research must include vulnerable users every step of the way. And it also suggests that there are simply no shortcuts to this process. Indeed, students remarked that they would need to speak with disabled users to really understand their challenges. But there are other lessons as well. We had thought that experience of structural inequality might have provided additional insight for students into the potential harms encountered by (or that could befall) older AAT users. That didn’t work out. Yet we were encouraged that our design may have, at the very least, gotten student designers into the practice of thinking about identity in relationship to systems of power (that is, to identify these struggles and their mechanisms)—even if that is not ultimately where they landed. 

The next step is to help students engage users to understand the consequences of their technology design choices for others and to do so by learning about the structures at play. While it’s clear that AI challenges cannot be solved merely with technical approaches, we need to find a way to address emerging and dynamic tensions stemming from dominant social and political configurations that require social and political awareness and an understanding of power [53] and surveillance capitalism. According to the AI Now Institute, “a more complete definition of AI includes technical approaches, social practices and industrial power” \cite{61}. In our approach, we came close to having students interrogate the role of society, culture, and powerful institutions that undergird the AI systems they use. This knowledge is essential to understanding AI’s intersectional vectors when collaborating in design with vulnerable disabled users (or any marginalized user).

\subsection{Future opportunities}
Our design may have partially fulfilled the first step of getting students and designers into the practice of thinking about identity in relationship to systems of power (that is, to identify struggles and their mechanisms). The next step will be to help them understand the consequences of their technology design choices for others in lock-step with society, culture, and powerful institutions. Below we describe activities that address both steps that we will incorporate into future iterations of this work, our rational from the lessons learned, and also how we might realize and implement these activities.

\textbf{Activity 1:} Have student use the elicitation tool with AAT users input incorporated into the tool, particularly those with complex identities.

\textbf{Rationale:}
Students remarked often at the end of the interviews that they wish they could speak with AAT users.

\textbf{Thinking it through:}
We will need to explore ways of incorporating feedback from AAT communities into our elicitation activities—possibly as simple as including quotes. Bennet and Rosner \cite{5} also stipulate that rather than seek complete “understanding” we should be seeking “attunement.” That is, not “filling absences” but considering why they are there and what that means for the capabilities of those for whom we are designing for. Perhaps students should be made aware of blanks and have to fill them in with technical inferences. We expect that those who design systems might also have insight into its capabilities. Additionally, we might consider having students, after reading quotes about realities, challenges or worries, think about how that might occur (or what could occur) technically.

\textbf{Activity 2:} Leverage similarities and differences, while also emphasizing asymmetries.

\textbf{Rationale:}
Students struggled to connect their experiences of oppression in similar scenarios with older AAT users and also did not consider how their agency might differ---e.g., the idea that while they rely on Grammarly they could stop using these systems while under visa review.

\textbf{Thinking it through:}
We will find ways to continually map similarities and differences between student/designers and AAT users.  Bennet and Rosner assert that “reworking design empathy as ‘being with’ could raise asymmetries not as things to be avoided but as things to be ongoingly accountable to” \cite{5}. While activity 1 might help with that, we can also have an exercise that explicitly draws out the similarities students have with AAT users (e.g., dependence on spelling and writing tools) and also elicits asymmetries and experiences they don’t know. We did this to an extent in the interviews, asking students to imagine AAT users, but we could have this be an explicit exercise.

\textbf{Activity 3:} Focus more on surfacing links between identity and power and mutual struggle. For example, we could more clearly delineate and elucidate mechanisms and risks in our design and activities. We could encourage mapping of narratives of struggle to domains of power (e.g., surface contradictions to the idea that all technology is good).

\textbf{Rationale:}
Students identified power imbalances but only those that applied to them. Those they considered for AAT users were always for good reason.

\textbf{Thinking it through:}
We experimented with intersectionality in our interview methods, but we could do more. In future iterations, we will incorporate exercises that have students elicit vectors of oppression and power dynamics exploited by these technologies and consider how they might affect AAT users. We might, for instance, have exercises for considering how structural domains of power could exist across populations. To an extent, the tool already does this by using the data type cards and third-party cards to create narratives, first for students and then for AAT users. Yet we could emphasize these comparisons by, for example, taking the simple step of preserving the arrangements of the cards for the students and forcing visual comparison. This might, in fact, be an advantage to using a virtual toolkit.

\textbf{Activity 4:} Design fiction/speculative approach to design as a way to explore these narratives without constraint.

\textbf{Rationale:}
Students told us that the elicitation tool really made them reflect. Yet they also noted the limits to their ability to reflect.

\textbf{Thinking it through:}
Design fictions use elements of existing technology and design metaphors, as well as narrative, to imagine potential futures in provocative ways that foster ethical debate \cite{6}. Design fictions are increasingly popular critical device in academia and the arts. They are particularly effective at exposing, interrogating, and “queering” norms, with the potential to elucidate marginalized concerns and vulnerabilities and better serve the needs of populations who may not share those norms or priorities. We may not be able to include a design fiction exercise in the toolkit, but perhaps creating them could be a subsequent workshop session goal.

\section{Conclusions}
We took a study of a privacy elicitation toolkit for older adults who experience difficulties with computer pointing and typing tasks and deployed it with IS graduate students, with the goal of understanding how we can iterate on design justice principles with an intersectional approach. The results were mixed. On the one hand, we found that the tool, combined with intersectional methods, succeeded in eliciting reflections about the risks of collecting data for the purposes of enabling AI technologies. But the tool didn’t (at least, not in this current iteration) lead students to reflect on risks to other AAT users. That is, students were often able to associate their own risks with aspects of their identity that leave them vulnerable but did not extrapolate those identities or vulnerabilities to diverse real-world AAT users. Future work will incorporate interview and activity prompts that focus more on identity-based exploration, for instance, incorporating more intersectional interview methods where one’s identity and contexts of power are linked to specific experiences of risk. It will surface power dynamics with scenarios, employ design fictions as well as integrate user populations. 
 
\vspace{-.1cm}
\bibliographystyle{ACM-Reference-Format}
\begin{thebibliography}{10}
\begin{small}
\itemsep=-.5pt

\bibitem{1}
Altman, I. 1977. Privacy Regulation: Culturally Universal or Culturally Specific? Journal of Social Issues. 33, 3 (1977).

\bibitem{2}
Angwin, J. and Parris Jr., T. 2016. Facebook Lets Advertisers Exclude Users by Race. ProPublica.

\bibitem{3}
Barnes, S.B. 2006. A privacy paradox: Social networking in the United States. First Monday. 11, 9 (Sep. 2006).

\bibitem{4}
Benjamin, R. 2019. Assessing risk, automating racism. Science (New York, N.Y.). 366, 6464 (Oct. 2019), 421–422. DOI:https://doi.org/10.1126/science.aaz3873.

\bibitem{5}
Bennett, C.L. and Rosner, D.K. 2019. The Promise of Empathy: Design, Disability, and Knowing the “Other.” Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (New York, NY, USA, May 2019), 1–13.

\bibitem{6}
Bleecker, J. 2009. Design Fiction: A Short Essay on Design, Science, Fact and Fiction. Near Future Laboratory.

\bibitem{7}
Collins, P.H. 2019. Intersectionality as Critical Social Theory. Duke University Press Books.

\bibitem{8}
Collins, P.H. and Bilge, S. 2016. Intersectionality. Polity.

\bibitem{9}
Costanza-Chock, S. 2018. Design Justice, A.I., and Escape from the Matrix of Domination. MIT Comparative Media Studies/Writing. (Jul. 2018).

\bibitem{10}
Costanza-Chock, S. 2020. Design Justice: Community-Led Practices to Build the Worlds We Need. The MIT Press.

\bibitem{11}
Couldry, N. and Mejias, U.A. 2019. Data Colonialism: Rethinking Big Data’s Relation to the Contemporary Subject. Television \& New Media. 20, 4 (May 2019), 336–349. 

\bibitem{12}
Couldry, N. and Mejias, U.A. 2019. The Costs of Connection: How Data Is Colonizing Human Life and Appropriating It for Capitalism. Stanford University Press.

\bibitem{13}
Crenshaw, K. 1989. Demarginalizing the Intersection of Race and Sex: A Black Feminist Critique of Antidiscrimination Doctrine. University of Chicago Legal Forum. 1989, 1 (1989).

\bibitem{14}
Crenshaw, K. 1991. Mapping the Margins: Intersectionality, Identity Politics, and Violence against Women of Color. Stanford Law Review. 43, 6 (1991), 1241–1299.

\bibitem{15}
Dickinson, A., Arnott, J. and Prior, S. 2007. Methods for human – computer interaction research with older people. Behaviour \& Information Technology. 26, 4 (Jul. 2007), 343–352. 

\bibitem{16}
Editorial Board 2014. A Second Front in the Privacy Wars. The New York Times. https://www.nytimes.com/2014/02/24/opinion/a-second-front-in-the-privacy-wars.html

\bibitem{17}
Edwards, E.J., Monet, C.M. and Branham, S.M. 2020. Three Tensions Between Personas and Complex Disability Identities. Conference on Human Factors in Computing Systems Extended Abstracts (Honolulu, USA, 2020).

\bibitem{18}
Eubanks, V. 2018. Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. St. Martin’s Press.

\bibitem{19}
Eubanks, V. 2006. Technologies of Citizenship: Surveillance and Political Learning in the Welfare System. Surveillance and Security. T. Monahan, ed. Routledge.

\bibitem{20}
Grammarly.: https://www.grammarly.com/. 

\bibitem{21}
Grossman, T. and Balakrishnan, R. 2005. The bubble cursor: Enhancing target acquisition by dynamic resizing of the cursor’s activation area. (Jan. 2005), 281–290.

\bibitem{22}
Gürses, S. 2010. PETs and their users: a critical review of the potentials and limitations of the privacy as confidentiality paradigm. Identity in the Information Society. 3, 3 (Dec. 2010), 539–563. 

\bibitem{23}
Hagendorff, T. 2020. The Ethics of AI Ethics: An Evaluation of Guidelines. Minds and Machines. (Feb. 2020). 

\bibitem{24}
Hamidi, F., Poneres, K., Massey, A. and Hurst, A. 2020. Using a Participatory Activities Toolkit to Elicit Privacy Expectations of Adaptive Assistive Technologies. W4A’20 (Taipei, Taiwan, Apr. 2020).

\bibitem{25}
Hamidi, F., Poneres, K., Massey, A. and Hurst, A. 2018. Who Should Have Access to My Pointing Data?: Privacy Tradeoffs of Adaptive Assistive Technologies. Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility (New York, NY, USA, 2018), 203–216.

\bibitem{26}
Hancock, J.T., Naaman, M. and Levy, K. 2020. AI-Mediated Communication: Definition, Research Agenda, and Ethical Considerations. Journal of Computer-Mediated Communication. 25, 1 (Mar. 2020), 89–100.

\bibitem{27}
Hao, K. 2019. Facebook’s ad-serving algorithm discriminates by gender and race. MIT Technology Review.

\bibitem{28}
Hargittai, E. and Marwick, A. 2016. “What can I really do?” Explaining the privacy paradox with online apathy. International Journal of Communication. 10, (2016), 3737–3757.

\bibitem{29}
Keierleber, M. 2021. Dems Warn School Surveillance Tools Could Compound ‘Risk of Harm for Students.’

\bibitem{30}
Kokolakis, S. 2017. Privacy attitudes and privacy behaviour: A review of current research on the privacy paradox phenomenon. Computers \& Security. 64, (Jan. 2017), 122–134. 

\bibitem{31}
Ladner, R.E. 2015. Design for user empowerment. Interactions. 22, 2 (Feb. 2015), 24–29. 

\bibitem{32}
Lecher, C. and Varner, M. 2021. NYC’s School Algorithms Cement Segregation. This Data Shows How. THE CITY.

\bibitem{33}
Lin, B. 2021. Uber Patents Reveal Experiments With Predictive Algorithms to Identify Risky Drivers. The Intercept.

\bibitem{34}
Lutz, C., Hoffmann, C.P. and Ranzini, G. 2020. Data capitalism and the user: An exploration of privacy cynicism in Germany. New Media \& Society. 22, 7 (Jul. 2020), 1168–1187. 

\bibitem{35}
Madden, M., Gilman, M., Levy, K. and Marwick, A. 2017. Privacy, Poverty, and Big Data: A Matrix of Vulnerabilities for Poor Americans. Washington University Law Review. 95, 1 (Jan. 2017), 053–125.

\bibitem{36}
Marwick, A., Fontaine, C. and boyd,  danah 2017. “Nobody Sees It, Nobody Gets Mad”: Social Media, Privacy, and Personal Responsibility Among Low-SES Youth. Social Media + Society. 3, 2 (Apr. 2017).

\bibitem{37}
McDonald, N., Badillo-Urquiola, K., Ames, M.G., Dell, N., Keneski, E., Sleeper, M. and Wisniewski, P.J. 2020. Privacy and Power: Acknowledging the Importance of Privacy Research and Design for Vulnerable Populations. CHI’20 Extended Abstracts (Apr. 2020).

\bibitem{38}
McDonald, N. and Forte, A. 2021. Powerful Privacy Norms in Social Network Discourse. PACM on Human-Computer Interaction. CSCW. 5, 2 (2021).

\bibitem{39}
McDonald, N. and Forte, A. 2020. The Politics of Privacy Theories: Moving from Norms to Vulnerabilities. Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (New York, NY, USA, Apr. 2020), 1–14.

\bibitem{40}
Morris, M.R., Zolyomi, A., Yao, C., Bahram, S., Bigham, J.P. and Kane, S.K. 2016. “With most of it being pictures now, I rarely use it”: Understanding Twitter’s Evolving Accessibility to Blind Users. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (San Jose, California, USA, May 2016), 5506–5516.

\bibitem{41}
Newell, A.F. and Gregor, P. 2000. “User sensitive inclusive design” - in search of a new paradigm. Proceedings on the 2000 conference on Universal Usability (New York, NY, USA, Nov. 2000), 39–44.

\bibitem{42}
Newell, A.F., Gregor, P., Morgan, M., Pullin, G. and Macaulay, C. 2011. User-Sensitive Inclusive Design. Universal Access in the Information Society. 10, 3 (Aug. 2011), 235–243. 

\bibitem{43}
Palen, L. and Dourish, P. 2003. Unpacking “Privacy” for a Networked World. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (New York, NY, USA, 2003), 129–136.

\bibitem{44}
Phillips, D.J. 2004. Privacy policy and PETs: The influence of policy regimes on the development and social implications of privacy enhancing technologies. New Media \& Society. 6, 6 (Dec. 2004), 691–706. 

\bibitem{45}
Predicting Parkinson’s Disease with Smartphone Data: https://www.michaeljfox.org/grant/predicting-parkinsons-disease-smartphone-data. 

\bibitem{46}
Pruitt, J. and Grudin, J. 2003. Personas: Practice and Theory. Proceedings of the 2003 Conference on Designing for User Experiences (New York, NY, USA, 2003), 1–15.

\bibitem{47}
Rankin, Y.A. and Thomas, J.O. 2019. Straighten Up and Fly Right: Rethinking Intersectionality in HCI Research. Interactions. 26, 6 (Oct. 2019), 64–68.

\bibitem{48}
Saltz, J., Skirpan, M., Fiesler, C., Gorelick, M., Yeh, T., Heckman, R., Dewar, N. and Beard, N. 2019. Integrating Ethics Within Machine Learning Courses. ACM Trans. Comput. Educ. 19, 4 (Aug. 2019), 32:1-32:26. 

\bibitem{49}
Shinohara, K., Bennett, C.L., Pratt, W. and Wobbrock, J.O. 2018. Tenets for Social Accessibility: Towards Humanizing Disabled People in Design. ACM Transactions on Accessible Computing. 11, 1 (Mar. 2018), 6:1-6:31. 

\bibitem{50}
Stutzman, F. and Hartzog, W. 2012. Boundary Regulation in Social Media. Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work (New York, NY, USA, 2012), 769–778.

\bibitem{51}
Technology for detecting skin cancer is forging ahead – but not for people of color, apparently: 2021. https://www.mic.com/life/technology-detecting-skin-cancer-exccludes-people-of-color. 

\bibitem{52}
West, S.M. 2019. Data Capitalism: Redefining the Logics of Surveillance and Privacy. Business \& Society. 58, 1 (Jan. 2019), 20–41. 

\bibitem{53}
Whittaker, M., Crawford, K., Dobbe, R., Fried, G., Kaziunas, E., Mathur, V., West, S.M., Richarson, R., Schultz, J. and Schwartz, O. 2018. AI Now 2018 Report. AI Now Institute.

\bibitem{54}
Wilkinson, D., Namara, M., Badillo-Urquiola, K., Wisniewski, P.J., Knijnenburg, B.P., Page, X., Toch, E. and Romano-Bergstrom, J. 2018. Moving Beyond a “One-size Fits All”: Exploring Individual Differences in Privacy. Extended Abstracts of the 2018 CHI Conference on Human Factors in Computing Systems (New York, NY, USA, 2018), W16:1-W16:8.

\bibitem{55}
Wobbrock, J.O., Kane, S.K., Gajos, K.Z., Harada, S. and Froehlich, J. 2011. Ability-Based Design: Concept, Principles and Examples. ACM Transactions on Accessible Computing. 3, 3 (Apr. 2011), 9:1-9:27. 

\bibitem{56}
Wong, P.H. 2020. Democratizing Algorithmic Fairness. Philosophy \& Technology. 33, 2 (Jun. 2020), 225–244. 

\bibitem{57}
Wong, R.Y. and Mulligan, D.K. 2019. Bringing Design to the Privacy Table: Broadening “Design” in “Privacy by Design” Through the Lens of HCI. Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. Association for Computing Machinery. 1–17.

\bibitem{58}
Zuboff, S. 2015. Big other: surveillance capitalism and the prospects of an information civilization. Journal of Information Technology. 30, 1 (Mar. 2015), 75–89.

\bibitem{59}
Zuboff, S. 2019. The Age of Surveillance Capitalism: The Fight for a Human Future at the New Frontier of Power. Public Affairs.

\bibitem{60}
Zuboff, S. 2021. You Are the Object of a Secret Extraction Operation. The New York Times.

\bibitem{61}
2018. Algorithmic Accountability Policy Toolkit. AI Now Institute. \end{small}
\end{thebibliography}

\end{document}
