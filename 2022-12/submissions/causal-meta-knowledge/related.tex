In this section, we first review the related studies in causal discovery for propositional domains and relational domains. Then we discuss and clarify the distinction between the proposed approach and the most relevant  \textit{rule mining} methods for relational data.
We list the relevant research areas and our differences in the Tab.~\ref{tab:related}

\begin{table*}[ht]
\centering
\caption{Comparison of our work and related work.}
\label{tab:related}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{ccccc}
\cline{1-3}
\multicolumn{1}{|c|}{}              & \multicolumn{1}{c|}{Association}                                                                                            & \multicolumn{1}{c|}{Causality}                                                                                                          &                      &                      \\ \cline{1-3}
\multicolumn{1}{|c|}{Propositional} & \multicolumn{1}{c|}{Traditional machine learning}                                                                           & \multicolumn{1}{c|}{Traditioanal causal model}                                                                                          &                      &                      \\ \cline{1-3}
\multicolumn{1}{|c|}{relational}    & \multicolumn{1}{c|}{\begin{tabular}[c]{@{}c@{}}Rule mining(e.g.logical rule),\\ Graph representation learning\end{tabular}} & \multicolumn{1}{c|}{{\begin{tabular}[c]{@{}c@{}}Relational causal model(with attribute),\\ \textbf{Our(without attribute})\end{tabular}}} &                      &                      \\ \cline{1-3}
\multicolumn{1}{l}{}                & \multicolumn{1}{l}{}                                                                                                        & \multicolumn{1}{l}{}                                                                                                                    & \multicolumn{1}{l}{} & \multicolumn{1}{l}{}
\end{tabular}%
}
\end{table*}


% \xz{(put the following paragraph in introduction) Causal knowledge mining from observational data is a long-standing problem in the field of AI research. This study is concerned with knowledge representations on the relational domains, instead of the propositional domains which attracts lots of attentions of academia. Note the relational data is very common in the real world but few work attempts to discover causal rules from this kind of data. In this work, we fill this gap by describing the topological connection of entity pairs with meta-structure.  We hope our work can provide insights to this research community.}

\noindent
\textbf{Causal Discovery from Propositional Data.}
Rubin causal models~\cite{imbens2010rubin} and structural causal models (SCMs)~\cite{pearl2010causal} are the two dominated frameworks for causal discovery from propositional data. Particularly, the former analyzes the causal effect between treatment and effect with partial structural information, while the later employs Bayesian networks to identify causal structure.
Our situation resembles causal discovery in SCM because we are primarily concerned with unearthing causal relationships from KG.
Furthermore, there are two kinds of causal discovery algorithms, \textit{constraint-based} and \textit{score-based}~\cite{spirtes2000causation}, in SCMs.
Contrary to the score-based approaches, which are based on the global score, the constraint-based approaches can employ the local conditional independence to determine the causal relationship between specific variables, which is critical when only partial causal relationship is interested.
In addition, the constraint-based methods are non-parametric, which means that they do not depend on the specific functions to connect variables.
Based on above advantages, we follow the constrained-based design to develop our method.


\noindent
\textbf{Relational Causal Model.}
To our best knowledge, the relational causal model~\cite{maier2010learning,lee2016learning,lee2020towards,salimi2020causal} is the only framework designed to extract causal information from relational data.
The input of a relational causal model is a relational database containing entity and relation tables.
Each entity table contains all entities corresponding to the same concept, and each relation table contains all facts corresponding to the same relationship between two entities.
Consequently, a relational database is comparable to a KG.
Relational causal model finds causal relationship between related entity attributes.
For example, for the database with two relations: Develop(Employee, Product) and Funds(Company, Product), the relational causal relationship will give the results like [Employee, Develops, Products, Fundsby, Company].budgets $\,\to\,$  [Employee].Success.
We can see that
relational causal models emphasize \emph{relational models of attributes with a known entity-level connection graph}~\cite{maier2013sound}, while our research focuses on \emph{generative process of the connection}, which is the preceding step in the entire KG generation process.


\noindent
\textbf{Rule Mining from KG.}
Inductively knowledge reasoning involves generalising patterns from a given set of observed facts and then generating novel but potentially imprecise predictions.
In addition to the incomprehensible techniques based on embedding, \textit{rule mining} methods, which benefit from intuitive interpretation of the findings of link prediction, have maintained the popularity for decades.
The rule mining studies in KG can be divided into two categories according to the main objectives: metrics-oriented and completion-oriented.
Metrics-oriented methods~\cite{galarraga2013amie,galarraga2015fast,omran2019embedding}
% aim to discover the interpretable rules from KG to serve downstream tasks, such as link prediction, knowledge reasoning.
usually use predefined co-occurrence metrics, \textit{confidence} and \textit{support}, to find rules satisfying the given thresholds of the metrics, based on a top-down fashion.
Recently,  AnyBURL~\cite{meilicke2019anytime} designs a bottom-up technique for rule learning, which requires few computational resources.
% Due to the frequent open world assumption (OWA)\footnote{There are two opposing assumptions of KG: open world assumption (OWA) and closed world assumption (CWA). CWA assumes a knowledge representation is a complete description of the world. A statement that is true is also known to be true, and the unknown statement is false. OWA only assumes the known statement is true and the unknown statement is possibly true}~\cite{hogan2021knowledge} in KG, these methods mainly learn the monotonic rules.
% Based on the previous techniques, \cite{gad2016exception,ho2018rule,tanon2017completeness} investigate how to learn non-monotonic rules with the negated edges in the body.
The other line of research is completion-oriented.
Different from the predefined metrics-based methods, these studies are mainly based on end-to-end learning and target on the link completion task.
The explainable rules are mainly the intermediate results, which are obtained via analyzing the parameters of the model.
The trained models are used to predict the link directly.
% The core idea is that the joined relations in rule bodies can be represented as matrix multiplication.
Neural-LP~\cite{yang2017differentiable} adopts an attention mechanism to select a variable-length sequence as the body of rules for which confidence scores are learnt from the attention vectors.
DRUM~\cite{sadeghian2019drum} uses bidirectional recurrent neural networks to learn the relations of sequences, which are the body of rules, and their confidence scores are estimated via the recurrent neural network.
RNNLogic~\cite{qu2020rnnlogic} utilizes logic rules as a latent variable and trains both a rule generator and a reasoning predictor with logic rules.

Our work can be seen as one of solutions for rule mining.
Different from the past the association-based rule mining methods,
our work aims to discover deeper relationship ({\it i.e.} causation) via a more rigorous statistical inference system.
To the best of our knowledge, this is the fist attempt to study rule mining problem under causal perspective, as far as we know.



% Please add the following required packages to your document preamble:
% \usepackage{graphicx}

% \begin{table}[t]
% \centering
% \caption{Dataset statistics of all the experiments.}

% % \vspace{-2pt}
% \scalebox{1}{
% \begin{tabular}{c|c|c}
% \shline
%    & \textbf{Association} & \textbf{Causality}   \\
% \shline
% \textbf{Propositional} & Traditional machine learning & Traditioanal causal model\\
% \shline
% \textbf{relational} & Logical rules &  \\

% \shline
% \end{tabular}
% }
% \end{table}



% \textbf{Relational Causal Model.}
% There is a research topic in database called relational causal model (RCM), which is proposed firstly in ~\cite{maier2010learning}.
% And the several modified algorithms are proposed in recent years~\cite{lee2016learning,lee2020towards,salimi2020causal}.
% The input of RCM is the relational database, which includes two kinds of tables in  relational database: entity table and relation table.
% Each entity table includes all the entities according to the same concept, and each relation table includes all the facts according to the same relation between two entities.
% Therefore a relational database can be treated as a KG for each entity.
% RCM aims to find the causal relationship between the attributes of entities, which have certain relationships.
% For example, for the database with two relations: Develop(Employee, Product) and Funds(Company,Product), the relational causal relationship will give the results, such as the [Employee, Develops, Products, Fundsby, Company].budgets$\,\to\,$ [Employee].Success.

% RCM is a way to learn the causal knowledge about entities' attributes, but not the topological relations between entities, which is our target.
% Therefore learned causal knowledge of RCM can not be used to solve the link prediction task in KG.

% \noindent
% \textbf{Causal Discovery.}
% There are two main frameworks for causal discovery:  Rubin causal models ~\cite{imbens2010rubin} and structural causal models (SCMs)~\cite{pearl2010causal}.
% The former mainly investigates the causal effect between the treatment and the effect based on the potential outcome model, while the latter mainly focuses on discovering the causal structure between variable via bayesian network.
% Our problem is more similar to the traditional causal discovery problem in SCMs.

% Furthermore, there are two types of causal discovery algorithms, \textit{constraint-based} and \textit{score-based}~\cite{spirtes2000causation} in the SCMs.
% The constraint-based algorithms construct the causal structure based on conditional independence constraints, while the score-based algorithms generate a number of candidate causal graphs, assign a score to each, and select a final graph based on the scores.
% Note that the intermediate results generated by the constraint-based algorithms are also valuable in our problem.
% More importantly, given a reliable conditional independence testing method, this kind of methods can handle various types of data distributions and causal relations.
% Consequently, we design a causal rule discovery method based on a typical constrained based model, the PC algorithm \cite{tsagris2019bayesian}.

% Typical constraint-based algorithms include PC~\cite{tsagris2019bayesian} and Fast Causal Inference~\cite{glymour2019review}.
% Such approaches have  widely applicability because they can handle various types of data distributions and causal relations, given reliable conditional independence testing methods.

% For our problem, the intermediate conditional independence testing results can provide rich insights about the causal rules, so we design a causal rule discovery method based on the PC algorithm. 