\section{Results And Analysis}

\begin{table}[!htb]
    \caption{Performance of Chinese implicit sentiment analysis}
    \label{table:6_1}
    \centering
    \begin{tabular}{llllll}
        \toprule[1.5pt] 
        \textbf{Methods}              & \textbf{PLM} & \textbf{$F_{neu}$} & \textbf{$F_{pos}$} & \textbf{$F_{neg}$} & \textbf{$F_1$-macro} \\
        \hline
        SDT-CNN                       &              & 0.735              & 0.701              & 0.697              & 0.711                \\
        TextCNN                       &              & 0.734              & 0.724              & 0.712              & 0.723                \\
        CsHGCN                        &              & 0.848              & 0.670              & 0.775              & 0.764                \\
        BiLSTM + Att                  &              & 0.800              & 0.630              & 0.747              & 0.726                \\
        BiLSTM + Mult-Att             &              & 0.810              & 0.634              & 0.749              & 0.730                \\
        \multirow{3}*{CMPOA}          & random       & 0.800              & 0.568              & 0.702              & 0.690                \\
        ~                             & BERT         & 0.784              & 0.585              & 0.712              & 0.694                \\
        ~                             & ERNIE        & 0.816              & 0.662              & 0.761              & 0.746                \\
        \multirow{3}*{TreeLSTM+KG}    & random       & 0.750              & 0.560              & 0.692              & 0.667                \\
        ~                             & BERT         & 0.764              & 0.581              & 0.725              & 0.690                \\
        ~                             & ERNIE        & 0.805              & 0.650              & 0.779              & 0.745                \\
        \multirow{3}*{BiLSTM+MPOA}    & random       & 0.764              & 0.569              & 0.691              & 0.675                \\
        ~                             & BERT         & 0.808              & 0.684              & 0.669              & 0.720                \\
        ~                             & ERNIE        & 0.834              & 0.690              & 0.791              & 0.775                \\
        \multirow{3}*{LSTM+Att+KG}    & random       & 0.759              & 0.558              & 0.690              & 0.669                \\
        ~                             & BERT         & 0.771              & 0.593              & 0.732              & 0.699                \\
        ~                             & ERNIE        & 0.802              & 0.643              & 0.781              & 0.742                \\
        \multirow{3}*{BiLSTM+MPOA+KG} & random       & 0.773              & 0.554              & 0.692              & 0.673                \\
        ~                             & BERT         & 0.792              & 0.593              & 0.710              & 0.699                \\
        ~                             & ERNIE        & 0.819              & 0.655              & 0.749              & 0.741                \\
        \hline
        \multirow{3}*{Ours}           & random       & 0.789              & 0.571              & 0.702              & 0.687                \\
        ~                             & BERT         & 0.803              & 0.582              & 0.716              & 0.699                \\
        ~                             & ERNIE        & \textbf{0.849}     & \textbf{0.690}     & \textbf{0.799}     & \textbf{0.778}       \\
        \bottomrule[1.5pt]
    \end{tabular}
\end{table}

\subsection{Comparison with Baselines}

Experiment results are reported in Table.~\ref{table:6_1}, and by observing and analyzing the results it can be found that:

From the overall view of each metric, the method proposed in this paper has the best results on all metrics, indicating that our method can effectively improve the effectiveness of the implicit sentiment analysis task. Specifically, in our experiments, we compare the model proposed in this paper with models that have been effective in traditional explicit sentiment analysis tasks and with existing implicit sentiment analysis models, and our model obtained the best results.

Our model has a large improvement in the implicit sentiment analysis task compared to the traditional explicit sentiment analysis model, which indicates that the traditional explicit sentiment analysis approach has some shortcomings in the implicit sentiment analysis task.
Further, from the results of the model without the introduction of common sense knowledge information, it can be found that the introduction of additional external common sense knowledge has a 4\% improvement in the $F_1-macro$ metric for the implicit sentiment analysis task, which indicates that the introduction of the triplet  of common sense knowledge can provide critical sentiment information and can effectively compensate for the lack of information caused by the absence of explicit sentiment words.
On the other hand, the result analysis of the model without introducing contextual information illustrates that for the target implicit sentiment expression, fusion with contextual information can extend the sentiment semantic information of implicit sentiment sentences from another aspect to enhance the implicit sentiment analysis.
The above results and analysis show that effective extension of the implicit sentiment sentence representation can be achieved by fusing the common sense knowledge information provided by the external knowledge base with the contextual representation and the semantic information of the sentiment expression itself.

Meanwhile, the common sense knowledge fusion method proposed in this paper can effectively improve the interpretability of the model. Traditional methods, such as BiLSTM+MPOA, usually interpret the prediction results based on the attention weights of words in the sentence only. However, in implicit sentiment analysis tasks, words with higher attention weights are usually also neutral or objective words. The reason for the model prediction can be demonstrated by introducing external common sense knowledge. Take the sentence ``I got a good grade in this exam" as an example, the positive sentiment predicted by the model can be explained by the sentiment triple $(getting\ a\ good\ grade, causes, praise)$, which makes the prediction more intuitive and convincing.
In summary, the experimental results show that the method in this paper has improved the performance of Chinese implicit sentiment analysis and has shown the ability to interpret the results to a certain extent.

\begin{table}[!htb]
    \caption{Performance of Chinese implicit sentiment analysis The ablations of different components are reported separately in this table, where the model without additional explicit introduction of external common sense knowledge is denoted as ‘-KG’, ‘-MPA’ indicates that no multipolar orthogonal attention mechanism is used in the process of sentiment representation and knowledge incorporation, and ‘-Context’ indicates that no additional contextual information of additional sentiment representation is introduced.}

    \label{table:6_2}
    \centering
    \scalebox{1.2}{
        \begin{tabular}{lllll}
            \toprule[1.5pt] 
            \textbf{Methods} & \textbf{$F_{neu}$} & \textbf{$F_{pos}$} & \textbf{$F_{neg}$} & \textbf{$F_1$-macro} \\
            \hline
            Ours             & \textbf{0.849}     & \textbf{0.690}     & \textbf{0.799}     & \textbf{0.778}       \\
            -KG              & 0.816              & 0.662              & 0.761              & 0.746                \\
            -MPA             & 0.827              & 0.674              & 0.782              & 0.761                \\
            -Context         & 0.819              & 0.655              & 0.749              & 0.741                \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
\end{table}

\subsection{Ablation Study}

Tab.~\ref{table:6_2} shows the results of the metrics after removing each component from our model separately. The results of the three ablation experiments show that each component of the proposed method in this paper contributes to the model to some extent.

Specifically, removing the introduction of external knowledge has the greatest impact on the model's effectivenes; without the use of external common sense knowledge (`-KG'), we observe a more dramatic decrease in performance compared to all other components, with a decrease of nearly 4.1\% in $F_1-macro$, demonstrating the effectiveness and importance of introducing additional common sense knowledge for the implicit sentiment analysis task, which can effectively complement the implicit sentiment expression sentiment cues that are missing due to the absence of explicit sentiment vocabulary.
The next most effective effect of removing the introduction of contextual information on the model, with an overall performance decrease of 3.4\% when focusing only on the semantic information of the implicit sentiment expression itself (-Context'), which indicates that contextual information is also more significant for understanding the implicit sentiment expression and can be used when the implicit sentiment expression. This indicates that contextual information is also important for understanding implicit sentiment expressions, and can offer effective sentiment semantic information when the implicit sentiment expressions cannot provide sufficient sentiment features by their own information.
Finally, the multi-polar attention mechanism has the least effect on the model, and the addition of MPA mechanism improves the model by about 2.1\%, which also indicates that the  MPA mechanism is somewhat helpful, but not as effective as the above two components.

In summary, the various components used in the model proposed in this paper are all instrumental for the implicit sentiment analysis task.
