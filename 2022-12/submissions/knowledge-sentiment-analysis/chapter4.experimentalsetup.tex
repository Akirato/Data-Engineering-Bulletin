\section{Experimental Setup}

In this section, we present the details of the datasets used, the methods, and the implementation details of our models.

\subsection{Dataset}

In the field of Chinese implicit sentiment analysis research, the most widely used public dataset is the dataset used in the SMP2019 Chinese Implicit Sentiment Analysis Review (SMP-ECISA 2019) provided by Shanxi University \footnote{https://www.biendata.xyz/competition/ smpecisa2019/data/}.
The corpus of this dataset is collected from social media such as microblogs, travel websites, and product e-commerce, and a large-scale sentiment dictionary is used to filter the collected data, remove sentiment expression sentences with explicit sentiment words, and use the remaining text data as the Chinese implicit sentiment analysis dataset.
The main domains/topics covered in this dataset include but are not limited to: Spring Festival Gala, Haze, LeTV, National Exam, Travel, Dragon Boat Festival, etc. The classification of sentiment labels in the dataset is divided into 3 types: negative, neutral and positive, corresponding to the numerical labels 1, 2 and 0. Tab.~\ref{table:smp-ecisa-2019-dataset}~ shows the detailed statistics of the dataset.

\begin{table}
    \caption{SMP-ECISA 2019 Dataset}
    \label{table:smp-ecisa-2019-dataset}
    \centering
    \scalebox{1.25}{
        \begin{tabular}{cccccc}
            \toprule[1.5pt] %
                           & articles & labeled sentences & neutral & positive & negative \\
            \hline
            Training set   & 12,664   & 1,4774            & 6,989   & 3,828    & 3,957    \\
            Validation set & 4,391    & 5,143             & 2,553   & 1,232    & 1,358    \\
            Test set       & 6,380    & /                 & /       & /        & /        \\
            \hline
            After merging  & 17,055   & 19,917            & 9,542   & 5,060    & 5,315    \\
            \bottomrule[1.5pt]
        \end{tabular}
    }
\end{table}

In order to evaluate and compare existing methods and our method more fairly, we combined the training and validation sets to obtain 17,055 articles with a total of 19,917 labeled data, and re-divided the training set, validation set, and test set in the ratio of 8:1:1, and evaluated them using ten-fold cross-validation, and took the best result as the final result of the model.

\subsection{Evaluation Metrics}

To more reasonably and reliably measure the effectiveness of the method proposed in this paper, the F1 scores of the three emotional polarities and the macro-average F1 scores are calculated as indicators in this paper, as shown in the Eq.~(\ref{formula_4_12}), Eq.~(\ref{formula_4_13}).
\begin{equation}
    F_1i=\frac{2 * P_i * R_i}{P_i+R_i},
    \label{formula_4_12}
\end{equation}

\begin{equation}
    F_1-{\text{macro}}=\frac{1}{N} \sum_{i=1}^N F_1i,
    \label{formula_4_13}
\end{equation}

$i \in\{$ positive, negative, neutral $\}$ is one of the three sentiment polarities, and $P_i$ and $R_i$ are the accuracy and recall corresponding to the sentiment polarity $i$.

\subsection{Implementation Details}

Due to the limitation of the training hardware equipment, we can not train all the corpus at the same time, so the complete training set needs to be divided into many training batches. Based on a careful observation of the dataset, it was found that the corpus data in the dataset varied in length, which led to the internal setting of the model for the input length that may affect the final results of the model. Taking into account the hardware resources of the training environment, we chose 128 as the maximum length of input sentences, with each batch size of 16, and trained 5 epochs in total. For each single item in the BiLSTM, each has a hidden layer unit that needs to be learned by setting different sizes that will change the model's ability to capture features, and we set them all to 128. Additionally, the learning rate was set to 5e-5 and the dropout rate was 0.4.


\subsection{Baseline Methods}

Because of the limited research on the implicit sentiment analysis task of Chinese text, in this section, in addition to selecting some of the latest implicit sentiment analysis models according to the settings in the literature \cite{liao2022dynamic}, some representative models with good results in traditional explicit sentiment analysis tasks\cite{zhang2022incorporating} are selected as baselines for comparison experiments. The baseline experiments are described in detail as follows.

\paragraph{Bi-LSTM+multi-attention} 
The combination of Bi-LSTM combined with attention mechanism has been proven to be an effective model in many tasks related to sentiment analysis. The multi-headed attention mechanism can capture more of the differences between multiple attentions to some extent and enhance the effect even further.

\paragraph{Bi-LSTM+MPOA} 
\label{par:Bi-LSTM+MPOA}
Jiyao et al. \cite{wei2020BiLSTM} proposed an orthogonal multi-attention mechanism and fused it with Bi-LSTM to improve the traditional multi-attention mechanism by using orthogonal attention with specific sentiment polarity.

\paragraph{CMPOA} 
\label{par:CMPOA}
The model that Wang et al. \cite {WangSuGe2022} proposed integrates contextual information into the basic MPOA to compensate for the lack of information in the implicit sentiment sentence itself.


\paragraph{CsHGCN}
\label{par:CsHGCN}
Zuo et al. \cite{zuo2020context} proposed a heterogeneous graph convolutional network model to represent the implicit sentiment sentences. The entire context is treated as a heterogeneous graph at the document level and deep domain features are captured by graph convolution.

\paragraph{Tree-LSTM+KG} 
\label{par:Tree-LSTM+KG}
Syntactic structure information is important for modeling implicit sentiment sentences. The syntactic structure tree of the input sentence is obtained using LTP \cite{che2010ltp}, and then a Tree-LSTM \cite{tai2015improved} based model is constructed to fuse the syntactic structure to the embedding of the sentence.
