%!TEX root = data-validation-ml-systems.tex
\section{Current Solutions}
\label{sec:solutions}

Validating data at each of the stages in a ML workflow as sketched in \autoref{fig:ml-system} requires a wide range of competencies from data engineering over ML algorithms to user interface design. These competencies are often distributed across different roles in a team, and especially in small (5 to 10 members) teams, it is not unlikely that only one person has the competency to dive deep into a root cause analysis of the various data validation dimensions sketched in \autoref{sec:dimensions}. Solving problems in an on-call situation or when scaling up an ML system requires reliable automation for the data validation. As many of these validations have to be data dependent, such automation often amounts to {\em using ML to validate data in ML workflows} which has, for instance, been done for outlier removal \cite{Zhao2019}, data cleaning \cite{Krishnan2019} or missing value imputation \cite{Biessmann2018a}. 

\newpage
There are, however, simpler approaches that do not leverage ML models to validate data in ML systems, such as Tensorflow Extended (TFX) \cite{Baylor2017} or DEEQU \cite{Schelter2018}. In the following we will highlight some of the state of the art solutions to automated data validation in the context of ML systems.


\subsection{Schema Validation}

Validating syntactic correctness, also referred to as schema validation, is -- at first glance -- relatively simple and often integral part of data ingestion. Usually data violating data type constraints will immediately raise errors and break a data pipeline. Yet, these validations are often counteracted by humans. In practice it is not uncommon to use generic data types such as strings for data that is actually numeric, ordinal or categorical. While this will ensure that schema violations are minimized, as any data type can be cast to string, it imposes additional work on any downstream validation for data correctness and consistency. This problem is worsened by the fact that some software libraries popular in the ML community often cast string data that contains numeric data automatically to numeric data types. This can have effects on entire scientific communities. A recent study reports that casting errors induced by excel in biomedical data occur in approximately one fifth of the papers in leading genomics journals \cite{Ziemann2016}.

While simple validation such the data type can be trivially automated, it is unlikely that this will prevent engineers from having to deal with data type errors. There will always be on-call engineers who need to fix problems as fast as possible and the cost of having stale or broken data pipelines in a production system is often far bigger then the difficulty to quantify cost of accumulating hidden technical debt by specifying generic data types like strings and thereby circumventing data type checks. This is why this simple task of data type detection is actually an interesting research challenge. Often simple heuristics will do the job and the popular libraries use these heuristics to determine the data type of columns. The simplest heuristic is probably to try and cast a value to a certain data type and try other data types in case an error is raised\footnote{\url{https://github.com/pandas-dev/pandas/blob/v1.1.4/pandas/core/dtypes/common.py\#L138}}. Such heuristics often work well in practice but can be insufficient, especially when dealing with heterogeneous data, such as mixed data types in one column. Also, for the seemingly simple task of automating data type inference, there is interesting recent work on using ML models to infer a probabilistic notion of the most likely data type given the symbols used in a data base~\cite{Ceritli2020}. Approaches combining scalable heuristics with more sophisticated ML based data type inference techniques are a promising alternative to the current situation in which restrictive data type checks and broken data pipelines often lead data engineers to opt for too generic data types.

\subsection{Data Correctness and Data Consistency}

Validation of data consistency and correctness are one of the areas where classical DBMS research has made significant contributions, for instance in the data profiling literature \cite{Abedjan2018}. Popular approaches to validate correctness and consistency are rule based systems such as {\em NADEEF} \cite{Dallachiesat2013}. There are commercial versions of simple rule based systems, a popular example is trifacta\footnote{\url{https://www.trifacta.com/}}. And there are open source systems for semi-automated validation of consistency, such as Open-Refine\footnote{\url{https://openrefine.org/}}. Other approaches primarily investigated in an academic context include attempts to learn {\em (approximate/relaxed) functional dependencies} between columns of a table \cite{Papenbrock2015}. These functional dependencies between columns include for example a dependency between a column containing zip codes and another column containing city names. These dependencies can be used to automatically learn validation checks related to consistency and correctness \cite{Rekatsinas2017}. Such approaches are well established and there exist efficient methods for large scale functional dependency discovery \cite{Kruse2018}. Yet, in the context of data pipelines for ML systems none of these approaches have reached broad adoption. We discuss some potential reasons in \autoref{sec:conclusion}.

In addition to the solutions of the DBMS community there are a number of approaches developed at the intersection of DBMS and ML  communities, tailored to ML specific workflows \cite{Breck2019, Schelter2018}. The research is accompanied by open source libraries\footnote{\url{https://www.tensorflow.org/tfx/guide/tfdv}}\footnote{\url{https://github.com/awslabs/deequ}} that implement schema validation by checking for correct data types, but they also offer validation options for or other basic data properties, for instance whether the values are contained in a predefined set. Prominent examples for a data validation solution for ML systems are the ones integrated in SageMaker Data Wrangler\footnote{\url{https://aws.amazon.com/sagemaker/data-wrangler/}} and TFX \cite{Breck2019}. Their schema validation conveniently allows users to define integrity checks on the serving data in an ML system, e.g. for data type or value ranges. Schema validation parameters can also be derived from the training data, which allows to automate many of these checks.

\subsection{Validation of Statistical Properties}

All data sets are subject to noise, be it because humans enter wrong values into a form or because human software developers write code that leads to faulty data ingestion. Dealing with this noise is difficult for exact validation rules such as functional dependencies. Statistical learning techniques can help to deal with noisy data.

\paragraph{Error-Detection Solutions}
Taking a probabilistic stance on DBMSs is not a new idea \cite{Cavallo1987}. Recently, the idea of using statistical techniques for data validation tasks has become a major focus of research. One example are statistical learning approaches to error detection in data bases \cite{Rekatsinas2017,Heidari2019,Huang2017,MahdaviBerlinmahdavilahijani}. These approaches have the potential to automatically validate data correctness and to automatically clean data sets. Some of these systems can be automated to a large extent \cite{Krishnan2017, Krishnan2019}, others rely on a  semi-supervised approach \cite{Krishnan:2016b}.

\paragraph{ML Frameworks for Statistical Data Validation}
Also data validation solutions like TFX data validation \cite{Breck2019} or DEEQU \cite{Schelter2018} address data validation including statistical rules, such as deviation of values around the mean of a numeric column. These validation solutions can be very helpful, if one knows what to test for. But data sources can easily have billions of rows and hundreds of columns. For these cases it can infeasible to manually create, validate and maintain data quality checks. To alleviate the burden of manually creating constraints, the authors of DEEQU \cite{Schelter2018} propose to utilize historic data to generate column profiles and generate data quality constraints from these. These quality constraints can also make use of the temporal structure of data quality metrics collected over time using time series models or other anomaly detection methods.

\paragraph{Anomaly Detection Methods}
Instead of applying them to metrics computed on a data set, anomaly detection methods are also often used to detect anomalies in the data tuples directly. There are many methods available in easy to use software libraries, see for instance \cite{Zhao2019}, and there are commercial products that allow to automate anomaly detection in large scale industrial settings\footnote{\url{https://aws.amazon.com/de/blogs/big-data/real-time-clickstream-anomaly-detection-with-amazon-kinesis-analytics/}}. While the goal of these anomaly detection approaches is similar to the above error detection approaches originating in the DBMS community, many anomaly detection solutions emerged from the ML community. One of the most important differences is that anomaly detection approaches, as most ML methods, usually expect the data to come in matrix form, with all columns being numeric. Most methods from the DBMS community expect the data to be high cardinality categorical data, some also are applicable to numeric values, but that is not very common in research on functional dependencies for instance \cite{Papenbrock2015}. So applying anomaly detection methods to heterogeneous data sets with non-numeric (categorical, ordinal or even free text data) requires to apply {\em feature extraction} techniques to bring the data into a numeric format. This is a standard preprocessing step in ML pipelines, but popular software libraries for anomaly detection, such as  \cite{Zhao2019}, do not include this important preprocessing step. This makes these libraries difficult to apply for automated data validation. It is possible to integrate standard anomaly detection methods as statistical data validation steps in ML systems, but this imposes two additional challenges onto the engineering team. For one, this integration requires to write 'glue code' for the feature extraction -- which is often one of the major sources for accumulating {\em technical debt} in a software system. And secondly this requires to have a good evaluation metric for the anomaly detection. Which is, in contrast to standard supervised learning scenarios, often difficult to define and get ground truth data for.

\paragraph{Model Robustness and Generalization Performance}
Another central problem with all of the above approaches to statistical data validation in a ML context is that most of these methods are blind to the impact of data errors on downstream ML components. This is however one of the most important aspects for ML systems. Often it does not matter for the ML model, whether a feature is affected by a data shift, for instance when regularization of an ML model forces the ML model to ignore a feature. And in other cases tiny changes in the input, which human observers would not be able to detect, can have devastating consequences on the ML model output \cite{Athalye18}. Recent work in the ML community has shown that especially novel deep learning methods can suffer from severe stability problems \cite{DAmour2020}. Some aspects of this can be mitigated by employing standard ML concepts such as k-fold cross-validation (CV). This approach has unfortunately lost popularity due to the sheer compute demand of modern deep learning methods. Most deep learning papers usually use just a single train/validation/test split. Standard nested k-fold CV can have decisive advantages when it comes to measuring robustness of a ML model. However, these techniques only work when there is ground truth data available. In a production setting, this is often not the case. There exist however also other methods to measure the robustness of ML models when there is no ground truth data available. For instance in \cite{Schelter2020} the authors leverage a set of declaratively defined data errors applied to data for which ground truth is available and measure the predictive performance of a ML model under these perturbations. This allows to train a meta model that can be used to predict the predictive performance on new unseen data with high precision. Such an approach can be useful in production ML systems to automatically validate data errors with respect to their impact on downstream ML model performance.

\subsection{Fairness Validation}

Fairness is one of the most prominent examples of how ML systems can fail and severely influence the public opinion about a company or an entire scientific community. It is thus of utmost importance to ensure that this dimension of data validation in the context of ML systems is not neglected. Yet validating this dimension is especially difficult, for a number of reasons. First and foremost it is difficult to define fairness. An excellent overview over the last decades of fairness research with a focus on ML systems can be found in \cite{Mehrabi2019}. The main insight here is that fairness validation it is not only a technical challenge. Instead, it is imperative to include multiple scientific disciplines in this research, in particular also researchers from sociology, psychology and law. Setting the stage for such transdisciplinary research is a challenge in itself, for instance finding a common terminology is not trivial. But we have seen that the research community has made progress by fostering transdisciplinary discussions at newly emerging conferences\footnote{See for instance \url{https://dl.acm.org/conference/fat}}. The joint efforts of different communities have helped to identify many ways in which biases leading to unfair ML systems can enter a workflow. Many of these biases arise in the data generating process. Enabling scientists and engineers to identify such biases should be part of the research agenda of the data management and ML community \cite{Stoyanovich2020}. One example in this direction is {\em FairBench}~\cite{Yang2020}, an open source library that helps to trace changes in data distributions and visualize distortions with respect to protected group memberships throughout the pipeline. Another example is SageMaker Clarify\footnote{https://aws.amazon.com/sagemaker/clarify/}, an explainability feature for Amazon SageMaker that provides insights into data and ML models by identifying biases and explaining predictions. It is deeply integrated into Amazon SageMaker, a fully managed service that enables data scientists and developers to build, train, and deploy ML models at any scale. Clarify supports bias detection and feature importance computation across the ML lifecycle, during data preparation, model evaluation, and post-deployment monitoring. Libraries like these  are a prerequisite to better automation of fairness validation.
%
However, another more fundamental problem of fairness validation remains even if technical solutions for visualizing and detecting biases are available: Group membership variables are required for most technical solutions to fairness. Storing these variables can
be more challenging, from an operational stance, than storing other non-confidential data.


\subsection{Privacy Validation}
Similar to fairness, also privacy is a difficult to define validation dimension. However in contrast to fairness, the research community has converged to a few concepts that are relatively straightforward in terms of their definitions. Popular concepts include {\em differential privacy} \cite{Dworak_2006_diff} and {\em k-anonymity} \cite{Sweeney2002}, see also \autoref{sec:dimensions}.
Most ML related work on privacy focuses on differential privacy, where noise is added the data, gradients or the model coefficients. Validating and trading off privacy against predictive performance or {\em utility} of the ML model can be challenging \cite{Jayaraman2019}.
Empirically, evaluating privacy is often done using {\em membership inference attacks} \cite{Shokri2017}, which has also been adopted for unsupervised ML models \cite{Hayes2019}. One limitation of these approaches is that privacy validation is always dependent on a specific model and data set. General statements about privacy and utility independent of models and data is hence difficult.

\subsection{Validation of Robustness against adversarial attacks}

Privacy validation is aiming at defending the ML system against a certain type of adversarial attack, where for instance the identity of data points used for training the ML system is revealed. There are however other types of adversarial attacks, for instance when artificial examples are generated to result in ML predictions with a certain outcome. Validation of robustness against such types of attacks can be achieved by perturbations around the data manifold~\cite{pang2017robust, aleks2017deep}. This can be achieved by extracting latent representations of the input data~\cite{hendrycks2016early} or of the predictions~\cite{Bhagoji_2018, feinman2017detecting}. Alternative methods rely on training an additional classifier used to decide whether an example is adversarial or not~\cite{gong2017adversarial, grosse2017statistical, metzen2017detecting}. Complementary to the work on validating adversarial robustness, a lot of work has been devoted to making ML models more robust to adversarial attacks by augmenting the training datasets with adversarial examples~\cite{goodfellow2014explaining,aleks2017deep, zhang2019, Dingyuan2019}.


\subsection{Human in the loop evaluation}

Most of the above data quality dimensions are easy for humans to assess. This is why human audits are still one of the most direct and most robust options for data validation in ML systems. Expert auditors, such as researchers developing a new service, often can quickly identify errors and their root causes by simply inspecting input and outputs of a ML system. Among the most important disadvantages with this approach is that these validations are expensive and do not scale well. Sometimes human-in-the-loop validations can be scaled up using crowd-sourcing platforms such as Yandex' Toloka or Amazon Mechanical Turk. Increasing the quality of crowd-sourced validations is an active topic of ML research \cite{WortmanVaughan2018}. For instance there are attempts to render audits more efficient by including transparency of ML model predictions \cite{schmidt2019quantifying} or by providing more inciting incentives~\cite{Ho_2015, wang_2018}. Still, this approach can be difficult to automate and is generally used as an andon cord in a modelling pipeline rather than an integrated quality test. This is not only due to the fact that it is difficult to integrate human audits in build processes. Focussing on human judgements only can lead to biased validations, especially when using transparent ML methods~\cite{schmidt2020risk}.
